<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=0" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title>Prof. Junwei Liang / 梁俊卫</title>
  <!-- 搜索引擎优化stuff -->
	<meta name="description"
    content="Academic website for Junwei Liang. Dr. Junwei Liang is currently a tenure-track Assistant Professor at The Hong Kong University of Science and Technology (Guangzhou). He was a senior researcher at Tencent Youtu Lab working on cutting-edge computer vision research and applications. Prior to that, he received his Ph.D. degree from Carnegie Mellon University, working with Prof. Alexander Hauptmann. He is the recipient of Baidu Scholarship and Yahoo Fellowship, and awarded Rising Star Award at the World AI Conference in 2020. He is the winner of several public safety video analysis competition, including ASAPS and TRECVID ActEV. His work has helped and reported by major news agencies like the Washington Post and New York Times. His research interests include human trajectory forecasting, action recognition, and large-scale computer vision and video analytics in general. His mission: develop AI technologies for social good.">
	<meta name="keywords" content="Junwei Liang,CMU,HKUST,HKUST-GZ,Professor,computer vision,PhD,梁俊卫,Carnegie Mellon University,The Hong Kong University of Science and Technology">

</head>
<body>
<div id="sidebar">
  <img class='me' src="resources/me.jpeg"></img>
  <br/>
  <div class="info">
    <h2 class="name">Prof. Junwei Liang</h2>
    <h2 class="name_chinese">梁俊卫</h2>
    <h2 class="email">junweiliang1114@gmail.com</h2>
    <h2 class="email">junweiliang@hkust-gz.edu.cn</h2>
    <h2 class="email">HKUST (Guangzhou) / Office: E4-304</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Google Scholar</a>
    </h2>
    <h2 class="link">
      <a href="https://www.semanticscholar.org/author/Junwei-Liang/1915796">[Semantic Scholar]</a>
      <a href="https://www.researchgate.net/profile/Junwei_Liang3">[Research Gate]</a>
    </h2>
    <h2 class="link">
      <a href="https://github.com/JunweiLiang">[Github]</a>
      <a href="https://paperswithcode.com/search?q=author%3AJunwei+Liang">[PaperWithCode]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.zhihu.com/people/junwei-liang-50">[知乎]</a>
      <a href="https://www.youtube.com/channel/UC-z7ZWp8Rbu2xhxnbAL_bRQ">[Youtube]</a>
      <a href="https://space.bilibili.com/1746376957/">[B站]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.linkedin.com/in/junweiliang/">[LinkedIn]</a>
      <a href="https://twitter.com/JunweilLiang">[Twitter]</a>
      <a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">[小红书]</a>
    </h2>

    <!--
    <a class="quickLink" href="https://medium.com/@junweil">
      <img class='medium' style="" src="resources/medium.png"></img>
    </a>
    <a class="quickLink" href="https://dblp.org/pers/hd/l/Liang_0001:Junwei">
      <img class='dblp' style="height:20px" src="resources/dblp.png"></img>
    </a>
    <a class="quickLink" href="http://aminer.cn/profile/junwei-liang/562cb48c45cedb3398c9e13b">
      <img class='aminer' style="height:20px;width: 50px;margin-top:4px" src="resources/aminer.png"></img>
    </a>
    <a class="quickLink" href="https://g.co/kgs/gTWf5W">
      <img class='aminer' name="Google knowledge graph" style="height:30px;width: 30px;margin-top:0px" src="resources/gkg.png"></img>
    </a>-->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./projects.html#projects">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Projects
    </a>
    <a class="nav_item" href="./projects.html#publications">
      <i class="icon icon-file icon-white"></i> &nbsp; Publications
    </a>
    <a class="nav_item" href="./teaching.html#teaching">
      <i class="icon icon-user icon-white"></i> &nbsp; Teaching / Talks
    </a>
    <a class="nav_item" href="./index.html#awards">
      <i class="icon icon-bookmark icon-white"></i> &nbsp; Honors / Awards
    </a>
    <a class="nav_item" href="./index.html#media">
      <i class="icon icon-volume-up icon-white"></i> &nbsp; Selected Media
    </a>
    <a class="nav_item" href="./awesome.html">
      <i class="icon icon-list icon-white"></i> &nbsp; Awesome Lists
    </a>
    <a class="nav_item" href="./letter.html">
      <i class="icon icon-pencil icon-white"></i> &nbsp; Letter
    </a>

  </div>
</div>

<style type="text/css">
  #main{
    background: #f7f7f7;
  }
  #main > div.publications > ol > li{
    background: white;
    box-shadow: 2px 5px 5px #c9c9c9;
    margin-bottom: 10px;
    padding: 20px;
  }
  #main div.img{
    padding: 20px;
    text-align: center;
    padding-right: 150px;
  }
  #main div.img > div.img_caption{
    font-weight: 450;
    font-size: 1.1em;
    line-height: 40px;
  }
</style>
<div id="main">

  <div class="title">
    <a class="title_link" id="projects" href="#projects">Projects</a>
  </div>

  <div class="content">
    See our precognition lab website for a <span style="font-weight: bold;"><a href="https://precognition.team/projects.html#roadmap">research roadmap</a></span>
    and <span style="font-weight: bold;"><a href="https://precognition.team/projects.html#demos">demos</a></span>.
    <br/>
    Here are on-going or finished research grants:
    <ul>
      <li>
        国自然青年基金，2024 (PI)
      </li>
      <li>
        广州市校（院）企联合资助项目，2024 (PI)
      </li>
      <li>
        广州市基础与应用基础研究专题（青年博士“启航”项目），2024 (PI)
      </li>
      <li>
        美团深圳机器人研究院课题，2024 (PI)
      </li>
    </ul>


  </div>


  <div class="title">
    <a class="title_link" id="publications" href="#publications">Publications</a>
  </div>

  <div class="content publications">
    <ol>
      The following are my PhD publications. For more up-to-date research in my lab, please visit the <a href="https://precognition.team/index.html#publications">Precognition Lab</a> website.
      <li>
        <div class="imgblock"><img src="camera_ready/multiverse.gif"></img></div>
        <span class="title">The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Kevin Murphy, Ting Yu, Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">CVPR 2020</span> &nbsp;
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Multiverse&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1912.06445" target="_blank">[Paper]</a>
          <a class="" href="https://precognition.team/next/multiverse/resources/cvpr2020.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://youtu.be/RW45YQHxIhk" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/next/multiverse" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">[blog]</a>
          <a href="https://zhuanlan.zhihu.com/p/148343447">[知乎]</a>
          <a href="https://research.google/pubs/pub49224/">[Google Research]</a>
          <a href="https://mp.weixin.qq.com/s/s6bk5psLwpGpO1VwtQqo_g">[读芯术学术报告]</a>
          <a href="https://sites.google.com/di.uniroma1.it/patcast/program?authuser=0">[Invited presentation at ICPR'20 pattern forecasting workshop]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <!--<div class="imgblock"><img src="camera_ready/peekfuture.png"></img></div>-->
        <div class="imgblock" style="height:280px"><img src="camera_ready/next.gif"></img></div>
        <span class="title">Peeking into the Future: Predicting Future Person Activities and Locations in Videos
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Juan Carlos Niebles, Alexander Hauptmann, Li Fei-Fei</div>
        <div class="info"><span class="label label-info">CVPR 2019</span> <span class="text-error">(Translated and reported by multiple Chinese media (<a href="https://weixin.sogou.com/weixin?type=1&s_from=input&query=%E9%87%8F%E5%AD%90%E4%BD%8D" target="_blank">量子位</a> & <a href="https://weixin.sogou.com/weixin?type=1&s_from=input&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83" target="_blank">机器之心</a>, 02/13/2019), with 30k+ views in a week.)</span> </div>
        <div class="info"><span class="text-error">#1 Tensorflow-based code on <a href="https://paperswithcode.com/task/trajectory-prediction">PaperWithCode</a> in Trajectory Prediction task. </span> <iframe src="https://ghbtns.com/github-btn.html?user=google&repo=next-prediction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/1902.03748" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/future19.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=NyrGxGoS01U" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/next" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://research.google/pubs/pub47873/">[Google Research]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/cross_action.jpg"></img></div>
        <span class="title">Multi-dataset Training of Transformers for Robust Action Recognition
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Enwei Zhang, Jun Zhang, Chunhua Shen</div>
        <div class="info"><span class="label label-info">NeurIPS 2022</span>
          <span class="text-error">(<a href="https://nips.cc/virtual/2022/spotlight/65262">Spotlight paper</a>, 3.7% acceptance rate, 384/10411)</span>
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2209.12362">Paper</a>]
          [<a href="https://github.com/JunweiLiang/MultiTrain">Project Page/Code/Model</a>]
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=MultiTrain&type=star&count=true" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>
        </div>
        <div style="clear:both"></div>
      </li><li>
        <div class="imgblock"><img src="camera_ready/video_text.jpg"></img></div>
        <span class="title">Text-Adaptive Multiple Visual Prototype Matching for Video-Text Retrieval
        </span>
        <div class="info text-success italic">Chengzhi Lin, Ancong Wu, <span style="font-weight:bold">Junwei Liang</span>,  Jun Zhang, Wenhang Ge, Wei-Shi Zheng, Chunhua Shen</div>
        <div class="info"><span class="label label-info">NeurIPS 2022</span> &nbsp;
        </div>
        <div class="stuff">
          [<a href="https://arxiv.org/abs/2209.13307">Paper</a>]
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/aicity.jpg"></img></div>
        <span class="title">Stargazer: A transformer-based driver action detection system for intelligent transportation
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, He Zhu, Enwei Zhang, Jun Zhang</div>
        <div class="info"><span class="label label-info">CVPRW 2022</span> &nbsp;
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=aicity_action&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info"><span class="text-error">#2 (out of 150 teams) on the public leaderboard of the Naturalist Driver Action Recognition Task - AI City Challenge @ CVPR 2022. </span></div>
        <div class="stuff">
          <a class="" href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.pdf" target="_blank">[Paper]</a>
          <a class="" href="https://www.youtube.com/watch?v=u4CrNKt4P54" target="_blank">[Presentation]</a>
          <a class="" href="https://github.com/JunweiLiang/aicity_action" target="_blank">[Project Page/Code/Model]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/msnet.jpg"></img></div>
        <span class="title">MSNet: A Multilevel Instance Segmentation Network for Natural Disaster Damage Assessment in Aerial Videos
        </span>
        <div class="info text-success italic">Xiaoyu Zhu, <span style="font-weight:bold">Junwei Liang</span>, Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">WACV 2021</span> &nbsp;
          <iframe src="https://ghbtns.com/github-btn.html?user=zgzxy001&repo=MSNET&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info"><span class="text-error">Reported by CMU news </span></div>
        <div class="stuff">
          <a class="" href="http://openaccess.thecvf.com/content/WACV2021/papers/Zhu_MSNet_A_Multilevel_Instance_Segmentation_Network_for_Natural_Disaster_Damage_WACV_2021_paper.pdf" target="_blank">[Paper]</a>
          <a class="" href="https://www.cmu.edu/news/stories/archives/2020/august/drones-hurricane-damage.html" target="_blank">[CMU News]</a>
          <a class="" href="https://github.com/zgzxy001/MSNET" target="_blank">[Project Page/Code/Model]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/simaug.gif"></img></div>
        <span class="title">SimAug: Learning Robust Representations from Simulation for Trajectory Prediction
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">ECCV 2020</span> &nbsp;
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Multiverse&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="stuff">
          <a class="" href="https://arxiv.org/abs/2004.02022" target="_blank">[Paper]</a>
          <a class="" href="https://precognition.team/next/simaug/resources/eccv2020.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://precognition.team/next/simaug" target="_blank">[Project Page/Code/Model]</a>
          <a href="https://research.google/pubs/pub49354/">[Google Research]</a>
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"  style="height:170px;" ><img src="camera_ready/mm19.gif"></img></div>
        <span class="title">Shooter Localization Using Social Media Videos
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Jay Aronson, Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">ACM Multimedia (MM) 2019</span> <span class="text-error"><br/>(Press coverage:
          <a href="https://pittsburgh.cbslocal.com/2019/11/20/cmu-develops-video-system-locate-mass-shooters/">
            <img class="press" src="resources/cbs.png"></img>
          </a>,
          <a href="https://www.cmu.edu/news/stories/archives/2019/november/system-locates-shooters-using-smartphone-video.html">
            <img class="press" src="resources/cmu.png"></img>
          </a>,

          <a href="https://www.wpxi.com/news/top-stories/shooters-can-be-located-with-smartphone-video-using-new-cmu-developed-tool/1010922936">
            <img class="press" src="resources/wpxi.png"></img>
          </a>,
          <a href="https://www.post-gazette.com/business/tech-news/2019/11/20/Carnegie-Mellon-CMU-develops-cellphone-smartphone-video-system-location-shooter-triangulate/stories/201911200101">
            <img class="press" src="resources/post.png"></img>
          </a>,
          <a href="https://www.dailymail.co.uk/sciencetech/article-7707501/Carnegie-Mellon-aims-end-pro-longed-massacres-locates-active-shooters.html">
            <img class="press" src="resources/dailymail.png"></img>
          </a>,
          <a href="https://gizmodo.com/smartphone-videos-can-now-be-analyzed-and-used-to-pinpo-1839979803">
            <img class="press" src="resources/gizmodo.png"></img>
          </a>,
          <a href="https://www.msn.com/en-us/news/us/researchers-at-carnegie-mellon-university-develop-video-system-to-locate-mass-shooters/ar-BBX3wRA">
            <img class="press" src="resources/msn.png"></img>
          </a>,
          <a href="https://www.techspot.com/news/82881-researchers-develop-system-can-pinpoint-shooter-location-using.html">
            <img class="press" src="resources/techspot.png"></img>
          </a>,
          <a href="https://www.sciencedaily.com/releases/2019/11/191120070712.htm">
            <img class="press" src="resources/science_daily.png"></img>
          </a>,
          <a href="https://gcn.com/articles/2019/11/22/smartphone-shooter-location.aspx">
            <img class="press" src="resources/gcn.png"></img>
          </a>
        )</span> </div>
        <div class="stuff">
          <a class="" href="https://www.cmu.edu/chrs/publications/pdf/shooter-localization-using-social-media-videos.pdf" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/mm19.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=6q7LqqzrY2I" target="_blank">[Demo Video]</a>
          <a class="" href="https://vera.cs.cmu.edu" target="_blank">[Project Page]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/memexqa.png"></img></div>
        <span class="title">Focal Visual-Text Attention for Memex Question Answering
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Liangliang Cao, Yannis Kalantidis, Li-Jia Li, and Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">TPAMI 2019</span></div>
        <div class="stuff">
          <a class="" href="https://ieeexplore.ieee.org/document/8603827" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/tpami19.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=hH-SXKA7hE8" target="_blank">[Demo Video]</a>
          <a class="" href="https://precognition.team/memexqa" target="_blank">[Code/Model/Dataset]</a>
          <a href="https://research.google/pubs/pub47871/">[Google Research]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/fvta.png"></img></div>
        <span class="title">Focal Visual-Text Attention for Visual Question Answering
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Liangliang Cao, Li-Jia Li, and Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">CVPR 2018</span> <span class="text-error">(Spotlight Paper, <span style="font-weight:bold">6.8%</span> acceptance rate)</span></div>
        <div class="stuff">
          <a class="" href="camera_ready/cvpr18.pdf" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/cvpr18.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://precognition.team/memexqa/fvta.html" target="_blank">[Code/Model]</a>
          <a  class="" href="https://youtu.be/TBOnKekODCI?t=1h11m29s" target="_blank">[Presentation]</a>
          <a href="https://research.google/pubs/pub47012/">[Google Research]</a>
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/3d_recon.png"></img></div>
        <span class="title">An Event Reconstruction Tool for Conflict Monitoring Using Social Media
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Desai Fan, Han Lu, Poyao Huang, Jia Chen, Lu Jiang, and Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">AAAI 2017 Demo</span></div>
        <div class="stuff">
          <a class="" href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14659/14033" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/aaai17_2.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=DAhL5QL9Ko4" target="_blank">[Demo Video]</a>
          <a class="" href="https://vera.cs.cmu.edu/VERA_3D_Reconstruction/" target="_blank">[Project Page]</a>
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/aaai17_1.png"></img></div>
        <span class="title">Webly-Supervised Learning of Multimodal Video Detectors
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, and Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">AAAI 2017 Demo</span></div>
        <div class="stuff">
          <a class="" href="camera_ready/aaai17_1.pdf" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/aaai17_1.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=3he6VDwYMCQ" target="_blank">[Demo Video]</a>
          <a class="" href="posters/well_poster.pptx" target="_blank">[Poster]</a>
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/ijcai16.png"></img></div>
        <span class="title">Learning to Detect Concepts from Webly-Labeled Video Data
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang, Deyu Meng, and Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">IJCAI 2016</span></div>
        <div class="stuff">
          <a class="" href="camera_ready/ijcai16.pdf" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/ijcai16.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=3he6VDwYMCQ" target="_blank">[Demo Video]</a>
        </div>
        <div style="clear:both"></div>
      </li>
      <li>
        <div class="imgblock"><img src="camera_ready/icmr16.png"></img></div>
        <span class="title">Video Description Generation using Audio and Visual Cues
        </span>
        <div class="info text-success italic">Qin Jin, and <span style="font-weight:bold">Junwei Liang</span></div>
        <div class="info"><span class="label label-info">ICMR 2016</span></div>
        <div class="stuff">
          <a class="" href="camera_ready/icmr16.pdf" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/icmr16.bib" target="_blank">[BibTex]</a>
        </div>
        <div style="clear:both"></div>
      </li>

      <li>
        <div class="imgblock"><img src="camera_ready/icassp17_1.png"></img></div>
        <span class="title">Temporal Localization of Audio Events for Conflict Monitoring in Social Media
        </span>
        <div class="info text-success italic"><span style="font-weight:bold">Junwei Liang</span>, Lu Jiang and Alexander Hauptmann</div>
        <div class="info"><span class="label label-info">ICASSP 2017</span></div>
        <div class="stuff">
          <a class="" href="camera_ready/icassp17_1.pdf" target="_blank">[Paper]</a>
          <a class="" href="camera_ready/icassp17_1.bib" target="_blank">[BibTex]</a>
          <a class="" href="https://www.youtube.com/watch?v=em-0CsJoqeU" target="_blank">[Demo Video]</a>
        </div>
        <div style="clear:both"></div>
      </li>


    </ol>
  </div>

  <div class="title">
    <a class="title_link" id="past-projects" href="#past-projects">Past Projects</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="title">Efficient Action Detection <div class="float-right time">2021 - 2022</div>
        <div class="info">
          <a href="https://github.com/JunweiLiang/aicity_action">Naturalist Driver Action Recognition</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=aicity_action&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
      </li>

      <li>
        <span class="title">Trajectory/Activity Forecasting [sponsored by <a href="https://www.iarpa.gov/research-programs/diva">IARPA</a>]</span> <div class="float-right time">2018 - 2021</div>
        <div class="info">
          <a href="https://precognition.team/next/simaug">SimAug - Multi-view Adversarial Learning</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Multiverse&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info">
          <a href="https://precognition.team/next/multiverse">Multiverse - 3D Simulation</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Multiverse&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info">
          <a href="https://precognition.team/next/index.html">Next-Prediction</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=google&repo=next-prediction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info">
          <a href="https://github.com/JunweiLiang/social-distancing-prediction">COVID-19 Project - Social Distancing Early Forecasting [Awarded $6200 GCP research credits]</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=social-distancing-prediction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info">
          <a href="https://github.com/JunweiLiang/Object_Detection_Tracking">Object Detection and Tracking in Videos</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Object_Detection_Tracking&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
      </li>

      <li>
        <span class="title">Public Safety / AI for Social Good [sponsored by <a href="https://www.nist.gov/ctl/pscr/real-time-video-analytics-situation-awareness">NIST</a>] [<a href="https://www.herox.com/ASAPS1/update/3483">ASAPS challenge</a> winner]</span> <div class="float-right time">2017 - 2021</div>
        <div class="info">
          <a href="https://vera.cs.cmu.edu/">Gunshot Detection & Shooter Localization</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=VERA_Shooter_Localization&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info">
          <a href="https://vera.cs.cmu.edu/VERA_3D_Reconstruction/">3D Event Reconstruction</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=VERA_3D_Reconstruction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info">
          <a href="https://aladdin1.inf.cs.cmu.edu/human-rights">Video Analytic Toolkit</a>
        </div>
      </li>

      <li>
        <span class="title">Multimodal Question Answering</span> <div class="float-right time">2016 - 2019</div>
        <div class="info">
          <a href="https://precognition.team/memexqa/">MemexQA - the First Personal Multimedia Collection QA dataset</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=FVTA_MemexQA&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
        <div class="info">
          <a href="#">Dual Attention Network</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=DualAttentionNetwork&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
      </li>

      <li>
        <span class="title">Weakly Supervised Learning</span> <div class="float-right time">2015 - 2017</div>
        <div class="info">
          <a href="https://www.cs.cmu.edu/~junweil/camera_ready/ijcai16.pdf">Webly-labeled Learning</a>
        </div>
        <div class="info">
          <a href="#">Video Semantic Features</a>
          <iframe src="https://ghbtns.com/github-btn.html?user=JunweiLiang&repo=Semantic_Features&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
        </div>
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="review" href="#review">Academic Service</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="title">Workshop Organizer</span>
        <div class="info">Open-world Visual Perception Workshop @PRCV 2023 [<a href="https://mp.weixin.qq.com/s/ib9aKBhQhoaAFqZB93F3wQ">link</a>]</div>
        <div class="info">Precognition Workshop @CVPR 2023 [<a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">link</a>] [<a href="https://www.youtube.com/watch?v=Z3JhfOp0eGM">CVPR Recording</a>]</div>
      </li>
      <li>
        <span class="title">Journal Reviewer</span>
        <div class="info">IEEE Transactions on Pattern Analysis and Machine Intelligence <span style="font-weight: bold;">(TPAMI)</span></div>
        <div class="info">IEEE Transactions on Image Processing (TIP)</div>
        <div class="info">Pattern Recognition</div>
        <div class="info">Artificial Intelligence Review (AIRE)</div>
        <div class="info">IEEE Transactions on Intelligent Transportation Systems (ITS)</div>
        <div class="info">IEEE Transactions on Multimedia</div>
        <div class="info">IEEE Internet of Things Journal</div>
        <div class="info">IEEE Transactions on Big Data</div>
        <div class="info">IEEE Transactions on Medical Imaging</div>
        <div class="info">IEEE Access</div>
        <div class="info">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</div>
        <div class="info">ACM TOMM</div>
        <div class="info">Neurocomputing</div>
        <div class="info">Nature Scientific Reports</div>
        <div class="info">Information Sciences</div>
        <div class="info">Defense Technology</div>
      </li>
      <li>
        <span class="title">Conference Reviewer</span>
        <div class="info">CVPR/ICCV/WACV/ECCV/ACM Multimedia</div>
        <div class="info">AAAI/NeurIPS/ICLR/ICRA</div>
        <div class="info">NAACL-HLT SRW 2021</div>
        <div class="info">ACL 2020 Student Research Workshop</div>
        <div class="info">CVPR 2020 AI for Content Creation Workshop</div>
      </li>
    </ul>
  </div>

</div>


<!--
	a Junwei Liang's production
	contact: junweiliang1114@gmail.com
-->
</body>
</html>
