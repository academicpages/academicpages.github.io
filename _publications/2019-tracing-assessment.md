---
title: "Towards validity for a formative assessment for language-specific program tracing skills"
collection: publications
permalink: /publication/2019-tracing-assessment
excerpt: "Part of my undergrad research in [Amy Ko's Lab](https://faculty.washington.edu/ajko)"
date: 2019-11-21
venue: 'Koli Calling'
paperurl: '/files/3364510.3364525.pdf'
citation: 'Nelson, G.L., <u><b>Hu, A.D.</b></u>, Xie, B., & Ko, A.J. (2019). Towards validity for a formative assessment for language-specific program tracing skills. <i>Proceedings of the 19th Koli Calling International Conference on Computing Education Research</i>.'
---

Full abstract:

Formative assessments can have positive effects on learning, but few exist for computing, even for basic skills such as program tracing. Instead, teachers often rely on overly broad test questions that lack the diagnostic granularity needed to measure early learning. We followed Kane's framework for assessment validity to design a formative assessment of JavaScript program tracing, developing "an argument for effectiveness for a specific use." This included: 1) a fine-grained scoring model to guide practice, 2) item design to test parts of our fine-grained model with low confound-caused variance, 3) a covering test design that samples from a space of items and covers the scoring model, and 4) a feasibility argument for effectiveness for formative use (can target and improve learning). We contribute a distillation of Kane's framework situated for computing education, and a novel application of Kane's framework to formative assessment of program tracing, focusing on scoring, generalization, and use. Our application also contributes a novel way of modeling possible conceptions of a programming language's semantics by modeling prevalent compositions of control flow and data flow graphs and the paths through them, a process for generating test items, and principles for minimizing item confounds.