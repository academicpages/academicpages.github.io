---
title: "Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?"
collection: publications
category: preprints
permalink: /publication/2025/10/post-ape/
excerpt: |
      Automatic post-editing (APE) aims to improve machine translations by correcting residual errors. While recent large language models (LLMs) exhibit strong translation abilities, their capacity to perform APE—particularly with document-level context in mind—remains underexplored. This work presents a comparative study of proprietary and open-weight LLMs, examining their APE quality, behavior, and efficiency. Results show that proprietary models attain human-level post-editing quality even with simple one-shot prompting, whereas open-weight models often over-edit or hallucinate under long-context inputs. Efficiency analysis further reveals that proprietary models handle document context with modest token growth and minimal latency, while open-weight models suffer from substantial slowdowns despite processing fewer tokens. Automatic metrics fail to capture these qualitative improvements, underscoring the necessity of human evaluation. Our findings highlight both the potential and limitations of current LLMs for document-aware APE and provide insights toward more efficient long-context modeling for translation refinement.
date: 2025-10-21
image: 'todo'
venue: 'TechRxiv (under review)'
paperurl: 'https://www.techrxiv.org/users/915991/articles/1342797-do-llms-truly-benefit-from-longer-context-in-automatic-post-editing'
codeurl: 'https://github.com/trotacodigos/Reference-Contamination.git'
bibtexurl: 'todo'
authors: '<span class="me">Ahrii Kim</span>'
---