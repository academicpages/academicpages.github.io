---
title: "Enhancing Bangla Language Next Word Prediction and Sentence Completion through Extended RNN with Bi-LSTM Model On N-gram Language"
collection: publications
category: conferences
permalink: /publication/next-word-prediction
excerpt: 'Texting stands out as the most prominent form of communication worldwide. Individual spend significant amount of time writing whole texts to send emails or write something on social media, which is time consuming in this modern era. Word prediction and sentence completion will be suitable and appropriate in the Bangla'
date: 24 June 2024
venue: '3rd International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE)'
publisher: 'IEEE'
paperurl: 'https://arxiv.org/pdf/2405.01873'
citation: 'Islam, Md Robiul, Al Amin, and Aniqua Nusrat Zereen. "Enhancing Bangla Language Next Word Prediction and Sentence Completion through Extended RNN with Bi-LSTM Model On N-gram Language." arXiv preprint arXiv:2405.01873 (2024).'
---

Texting stands out as the most prominent form of communication worldwide. Individual spend significant amount of time writing whole texts to send emails or write something on social media, which is time consuming in this modern era. Word prediction and sentence completion will be suitable and appropriate in the Bangla language to make textual information easier and more convenient. This paper expands the scope of Bangla language processing by introducing a Bi-LSTM model that effectively handles Bangla next-word prediction and Bangla sentence generation, demonstrating its versatility and potential impact. We proposed a new Bi-LSTM model to predict a following word and complete a sentence. We constructed a corpus dataset from various news portals, including bdnews24, BBC News Bangla, and Prothom Alo. The proposed approach achieved superior results in word prediction, reaching 99% accuracy for both 4-gram and 5-gram word predictions. Moreover, it demonstrated significant improvement over existing methods, achieving 35%, 75%, and 95% accuracy for uni-gram, bi-gram, and tri-gram word prediction, respectively.
