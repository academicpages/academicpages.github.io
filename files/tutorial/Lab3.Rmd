---
title: "Topic Modeling in Social Science"
author: "Yongjun Zhang"
date: ""
output:
  rmdformats::readthedown:
    highlight: pygments
---

```{=html}
<style type="text/css">
p{ /* Normal  */
   font-size: 18px;
}
body{ /* Normal  */
   font-size: 18px;
}
td {  /* Table  */
   font-size: 14px;
}
h1 { /* Header 1 */
 font-size: 32px;
}
h2 { /* Header 2 */
 font-size: 26px;
}
h3 { /* Header 3 */
 font-size: 22px;
}
code.r{ /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 14px
}
</style>
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE)

```

# Latent Dirichlet Allocation

> We will start with the simple latent dirichlet allocation with gibbs sampling. This part is adapted from [Ethen Liu](https://github.com/ethen8181/machine-learning/blob/master/clustering_old/topic_model/)'s intuitive demo to LDA. For detailed intuitive or tech intro, please check our lecture slides or Blei's article. You can also check here for [gibbs sampling](http://www.pnas.org/cgi/doi/10.1073/pnas.0307752101).

**Latent Dirichlet Allocation** (LDA) is a probabilistic topic modeling method that allows us to discover and annotate texts. The key assumptions are as follows (see Mohr and Bogdanov 2013) .

> Each document (text) within a corpus is viewed as a bag-of-words produced according to a mixture of themes that the author of the text intended to discuss. Each theme (or topic) is a distribution over all observed words in the corpus, such that words that are strongly associated with the document's dominant topics have a higher chance of being selected and placed in the document bag. Thus, the goal of topic modeling is to find the parameters of the LDA process that has likely generated the corpus.

Based on this week's reading (Blei 2012), we know that the topic distribution for each document is

$$ \theta \sim Dirichlet(\alpha) $$

Where $Dirichlet(\alpha)$ denotes the Dirichlet distribution for parameter $\alpha$.

The word distribution for each topic also modeled by a Dirichlet distribution with a different parameter $\eta$.

$$ \beta \sim Dirichlet(\eta) $$

Our goal is to estimate the $\theta$ and $\beta$ using observed words in documents. That being said, we are trying to understand which words are important for which topic and which topics are important for a particular document, respectively.

Note that the [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution) is a probability distribution for parameters $\alpha$. Where $\alpha$ governs the concentration of the distribution. Sometimes people call this *concentration parameter* or *scaling parameter*. When $\alpha$ approaches 0, it means documents are concentrated in a few topics. So a higher value suggests that topics are more evenly distributed accross the documents. This also applied to $\beta$ regarding topic-word.

We will use Gibbs sampling to compute the conditional probability specified in Blei's article (eq 2). Generally speaking, LDA is a generative model of word counts. We are interested in the conditional probability of hidden topic structure given the observed words in documents.

To simplify the demo process, we will use 10 short strings to represent 10 documents (Note that recent study shows that the length of document and the number of documents do influence our results. Just be careful about this). We deliberately get 5 sentences describing Chinese food and 5 sentences describing American football from Wikipedia.

Usually before running topic model, we need to normalize our texts as shown in our lecture (like tidy texts, removing stop words, white-spaces, etc.). We often use tidytext, tm, or [quanteda](https://tutorials.quanteda.io/) packages in R to preprocess the texts, but now let us stick to basic stuff. I strongly suggest you to take some time to read the quanteda tutorial.

```{r}
raw_docs <- c(
	"Chinese cuisine is an important part of Chinese culture, which includes cuisine originating from the diverse regions of China, as well as from Overseas Chinese who have settled in other parts of the world.",
	"The preference for seasoning and cooking techniques of Chinese provinces depend on differences in historical background and ethnic groups.",
	"Chinese society greatly valued gastronomy, and developed an extensive study of the subject based on its traditional medical beliefs.",
	"There are a variety of styles of cooking in China, but Chinese chefs have classified eight regional cuisines according to their distinct tastes and local characteristics. ",
	"Based on the raw materials and ingredients used, the method of preparation and cultural differences, a variety of foods with different flavors and textures are prepared in different regions of the country. ",
	"American football, referred to as football in the United States and Canada and also known as gridiron,is a team sport played by two teams of eleven players on a rectangular field with goalposts at each end",
	"American football evolved in the United States, originating from the sports of soccer and rugby. The first American football match was played on November 6, 1869, between two college teams, Rutgers and Princeton, using rules based on the rules of soccer at the time.",
	"American football is the most popular sport in the United States. The most popular forms of the game are professional and college football, with the other major levels being high school and youth football. ",
	"In football, the winner is the team that has scored more points at the end of the game. There are multiple ways to score in a football game. ",
	"Football games last for a total of 60 minutes in professional and college play and are divided into two halves of 30 minutes and four quarters of 15 minutes."
)

# lower cases and remove punctuation or double spaces
raw_docs <- stringr::str_replace_all(tolower(raw_docs),"[:punct:]","")

# remove stop words
stopwords_regex = paste(stopwords::stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
raw_docs <- stringr::str_replace_all(raw_docs,stopwords_regex, '')

# remove the most frequent words, chinese,american, football
raw_docs <- stringr::str_replace_all(raw_docs,"chinese|american|football", '')
raw_docs[[1]]

# let us squish our text, removing extra spaces
raw_docs <- stringr::str_squish(raw_docs)

# segmenting each work, similar to tokenization.
docs <- strsplit(raw_docs, split = " ")
docs[[1]]

# get a vocabulary of unique words in our corpus
vocab <- unique( unlist(docs) )

# represent strings using numerical numbers
# use the base match function match(x,table)
# If x[i] is found to equal table[j] then the value returned in the i-th position of the return value is j, for the smallest possible j. 
for( i in 1:length(docs) ) {
	docs[[i]] <- match( docs[[i]], vocab )
}
docs

```

In LDA, we have to specify the number of clusters (i.e., topics) first. Usually it was denoted by K. In this case, let us do 2.

If we recall correctly, in Blei's article, he described the generative process of LDA. It has several major steps.

```{r, out.width="",out.height="",fig.align='center', fig.cap='Blei 2012'}
knitr::include_graphics('figure1.png')

```

> Here, let us first go through each document and randomly assign each word in the document to one of the K topics. This is the topic assignment process. The right side of Blei's article in Figure 1.

> Then we create a **word-topic matrix**, which is the count of each word being assigned to each topic. And a **document-topic matrix**, which is the number of words assigned to each topic for each document.

```{r}

# cluster number 
K <- 2 

# initialize count matrices 
# @wt : word-topic matrix 
wt <- matrix( 0, nrow = K, ncol = length(vocab) )
colnames(wt) <- vocab

# @ta : topic assignment list
ta <- lapply( docs, function(x) rep( 0, length(x) ) ) 
names(ta) <- paste0( "doc", 1:length(docs) )

# @dt : counts correspond to the number of words assigned to each topic for each document
dt <- matrix( 0, length(docs), K )

set.seed(2020)
for( d in 1:length(docs) ) { 
	# randomly assign topic to word w
	for( w in 1:length( docs[[d]] ) ) {
		ta[[d]][w] <- sample(1:K, 1) 

		# extract the topic index, word id and update the corresponding cell in the word-topic count matrix  
		ti <- ta[[d]][w]
		wi <- docs[[d]][w]
		wt[ti, wi] <- wt[ti, wi] + 1    
		# josh's comments- the initial value for wt[ti,wi] is 0, and now we update it to 1 because we assign a word to that topic. so the count of words increases to 1.
	}

	# count words in document d assigned to each topic t
  # Josh's comment-okay, dt is a container for topic-document count 
	for( t in 1:K ) {
		dt[d, t] <- sum( ta[[d]] == t )
	}
}

# randomly assigned topic to each word
print(ta)
print(wt)
print(dt)

```

> Notice that this random assignment gives you both the topic representations of all the documents and word distributions of all the topics (bad ones!!!). We need to improve this!! Optimize it!

> There are a couple of ways to do this. But we focus on Gibbs Sampling method that performs the following steps for a user-specified iteration:

> For each document d, go through each word w. Reassign a new topic to w from topic t with "the probability of word w given topic t" $\times$ "probability of topic t given document d", denoted by the following mathematical notations:

$$ P( z_i = j \text{ }| \text{ } z_{-i}, w_i, d_i ) 
    \propto \frac{ C^{WT}_{w_ij} + \eta }{ \sum^W_{ w = 1 }C^{WT}_{wj} + W\eta } \times
      \frac{ C^{DT}_{d_ij} + \alpha }{ \sum^T_{ t = 1 }C^{DT}_{d_it} + T\alpha }
$$

This formula is confusing! Let us talk bit by bit.

> Starting from the left side of the equal sign:

-   $P(z_i = j)$ : The probability that token i is assigned to topic j.
-   $z_{-i}$ : Represents topic assignments of all other tokens.
-   $w_i$ : Word (index) of the $i_{th}$ token.
-   $d_i$ : Document containing the $i_{th}$ token.

> For the right side of the equal sign:

-   $C^{WT}$ : Word-topic matrix, the `wt` matrix we generated.
-   $\sum^W_{ w = 1 }C^{WT}_{wj}$ : Total number of tokens (words) in each topic.
-   $C^{DT}$ : Document-topic matrix, the `dt` matrix we generated.
-   $\sum^T_{ t = 1 }C^{DT}_{d_it}$ : Total number of tokens (words) in document i.
-   $\eta$ : Parameter that sets the topic distribution for the words, the higher the more spread out the words will be across the specified number of topics (K).
-   $\alpha$ : Parameter that sets the topic distribution for the documents, the higher the more spread out the documents will be across the specified number of topics (K).
-   $W$ : Total number of words in the set of documents.
-   $T$ : Number of topics, equivalent of the K we defined earlier.

```{r}

# parameters 
alpha <- 1
eta <- 1

# initial topics assigned to the first word of the first document
# and its corresponding word id 
t0  <- ta[[1]][1]
wid <- docs[[1]][1]

# z_-i means that we do not include token w in our word-topic and document-topic count matrix when sampling for token w, only leave the topic assignments of all other tokens for document 1
dt[1, t0] <- dt[1, t0] - 1 
wt[t0, wid] <- wt[t0, wid] - 1

# Calculate left side and right side of equal sign
left  <- ( wt[, wid] + eta ) / ( rowSums(wt) + length(vocab) * eta )
right <- ( dt[1, ] + alpha ) / ( sum( dt[1, ] ) + K * alpha )

# draw new topic for the first word in the first document 
# The optional prob argument can be used to give a vector of weights for obtaining the elements of the vector being sampled. They need not sum to one, but they should be non-negative and not all zero.
t1 <- sample(1:K, 1, prob = left * right)
t1

```

> After the first iteration, the topic for the first word in the first document is updated to `r t1`.Just remember after drawing the new topic we need to update the topic assignment list with newly sampled topic for token w; re-increment the word-topic and document-topic count matrices with the new sampled topic for token w.
>
> We will use Ethen Liu's user-written function [`LDA1`][LDA] as a demo to run some interations, which takes the parameters of:

-   `docs` Document that have be converted to token (word) ids.
-   `vocab` Unique tokens (words) for all the document collection.
-   `K` Number of topic groups.
-   `alpha` and `eta` Distribution parameters as explained earlier.
-   `iterations` Number of iterations to run gibbs sampling to train our model.
-   Returns a list containing the final weight-topic count matrix `wt` and document-topic matrix `dt`.

```{r}

# define parameters
K <- 2 
alpha <- 1
eta <- 0.001
iterations <- 1000

source("LDA_functions.R")
set.seed(2020)
lda1 <- LDA1( docs = docs, vocab = vocab, 
			  K = K, alpha = alpha, eta = eta, iterations = iterations )
lda1

```

> After we're done with learning the topics for `r iterations` iterations, we can use the count matrices to obtain the word-topic distribution and document-topic distribution.
>
> To compute the probability of word given topic:

$$\beta_{ij} = \frac{C^{WT}_{ij} + \eta}{\sum^W_{ k = 1 }C^{WT}_{kj} + W\eta}$$

> Where $\beta_{ij}$ is the probability of word i for topic j.

```{r}

# topic probability of every word 
( beta <- ( lda1$wt + eta ) / ( rowSums(lda1$wt) + length(vocab) * eta ) )

```

$$\theta_{dj} = \frac{C^{DT}_{dj} + \alpha}{\sum^T_{ k = 1 }C^{DT}_{dk} + T\alpha}$$

Where $\theta_{dj}$ is the proportion of topic j in document d.

```{r}

# topic probability of every document
( theta <- ( lda1$dt + alpha ) / ( rowSums(lda1$dt) + K * alpha ) )

```

> Recall that LDA assumes that each document is a mixture of all topics, thus after computing the probability that each document belongs to each topic ( same goes for word & topic ) we can use this information to see which topic does each document belongs to and the more possible words that are associated with each topic. For more details on Gibbs Sampling, you can check Griffiths and Steyvers 2004 Finding Scientific topics.

```{r}

# topic assigned to each document, the one with the highest probability 
topic <- apply(theta, 1, which.max)

# possible words under each topic 
# sort the probability and obtain the user-specified number n
Terms <- function(beta, n) {
	term <- matrix(0, n, K)
	for( p in 1:nrow(beta) ) {
		term[, p] <- names( sort( beta[p, ], decreasing = TRUE )[1:n] )
	}
	return(term)
}
term <- Terms(beta = beta, n = 2)

```

> We specified that we wanted to see the top 2 terms associated with each topic. The following section prints out the original raw document, which is grouped into `r K` groups that we specified and words that are likely to go along with each topic.

```{r}

list( original_text = raw_docs[topic == 1], words = term[, 1] )
list( original_text = raw_docs[topic == 2], words = term[, 2] )

```

> The output tells us that the first topic seems to be discussing something about united states , while the second is something about food. It is still messy, not that intuitive. But at least it is a good starting point.

Now let us move to use the R library *topicmodels* to fit a LDA.

> Since the starting point of gibbs sampling is chosen randomly, thus it makes sense to discard the first few iteration ( also known as `burn-in` periods ). Due to the fact that they most likely do not correctly reflect the properties of distribution. And another parameter is `thin`, the number of iterations ommitted during the training. This serves to prevent correlations between samples during the iteration.
>
> We'll use the `LDA` function from the *topicmodels* library to implement gibbs sampling method on the same set of raw documents and print out the result for you to compare. Note that library has a default of value of 50 / K for $\alpha$ and 0.1 for $\eta$.

```{r, message=FALSE, warning=TRUE}

# load packages if not installed, using install.packages("topicmodels")
library(tm)
library(topicmodels)

# @burnin : number of omitted Gibbs iterations at beginning
# @thin : number of omitted in-between Gibbs iterations
docs1 <- Corpus( VectorSource(raw_docs) )
dtm <- DocumentTermMatrix(docs1)
# josh'cc- the input of LDA is a document-term matrix. You can use tm::DocumentTermMatrix to create it. Note you can also use tidytext package to do this. We will talk about it later when we fit a stm.
lda <- LDA( dtm, k = 2, method = "Gibbs", 
	   		control = list(seed = 2020, burnin = 500, thin = 100, iter = 4000) )

list( original_text = raw_docs[ topics(lda) == 1 ], words = terms(lda, 3)[, 1] )
list( original_text = raw_docs[ topics(lda) == 2 ], words = terms(lda, 3)[, 2] )

```

> Notice that after training the model for 4000 iterations and using a different $\alpha$ and $\eta$ value, we obtained a different document clustering result and different words that are more likely to associate with each topic. Since the goal here is to peform a clustering (unsupervised) method to unveil unknown patterns, the solutions will most likely differ as there is no such thing as a correct answer. We should try a range of different values of K to find the optimal topic grouping of the set of documents and see which result matches our intuition more.

# Structural Topic Model

> In this part we heavily rely on [stm's tutorial](http://structuraltopicmodel.com/) by Molly Roberts, Brandon Stewart and Dustin Tingley and an application by[Jula Silge] (<https://juliasilge.com/blog/sherlock-holmes-stm/>). We will go through their tutorial and show you how to do stm in R librabry stm.

Let us install stm first.

```{r, message=FALSE, warning=TRUE}
#library(devtools)
#install_github("bstewart/stm",dependencies=TRUE)
library(stm)
library(tidyverse)
```

We use the data from stm tutorial, it is about political blogs in 2008 <http://reports-archive.adm.cs.cmu.edu/anon/ml2010/CMU-ML-10-101.pdf>.

```{r, message=FALSE, warning=TRUE}
data <- read_csv(url("https://yongjunzhang.com/files/tutorial/poliblogs2008.csv"))
```

Before we run topic models like lda, we need to preprocess data. STM provides several functions to automatially do stemming, stopwords removal, low frequency words removal, etc for you.

Here is the graph of stm processors:

```{r, out.width="",out.height="",fig.align='center', fig.cap='STM process'}
knitr::include_graphics('figure2.png')

```

Let us use the textProcessor to preprocess texts. Here is the function:

> textProcessor(documents, metadata = NULL, lowercase = TRUE, removestopwords = TRUE, removenumbers = TRUE, removepunctuation = TRUE, ucp = FALSE, stem = TRUE, wordLengths = c(3, Inf), sparselevel = 1, language = "en", verbose = TRUE, onlycharacter = FALSE, striphtml = FALSE, customstopwords = NULL, custompunctuation = NULL, v1 = FALSE)

```{r}
#Preprocessing
#stemming/stopword removal, etc.
#Josh-cc, if you don't know the details of a function, you can use ? to check the documentation of that function. ?textProcessor
processed <- textProcessor(data$documents, metadata=data)
```

Let us use prepDocuments to perform several corpus manipulations including removing words and renumbering word indices. here is the function:

> prepDocuments(documents, vocab, meta = NULL, lower.thresh = 1, upper.thresh = Inf, subsample = NULL, verbose = TRUE)

```{r}
#before running prepDocuments, you can use plotRemoved function to check the appropriate threshold to remove words or docuemnts.
#take a look at how many words and documents would be removed with different lower.thresholds !!! check Error: could not find function "plotRemoved"
plotRemoved(processed$documents, lower.thresh=seq(1,200, by=100))
```

```{r}
#structure and index for usage in the stm model. Verify no-missingness. can remove low frequency words using 'lower.thresh' option. See ?prepDocuments for more info
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=1)
```

```{r}
#output will have object meta, documents, and vocab 
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
```

Now, let us use stm function fit a stm model.

> The function takes sparse representation of a document-term matrix, an integer number of topics, and covariates and returns fitted model parameters. Covariates can be used in the prior for topic prevalence, in the prior for topical content or both.

> stm(documents, vocab, K, prevalence = NULL, content = NULL, data = NULL, init.type = c("Spectral", "LDA", "Random", "Custom"), seed = NULL, max.em.its = 500, emtol = 1e-05, verbose = TRUE, reportevery = 5, LDAbeta = TRUE, interactions = TRUE, ngroups = 1, model = NULL, gamma.prior = c("Pooled", "L1"), sigma.prior = 0, kappa.prior = c("L1", "Jeffreys"), control = list())

```{r}
#run an stm model using the 'out' data. 20 topics. Asking how prevalaence of topics varies across documents' meta data, including 'rating' and day. !! option s(day) applies a spline normalization to day variable.

# max.em.its should be at least 100. We use 15 just as demo
poliblogPrevFit <- stm(out$documents,out$vocab,K=20,prevalence =~ rating+ s(day), max.em.its=15, data=out$meta,seed=2020)

```

Like LDA, stm also need to specify the number of topics or themes (K) before fitting. Fortunately, stm provides a function selectModel to help you select the models with high likelihood values.

> selectModel(documents, vocab, K, prevalence = NULL, content = NULL, data = NULL, max.em.its = 100, verbose = TRUE, init.type = "LDA", emtol = 1e-05, seed = NULL, runs = 50, frexw = 0.7, net.max.em.its = 2, netverbose = FALSE, M = 10, N = NULL, to.disk = F, ...)

```{r}
#let STM help you compare a number of models side by side. It will keep the models that don't stink (i.e. that converge quickly) 
poliblogSelect <- selectModel(out$documents,out$vocab,K=20,prevalence =~ rating+s(day), max.em.its=15,data=meta,runs=20,seed=2020)

#plot the different models that make the cut along exclusivity and semantic coherence of their topics
plotModels(poliblogSelect)

#the 3rd one looks best, so choose it and give it the name poliblogPrevFit
poliblogPrevFit<-poliblogSelect$runout[[3]] #choose the third model
```

Now it is time to intepret the stm model.

```{r}
###LIST OF TOP WORDS for topics 1, 7, & 10
labelTopics(poliblogPrevFit, c(1, 7, 10))
```
Let us do a wordcloud, but I am not suggesting you to do this in your published research.
```{r}
###WORDCLOUD for a specified TOPIC
cloud(poliblogPrevFit, topic=7)
```
Let us find some texts that are most representative for a particular topic using findThoughts function:

> Outputs most representative documents for a particular topic. Use this in order to get a better sense of the content of actual documents with a high topical content.

> findThoughts(model, texts = NULL, topics = NULL, n = 3,
  thresh = NULL, where = NULL, meta = NULL)

```{r}
#object 'thoughts1' contains 2 documents about topic 1. 'texts=shortdoc,' gives you just the first 250 words
data <- data %>% 
  mutate(shortdoc=str_extract(documents,"^.{250}")) 
thoughts1 <- findThoughts(poliblogPrevFit,
                          texts=data$shortdoc,
                          n=2,
                          topics=1)$docs[[1]]
#will show you the output
plotQuote(thoughts1, width=40, main="Topic 1")
```
Let use find more documents for topics
```{r}
#how about more documents for more of these topics?
thoughts7 <- findThoughts(poliblogPrevFit, 
                          texts=data$shortdoc, 
                          n=2, 
                          topics=7)$docs[[1]]
thoughts10 <- findThoughts(poliblogPrevFit, 
                           texts=data$shortdoc,
                           n=2, 
                           topics=10)$docs[[1]]
thoughts4 <- findThoughts(poliblogPrevFit, 
                          texts=data$shortdoc,
                          n=2, 
                          topics=4)$docs[[1]]

#And in a 2X2 table? We like 2X2 tables!  --- Note: this command will force all remaining plots into a 2X2 table format
par(mfrow = c(2, 2),mar=c(.5,.5,1,.5)) 
plotQuote(thoughts1, width=40, main="Topic 1")
plotQuote(thoughts4, width=40, main="Topic 4")
plotQuote(thoughts7, width=40, main="Topic 7")
plotQuote(thoughts10, width=40, main="Topic 10")
```
Let us see PROPORTION OF EACH TOPIC in the entire CORPUS.

```{r}
## Just insert your STM output
plot.STM(poliblogPrevFit, type="summary", n=5,xlim=c(0,.4))
```

Let us see how topics are correlated...
```{r}
##see GRAPHICAL NETWORK DISPLAY of how closely related topics are to one another, (i.e., how likely they are to appear in the same document) Requires 'igraph' package
mod.out.corr<-topicCorr(poliblogPrevFit)
plot.topicCorr(mod.out.corr)
```

Let use see topical content by covariates
```{r}
##VISUALIZE DIFFERENCES BETWEEN TWO DIFFERENT TOPICS using the ,type="perspectives" option
plot.STM(poliblogPrevFit,type="perspectives", topics=c(9, 10))
```

Let see how prevalence of topics varies across documents based on document covariates.

```{r}

###See CORRELATIONS BTWN METADATA & TOPIC PREVALANCE in documents
###First, must estimate an effect of the metadata covariates on topic prevalence in a document, so that we have anything to plot
###Estimating the expected proportion of a document that belongs to a topic as a function of a covariate
#Right-quick, for this vignette, we need to tell R to convert the 'rating' variable in the meta file (which is currently a string variable) into a categorical variable
meta$rating<-as.factor(meta$rating)

#since we're preparing these coVariates by estimating their effects we call these estimated effects 'prep'
#we're estimating Effects across all 20 topics, 1:20. We're using 'rating' and normalized 'day,' using the topic model poliblogPrevFit. 
#The meta data file we call meta. We are telling it to generate the model while accounting for all possible uncertainty. Note: when estimating effects of one covariate, others are held at their mean
prep <- estimateEffect(1:20 ~ rating+s(day),poliblogPrevFit,meta=meta, uncertainty = "Global")

###See how PREVALENCE of TOPICS DIFFERS across VALUES of a CATEGORICAL COVARIATE  
plot.estimateEffect(prep, covariate = "rating", topics = c(1, 7, 10),
                    #topic model=poliblogPrevFit. Method="difference" 
                    model=poliblogPrevFit, method="difference",
                    #only using two values of covariate, and labeling them... assume we could do this with a non-binary covariate and just specify
                    cov.value1="Liberal",cov.value2="Conservative",
                    xlab="More Conservative ... More Liberal",
                    main="Effect of Liberal vs. Conservative",
                    xlim=c(-.1,.1), labeltype = "custom",
                    custom.labels = c('Jeremiah Wright', 'Sarah Palin',
                                      'Bush Presidency'))
```

```{r}
#See how PREVALENCE of TOPICS DIFFERS across VALUES of a CONTINUOUS COVARIATE
#plotting prep data on day variable, a continuous variable with a continous plot. focusing on topic 7. not sure what model= z means !!! (removal has no effect on image) will want to do this for protester topics a lot!
plot.estimateEffect(prep, "day", method="continuous", topics=7, model=z,
                    printlegend=FALSE, xaxt="n", xlab="Time (2008)")
monthseq <- seq(from=as.Date("2008-01-01"),
                to=as.Date("2008-12-01"), by="month")
monthnames <- months(monthseq)
axis(1,at=as.numeric(monthseq)-min(as.numeric(monthseq)),
     labels=monthnames)
```

Let us see how words of the topics are emphasized differently across documents according to document covariates

```{r}
#### Instead of looking at how prevalent a topic is in a class of documents categorized by meta-data covariate... 
#### ... let's see how the words of the topic are emphasized differently in documents of each category of the covariate
##First, we we estimate a new stm. It's the same as the old one, including prevalence option, but we add in a content option
poliblogContent <- stm(out$documents,out$vocab,K=20,
                       prevalence =~ rating+ s(day), content=~rating,
                       max.em.its=15, data=out$meta,seed=2020)
##Next, we plot using the ,type="perspectives" option to the plot.STM function
plot.STM(poliblogContent,type="perspectives", topics=10)
```

```{r}
###Interacting covariates. Maybe we have a hypothesis that cities with low $$/capita become more repressive sooner, while cities with higher budgets are more patient 
##first, we estimate an STM with the interaction
poliblogInteraction <- stm(out$documents,out$vocab,K=20,
                           prevalence =~ rating* day, max.em.its=15,
                           data=out$meta,seed=2020)
#Then, as above, we prep the covariates usine the estimateEffect() function -- but this time, we include the interaction variable.
prep <- estimateEffect(c(1) ~ rating*day, poliblogInteraction,
                       metadata=meta, uncertainty="None")
#Then, we use our plotting function
plot.estimateEffect(prep, covariate="day", model=poliblogInteraction,
                    method="continuous",xlab="Days", moderator="rating",
                    moderator.value="Liberal", linecol="blue", ylim=c(0,.08),
                    printlegend=F)
plot.estimateEffect(prep, covariate="day", model=poliblogInteraction,
                    method="continuous",xlab="Days", moderator="rating",
                    moderator.value="Conservative", linecol="red", add=T,
                    printlegend=F)
legend(0,.08, c("Liberal", "Conservative"),
       lwd=2, col=c("blue", "red"))
```

Now let us use supplement packages to visualize stm outputs.

> [stmprinter: Print multiple stm model dashboards to a pdf file for inspection](https://github.com/mikajoh/stmprinter). Beautiful automated reports from multiple stm runs.

> stminsights: A Shiny Application for Inspecting Structural Topic Models. A shiny GUI with beautiful graphics.

> themetagenomics: Exploring Thematic Structure and Predicted Functionality of 16s rRNA Amplicon Data. . STM for rRNA data.

> [tidystm: Extract (tidy) effects from estimateEffect](devtools::install_github(%22mikajoh/tidystm%22,%20dependencies%20=%20TRUE)). Makes it easy to make ggplot2 graphics for STM.

> stmgui: Shiny Application for Creating STM Models" . This is a Shiny GUI for running basic STM models.

> stmBrowser: An R Package for the Structural Topic Model Browser.'' This D3 visualization allows users to interactively explore the relationships between topics and the covariates estimated from the stm package in R.

> stmCorrViz: A Tool for Structural Topic Model Visualizations. This package uses D3 to generate an interactive hierarchical topic explorer.

```{r}
if (!requireNamespace("pacman"))
  install.packages('pacman')
library(pacman)
packages<-c("stmprinter","stminsights","themetagenomics", "tidystm","stmgui",
            "stmBrowser","stmCorrViz")
p_load(packages,character.only = TRUE)
#devtools::install_github("mikajoh/stmprinter")
#devtools::install_github("mikajoh/tidystm", dependencies = TRUE)
#devtools::install_github("mroberts/stmBrowser",dependencies=TRUE)
```

Let us use stmBrowser to visualize our topic models. Check here for more details <https://github.com/mroberts/stmBrowser>.
The major function is stmBrowser.

```{r}
stmCorrViz::stmCorrViz(poliblogPrevFit,
                       file_out = "lab3-viz.html",
                       documents_raw=data$shortdoc)

```
You can check here for the [generated html visualization file](https://yongjunzhang.com/files/tutorial/lab3-viz.html).

# Using TidyText with STM to Fit a Topic Model

> So far, we have walked through stm tutorial. Now let us switch back to tidytext. Again you can check tidymining book for details how to model stm.


let us make a document-term matrix first

```{r}
library(tidytext)

tidy_data <- data %>% 
  mutate(docid=row_number()) %>% 
  unnest_tokens(word,documents) %>% 
  anti_join(get_stopwords())

tidy_data %>%
    count(word, sort = TRUE)

```


What are the highest tf-idf words in our documents? Let us plot them

```{r}
data_tf_idf <- tidy_data %>%
    count(docid, word, sort = TRUE) %>%
    bind_tf_idf(word, docid, n) %>%
    arrange(-tf_idf) %>%
    group_by(docid) %>%
    top_n(10) %>%
    ungroup

data_tf_idf %>%
    filter(docid%in%c(10,100,500,1000,1500,5000,6000,10000)) %>% 
    mutate(word = reorder_within(word, tf_idf, docid)) %>%
    ggplot(aes(word, tf_idf, fill = docid)) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ docid, scales = "free", ncol = 2) +
    scale_x_reordered() +
    coord_flip() +
    theme(strip.text=element_text(size=11)) +
    labs(x = NULL, y = "tf-idf",
         title = "Highest tf-idf words in a Document")

```

let us get stm 

```{r}
library(quanteda)
library(stm)

dfm <- tidy_data %>%
    count(docid, word, sort = TRUE) %>%
    cast_dfm(docid, word, n)

topic_model <- stm(dfm, K = 20, 
                   verbose = FALSE, init.type = "Spectral")
```

let us plot the beta, which is topic-word distribution.
here is the plate graphical model of stm, in case you are not familiar with it.

```{r, out.width="",out.height="",fig.align='center', fig.cap='STM model'}
knitr::include_graphics('figure3.png')

```


```{r}
td_beta <- tidy(topic_model)

td_beta %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, ncol=4,scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "")
```

Now let plot gamma, the probability that each document is generated from each topic.

```{r}
td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(dfm))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 5) +
  labs(title = "Distribution of document probabilities for each topic",
       subtitle = "",
       y = "Number of documents", x = expression(gamma))
```


# Lab 2 Problem Set

Fit a stm model for the raw\_docs provided for LDA analysis at the beginning on Chinese food and American Football. Create a covar indicating the first 5 sentences as "China" and the second 5 sentences as "America."

Compare the difference in model outputs between lda and stm.

Send me a screen-shot before Tuesday 6 PM.

**The End**
