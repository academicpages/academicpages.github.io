---
layout: archive
title: ""
permalink: /readings/
author_profile: true
---

# ğŸ¤– Embodied Artificial Intelligence Seminar - Readings and Resources

Welcome to the CS6604 Embodied AI Seminar! 

Below is a list of topics we'll cover during the semester, along with recommended readings, and links to project pages or source code repositories where applicable.

For each paper, click on ğŸ“š for the PDF version and on ğŸŒ for additional resources.
<details>
  <summary><b>Topic 1: Benchmarks: Simulators, Environments, Datasets</b></summary>
  <ul>
    <li>ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes <a href="https://arxiv.org/abs/2304.04321">ğŸ“š</a> <a href="https://arnold-benchmark.github.io/">ğŸŒ</a></li>
    <li>iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes <a href="https://arxiv.org/abs/2012.02924">ğŸ“š</a> <a href="https://svl.stanford.edu/igibson/">ğŸŒ</a></li>
    <li>Matterport3D: Interpreting Visually-Grounded Navigation Instructions in Real Environments <a href="https://arxiv.org/abs/1711.07280">ğŸ“š</a> <a href="https://bringmeaspoon.org/">ğŸŒ</a></li>
    <li>CVDN: Vision-and-Dialog Navigation <a href="https://arxiv.org/abs/1907.04957">ğŸ“š</a></li>
    <li>Soundspaces: Audio-Visual Navigation in 3D Environments <a href="https://link.springer.com/chapter/10.1007/978-3-030-58539-6_2">ğŸ“š</a> <a href="https://vision.cs.utexas.edu/projects/audio_visual_navigation/">ğŸŒ</a></li>
    <li>AI2-THOR: An Interactive 3D Environment for Visual AI <a href="https://arxiv.org/abs/1712.05474">ğŸ“š</a> <a href="https://ai2thor.allenai.org/">ğŸŒ</a></li>
    <li>Rearrangement: A Challenge for Embodied AI <a href="https://arxiv.org/abs/2011.01975">ğŸ“š</a></li>
    <li>Visual Room Rearrangement <a href="https://arxiv.org/abs/2103.16544">ğŸ“š</a> <a href="https://ai2thor.allenai.org/rearrangement/">ğŸŒ</a></li>
    <li>ProcTHOR: Large-Scale Embodied AI Using Procedural AI Generation <a href="https://arxiv.org/abs/2206.06994">ğŸ“š</a> <a href="https://procthor.allenai.org/">ğŸŒ</a></li>
    <li>ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills <a href="https://arxiv.org/abs/2302.04659">ğŸ“š</a> <a href="https://maniskill2.github.io/">ğŸŒ</a></li>
    <li>Object Goal Navigation using Goal-Oriented Semantic Exploration <a href="https://arxiv.org/abs/2007.00643">ğŸ“š</a> <a href="https://devendrachaplot.github.io/projects/semantic-exploration.html">ğŸŒ</a></li>
    <li>Embodied Question Answering in Photorealistic Environments with Point Cloud Perception <a href="https://arxiv.org/abs/1904.03461">ğŸ“š</a> <a href="https://embodiedqa.org/">ğŸŒ</a></li>
    <li>Alfred: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Shridhar_ALFRED_A_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_CVPR_2020_paper.html">ğŸ“š</a> <a href="https://askforalfred.com/">ğŸŒ</a></li>
    <li>DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following <a href="https://arxiv.org/abs/2202.13330">ğŸ“š</a> <a href="https://github.com/xfgao/DialFRED">ğŸŒ</a></li>
    <li>Alexa Arena: A User-Centric Interactive Platform for Embodied AI <a href="https://arxiv.org/abs/2303.01586">ğŸ“š</a> <a href="https://github.com/amazon-science/alexa-arena">ğŸŒ</a></li>
    <li>VirtualHome: Simulating Household Activities via Programs <a href="https://arxiv.org/abs/1806.07011">ğŸ“š</a> <a href="http://virtual-home.org/">ğŸŒ</a></li>
    <li>BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000 Everyday Activities and Realistic Simulation <a href="https://proceedings.mlr.press/v205/li23a.html">ğŸ“š</a> <a href="https://behavior.stanford.edu/">ğŸŒ</a></li>
    <li>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge <a href="https://arxiv.org/abs/2206.08853">ğŸ“š</a> <a href="https://minedojo.org/">ğŸŒ</a></li>
  </ul>
</details>

<details>
  <summary><b>Topic 2: Conceptual Framing, World Models, Behavioral and Performance Metrics</b></summary>
  <ul>
    <li>World Models <a href="https://arxiv.org/abs/1803.10122">ğŸ“š</a> <a href="https://worldmodels.github.io/">ğŸŒ</a></li>
    <li>Machine Theory of Mind <a href="https://arxiv.org/abs/1802.07740">ğŸ“š</a> </li>
    <li>Collaborative World Models: An Online-Offline Transfer RL Approach <a href="https://arxiv.org/abs/2305.15260">ğŸ“š</a> </li>
    <li>Transformers are Sample-Efficient World Models <a href="https://arxiv.org/abs/2209.00588">ğŸ“š</a> </li>
    <li>Learning Temporally Abstract World Models without Online Experimentation <a href="https://proceedings.mlr.press/v202/freed23a/freed23a.pdf">ğŸ“š</a> </li>
    <li>Reward-Free Curricula for Training Robust World Models <a href="https://arxiv.org/abs/2306.09205">ğŸ“š</a> </li>
    <li>Recurrent World Models Facilitate Policy Evolution <a href="https://arxiv.org/abs/1809.01999">ğŸ“š</a> <a href="https://worldmodels.github.io/">ğŸŒ</a></li>
      <li>Discovering and Achieving Goals via World Models <a href="https://arxiv.org/abs/2110.09514">ğŸ“š</a> <a href="https://orybkin.github.io/lexa/">ğŸŒ</a></li>
    <li>Planning to Explore via Self-Supervised World Models <a href="https://arxiv.org/abs/2005.05960">ğŸ“š</a> <a href="https://ramanans1.github.io/plan2explore/">ğŸŒ</a></li>
    <li>Learning to Model the World with Language<a href="https://arxiv.org/abs/2308.01399">ğŸ“š</a> <a href="https://dynalang.github.io/">ğŸŒ</a></li>
    <li>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling <a href="https://arxiv.org/abs/2301.12050">ğŸ“š</a></li>
    <li>Dream to Control: Learning Behaviors by Latent Imagination <a href="https://arxiv.org/abs/1912.01603">ğŸ“š</a></li>
    <li>DayDreamer: World Models for Physical Robot Learning <a href="https://arxiv.org/abs/2206.14176">ğŸ“š</a> <a href="https://danijar.com/project/daydreamer/">ğŸŒ</a></li>
    <li>Mastering Diverse Domains through World Models <a href="https://arxiv.org/abs/2301.04104">ğŸ“š</a> <a href="https://danijar.com/project/dreamerv3/">ğŸŒ</a></li>
    <li>Mastering Atari with Discrete World Models <a href="https://arxiv.org/abs/2010.02193">ğŸ“š</a> <a href="https://danijar.com/project/dreamerv2/">ğŸŒ</a></li>
    <li>Masked World Models for Visual Control <a href="https://proceedings.mlr.press/v205/seo23a/seo23a.pdf">ğŸ“š</a> <a href="https://sites.google.com/view/mwm-rl">ğŸŒ</a></li>
    <li>Structured World Models from Human Videos <a href="https://www.roboticsproceedings.org/rss19/p012.pdf">ğŸ“š</a> <a href="https://human-world-model.github.io/">ğŸŒ</a></li>
    <li>Building Machines That Learn and Think Like People <a href="https://arxiv.org/abs/1604.00289">ğŸ“š</a></li>
    <li>Action and Perception as Divergence Minimization <a href="https://arxiv.org/abs/2009.01791">ğŸ“š</a></li>
    <li>Intrinsically Motivated Reinforcement Learning <a href="https://proceedings.neurips.cc/paper_files/paper/2004/file/4be5a36cbaca8ab9d2066debfe4e65c1-Paper.pdf">ğŸ“š</a> </li>
       <li>Decision Transformer: Reinforcement Learning via Sequence Modeling <a href="https://arxiv.org/abs/2106.01345">ğŸ“š</a> </li>
    <li>Curiosity-Driven Exploration of Learned Disentangled Goal Spaces <a href="http://proceedings.mlr.press/v87/laversanne-finot18a/laversanne-finot18a.pdf">ğŸ“š</a></li>
    <li>Encouraging and Evaluating Embodied Exploration <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2023_paper.pdf">ğŸ“š</a></li>
    <li>Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration <a href="https://proceedings.neurips.cc/paper/2020/file/274e6fcf4a583de4a81c6376f17673e7-Paper.pdf">ğŸ“š</a></li>
    <li>Learning to play with intrinsically-motivated, self-aware agents <a href="https://arxiv.org/abs/1802.07442">ğŸ“š</a></li>
    <li>On Evaluation of Embodied Navigation Agents <a href="https://arxiv.org/abs/1807.06757">ğŸ“š</a></li>
     <li>ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects <a href="https://arxiv.org/abs/2006.13171">ğŸ“š</a></li>
     <li>On the Evaluation of Vision-and-Language Navigation Instructions <a href="https://arxiv.org/abs/2101.10504">ğŸ“š</a></li>
     <li>A New Path: Scaling Vision-and-Language Navigation With Synthetic Instructions and Imitation Learning <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kamath_A_New_Path_Scaling_Vision-and-Language_Navigation_With_Synthetic_Instructions_and_CVPR_2023_paper.html">ğŸ“š</a></li>
     <li>Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation <a href="https://arxiv.org/abs/1905.12255">ğŸ“š</a></li>
     <li>Iterative Vision-and-Language Navigation  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Krantz_Iterative_Vision-and-Language_Navigation_CVPR_2023_paper.html">ğŸ“š</a></li>
    <li>GRIDTOPIX : Training Embodied Agents with Minimal Supervision <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jain_GridToPix_Training_Embodied_Agents_With_Minimal_Supervision_ICCV_2021_paper.pdf">ğŸ“š</a> <a href="https://unnat.github.io/gridtopix">ğŸŒ</a></li>
  <li>On the Limits of Evaluating Embodied Agent Model Generalization Using Validation Sets <a href="https://aclanthology.org/2022.insights-1.15.pdf">ğŸ“š</a></li>
  </ul>
</details>


<details>
  <summary><b>Topic 3: Learning about visual sensory information through interaction </b></summary>
  <ul>
    <li> Scene Graph Contrastive Learning for Embodied Navigation <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Singh_Scene_Graph_Contrastive_Learning_for_Embodied_Navigation_ICCV_2023_paper.pdf">ğŸ“š</a></li>
    <li> Learning Navigational Visual Representations with Semantic Map Supervision <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf">ğŸ“š</a></li>
    <li> Topological Semantic Graph Memory for Image-Goal Navigation <a href="https://proceedings.mlr.press/v205/kim23a/kim23a.pdf">ğŸ“š</a></li>
    <li> Object-Goal Visual Navigation via Effective Exploration of Relations among Historical Navigation States  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Object-Goal_Visual_Navigation_via_Effective_Exploration_of_Relations_Among_Historical_CVPR_2023_paper.pdf">ğŸ“š</a></li>
    <li> One-4-All: Neural Potential Fields for Embodied Navigation <a href="https://arxiv.org/pdf/2303.04011.pdf">ğŸ“š</a></li>
    <li> ğŸ… Emergence of Maps in the Memories of Blind Navigation Agents (ICLR'23 Outstanding Paper) <a href="https://iclr.cc/virtual/2023/oral/12560">ğŸ“š</a></li>
    <li> Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks <a href="https://arxiv.org/abs/1903.03878">ğŸ“š</a></li>
    <li>Graph Attention Memory for Visual Navigation <a href="https://arxiv.org/abs/1905.13315">ğŸ“š</a></li>
    <li>Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances <a href="https://arxiv.org/pdf/2211.15876.pdf">ğŸ“š</a></li>
    <li>Navigating to Objects Specified by Images <a href="https://arxiv.org/pdf/2304.01192.pdf">ğŸ“š</a> <a href="https://jacobkrantz.github.io/modular_iin">ğŸŒ</a></li>
    <li> TIDEE: Tidying Up Novel Rooms using Visuo-Semantic Commonsense Priors <a href="https://arxiv.org/abs/2207.10761">ğŸ“š</a> <a href="https://tidee-agent.github.io/">ğŸŒ</a></li>
    <li>Egocentric Planning for Scalable Embodied Task Achievement <a href="https://arxiv.org/pdf/2306.01295.pdf">ğŸ“š</a></li>
    <li> ALP: Action-Aware Embodied Learning for Perception <a href="https://arxiv.org/pdf/2306.10190.pdf">ğŸ“š</a></li>
    <li> Simple but Effective: CLIP Embeddings for Embodied AI <a href="https://arxiv.org/abs/2111.09888">ğŸ“š</a></li>
    <li> Continuous Scene Representations for Embodied AI <a href="https://arxiv.org/abs/2203.17251">ğŸ“š</a></li>
    <li> Graph-based Environment Representation for Vision-and-Language Navigation in Continuous Environments<a href="https://arxiv.org/pdf/2301.04352.pdf">ğŸ“š</a></li>
    <li> Learning Affordance Landscapes for Interaction Exploration in 3D Environments <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf">ğŸ“š</a></li>
   <li> PASTA: Pretrained Action-State Transformer Agents <a href="https://arxiv.org/abs/2307.10936">ğŸ“š</a></li>
  </ul>
</details>

<details>
  <summary><b>Topic 4: Learning about language and language-guided interaction </b></summary>
  <ul>
    <li> MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception <a href="https://arxiv.org/pdf/2312.07472.pdf">ğŸ“š</a></li>
    <li>Plan4MC: Skill reinforcement learning and planning for open-world Minecraft tasks <a href="https://arxiv.org/abs/2303.16563">ğŸ“š</a></li> 
    <li>VOYAGER: An Open-Ended Embodied Agent with Large Language Models <a href="https://arxiv.org/abs/2305.16291">ğŸ“š</a></li>
    <li>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents <a href="https://arxiv.org/pdf/2302.01560.pdf">ğŸ“š</a></li> 
    <li>Embodied Task Planning with Large Language Models <a href="https://arxiv.org/abs/2307.01848">ğŸ“š</a></li>
    <li> Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning <a href="https://arxiv.org/abs/2305.18499">ğŸ“š</a></li>
    <li>Chasing Ghosts: Instruction Following as Bayesian State Tracking  <a href="https://arxiv.org/pdf/1907.02022.pdf">ğŸ“š</a></li> 
    <li>Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Context-Aware_Planning_and_Environment-Aware_Memory_for_Instruction_Following_Embodied_Agents_ICCV_2023_paper.pdf">ğŸ“š</a></li>
    <li>SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation <a href="https://openreview.net/pdf?id=E5EoQqCVYX">ğŸ“š</a></li>
    <li>Building Cooperative Embodied Agents Modularly with Large Language Models <a href="https://arxiv.org/abs/2305.15695">ğŸ“š</a></li>
    <li>Asking Before Action: Gather Information in Embodied Decision Making with Language Models <a href="https://arxiv.org/pdf/2307.02485.pdf">ğŸ“š</a></li>
    <li>Language Models Meet World Models: Embodied Experiences Enhance Language Models <a href="https://arxiv.org/pdf/2305.10626.pdf">ğŸ“š</a></li>
    <li>DANLI: Deliberative Agent for Following Natural Language Instructions <a href="https://arxiv.org/pdf/2210.12485.pdf">ğŸ“š</a></li>
    <li>3D-LLM: Injecting the 3D World into Large Language Models <a href="https://arxiv.org/abs/2307.12981">ğŸ“š</a></li>
    <li>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought <a href="https://arxiv.org/abs/2305.15021">ğŸ“š</a></li>
    <li>PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World <a href="https://arxiv.org/abs/2106.00188">ğŸ“š</a></li>
    <li>Embodied Executable Policy Learning with Language-based Scene Summarization<a href="https://arxiv.org/pdf/2306.05696.pdf">ğŸ“š</a></li>
    <li>JARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models<a href="https://arxiv.org/abs/2311.05997">ğŸ“š</a></li>
    <li>See and Think: Embodied Agent in Virtual Environment<a href="https://arxiv.org/abs/2311.15209">ğŸ“š</a></li>
   <li>Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models<a href="https://arxiv.org/abs/2310.15127">ğŸ“š</a></li>
  </ul>
</details>

<details>
  <summary><b>Topic 5: Dive into robotics, Sim2Sim/Sim2Real Transfer, Embodied Communication, Multi-agent Embodied Collaboration</b></summary>
  <ul>
    <li> Eureka: Human-Level Reward Design via Coding Large Language Models <a href="https://arxiv.org/abs/2310.12931">ğŸ“š</a></li> 
    <li> PaLM-E: An Embodied Multimodal Language Model <a href="https://palm-e.github.io/assets/palm-e.pdf">ğŸ“š</a></li>
    <li> Learning Interactive Real-World Simulators <a href="https://arxiv.org/abs/2310.06114">ğŸ“š</a> <a href="https://universal-simulator.github.io/unisim/">ğŸŒ</a></li>
    <li> Open X-Embodiment: Robotic Learning Datasets and RT-X Models <a href="https://arxiv.org/abs/2310.08864">ğŸ“š</a> <a href="https://robotics-transformer-x.github.io/">ğŸŒ</a></li>
    <li> RT-2: Vision-Language-Action Models <a href="https://robotics-transformer2.github.io/assets/rt2.pdf">ğŸ“š</a> <a href="https://robotics-transformer2.github.io/">ğŸŒ</a></li>
    <li> Scaling Robot Learning with Semantically Imagined Experience <a href="https://arxiv.org/abs/2302.11550">ğŸ“š</a> <a href="https://diffusion-rosie.github.io/">ğŸŒ</a></li>
    <li> AR2-D2:Training a Robot Without a Robot <a href="https://arxiv.org/abs/2306.13818">ğŸ“š</a> <a href="www.ar2d2.site">ğŸŒ</a></li>
    <li> IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience<a href="https://arxiv.org/abs/2305.01098">ğŸ“š</a> <a href="https://www.joannetruong.com/projects/i2o.html">ğŸŒ</a></li>
    <li> VIMA: General Robot Manipulation with Multimodal Prompts <a href="https://vimalabs.github.io/assets/vima_paper.pdf">ğŸ“š</a> <a href="https://vimalabs.github.io/">ğŸŒ</a></li>
    <li> AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer <a href="https://arxiv.org/abs/2302.04903">ğŸ“š</a> <a href="https://irom-lab.github.io/AdaptSim/">ğŸŒ</a></li>
    <li> RoboCat: A self-improving robotic agent <a href="https://arxiv.org/abs/2306.11706">ğŸ“š</a> <a href="https://www.deepmind.com/blog/robocat-a-self-improving-robotic-agent">ğŸŒ</a></li>
    <li> Policy Stitching: Learning Transferable Robot Policies <a href="https://arxiv.org/abs/2309.13753">ğŸ“š</a> <a href="http://generalroboticslab.com/PolicyStitching/">ğŸŒ</a></li> 
    <li> EC2 : Emergent Communication for Embodied Control <a href="https://arxiv.org/abs/2304.09448">ğŸ“š</a></li>
    <li> Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Patel_Interpretation_of_Emergent_Communication_in_Heterogeneous_Collaborative_Embodied_Agents_ICCV_2021_paper.pdf">ğŸ“š</a></li>
    <li> Heterogeneous Embodied Multi-Agent Collaboration <a href="https://arxiv.org/abs/2307.13957">ğŸ“š</a> <a href="https://hetercol.github.io/">ğŸŒ
    </a></li>
    <li> Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments <a href="https://arxiv.org/pdf/2204.09667.pdf">ğŸ“š</a> <a href="https://jacobkrantz.github.io/sim-2-sim">ğŸŒ
    </a></li>
  </ul>
</details>


<details>
  <summary><b>Topic 6: Diffusion Policies</b></summary>
  <ul>
    <li>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion <a href="https://arxiv.org/pdf/2303.04137.pdf">ğŸ“š</a></li>
    <li>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration <a href="https://arxiv.org/pdf/2310.07896.pdf">ğŸ“š</a></li>
    <li>PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play <a href="https://openreview.net/forum?id=afF8RGcBBP">ğŸ“š</a></li>
    <li>Learning Universal Policies via Text-Guided Video Generation <a href="https://arxiv.org/pdf/2302.00111.pdf">ğŸ“š</a></li>
    <li>Compositional Foundation Models for Hierarchical Planning <a href="https://arxiv.org/pdf/2309.08587.pdf">ğŸ“š</a></li>
    <li>XSkill: Cross Embodiment Skill Discovery <a href="https://arxiv.org/pdf/2307.09955.pdf">ğŸ“š</a></li>
  </ul>
</details>


## Additional Resources
- ğŸ  [Course Syllabus](https://isminoula.github.io/cs6604FA23/)
- ğŸ—“ï¸ [Seminar Schedule](https://isminoula.github.io/cs6604FA23/#schedule)
- ğŸ§  [Reinforcement Learning Supplemental Reading](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)
- ğŸ“Š [Transformers Supplemental Reading](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)
- ğŸŒ Diffusion for [robotics](https://github.com/mbreuss/diffusion-literature-for-robotics) and [RL](https://github.com/opendilab/awesome-diffusion-model-in-rl)
