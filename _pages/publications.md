---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<!-- {% if site.author.googlescholar %}
  <div class="wordwrap">You can also find my articles on <a href="{{site.author.googlescholar}}">my Google Scholar profile</a>.</div>
{% endif %} -->

<!-- {% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->

## International Conferences (Peer-Reviewed)

-  **<u>Kazuki Yamauchi</u>**, Yusuke Ijima, and Yuki Saito<br>
**StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models**<br>
IEEE International Conference on Acoustics, Speech and Signal Processing (**IEEE ICASSP**), 2024. \[Poster presentation\]<br>
\[[arXiv](https://arxiv.org/abs/2311.16509)\] \[[demo](https://ntt-hilab-gensp.github.io/icassp2024stylecap/)\] \[[poster](/files/yamauchi24icassp_poster.pdf)\]



## Domestic Conferences

- **<u>å±±å†… ä¸€è¼</u>**, ä¸­ç”° äº˜, é½‹è—¤ ä½‘æ¨¹, çŒ¿æ¸¡ æ´‹<br>
**é›¢æ•£éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆéŸ³å£°åˆæˆã®ãŸã‚ã®éŸ³å£°ä¸»è¦³è©•ä¾¡å€¤äºˆæ¸¬ã«åŸºã¥ãdecodingæˆ¦ç•¥**<br>
æƒ…å ±å‡¦ç†å­¦ä¼šç ”ç©¶å ±å‘Š, Vol. 2024-SLP-152, No. 14, 2024å¹´6æœˆ. \[éŸ³å­¦ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ , ãƒã‚¹ã‚¿ãƒ¼ç™ºè¡¨\]<br>
ğŸ‰ **éŸ³å­¦ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ å„ªç§€ç™ºè¡¨è³** \[[link](https://www.ipsj.or.jp/award/musslp-award1.html)\]<br>
\[[pdf](/files/yamauchi24otogaku_paper.pdf)\] \[[poster](/files/yamauchi24otogaku_poster.pdf)\]


- **<u>å±±å†… ä¸€è¼</u>**, äº•å³¶ å‹‡ç¥, é½‹è—¤ ä½‘æ¨¹<br>
**StyleCap: éŸ³å£°ãŠã‚ˆã³è¨€èªã®è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãéŸ³å£°ã®ç™ºè©±ã‚¹ã‚¿ã‚¤ãƒ«ã«é–¢ã™ã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ**<br>
æ—¥æœ¬éŸ³éŸ¿å­¦ä¼š 2024å¹´æ˜¥å­£ç ”ç©¶ç™ºè¡¨ä¼š è¬›æ¼”è«–æ–‡é›†, 3-2-14, pp. 843--846, 2024å¹´3æœˆ. \[æ—¥æœ¬éŸ³éŸ¿å­¦ä¼š, å£é ­ç™ºè¡¨\]<br>
\[[pdf](/files/yamauchi24asjs_paper.pdf)\] \[[slide](/files/yamauchi24asjs_slide.pdf)\]


- **<u>å±±å†… ä¸€è¼</u>**, é½‹è—¤ ä½‘æ¨¹, çŒ¿æ¸¡ æ´‹<br>
**VQ-VAEã«åŸºã¥ãè§£é‡ˆå¯èƒ½ãªã‚¢ã‚¯ã‚»ãƒ³ãƒˆæ½œåœ¨å¤‰æ•°ã‚’ç”¨ã„ãŸå¤šæ–¹è¨€éŸ³å£°åˆæˆ**<br>
é›»å­æƒ…å ±é€šä¿¡å­¦ä¼šç ”ç©¶å ±å‘Š, SP2023-80, Vol. 123, No. 403, pp.220--225, 2024å¹´3æœˆ. \[éŸ³å£°ãƒ»éŸ³éŸ¿ãƒ»ä¿¡å·å‡¦ç†ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—, ãƒã‚¹ã‚¿ãƒ¼ç™ºè¡¨\]<br>
ğŸ‰ **SPç ”ç©¶ä¼šå­¦ç”Ÿãƒã‚¹ã‚¿ãƒ¼è³** \[[link](https://www.ieice.org/iss/sp/jpn/special/sp-poster-prize.html)\]<br>
\[[pdf](/files/yamauchi24sp03_paper.pdf)\] \[[poster](/files/yamauchi24sp03_poster.pdf)\]


- ç¹”ç”° æ‚ å¸Œ, **<u>å±±å†… ä¸€è¼</u>**, é½‹è—¤ ä½‘æ¨¹, çŒ¿æ¸¡ æ´‹<br>
**ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚°ã§åé›†ã—ãŸæ–¹è¨€ã‚¢ã‚¯ã‚»ãƒ³ãƒˆãƒ©ãƒ™ãƒ«ã«åŸºã¥ã End-to-End æ—¥æœ¬èªéŸ³å£°åˆæˆã®æ–¹è¨€é©å¿œ**<br>
é›»å­æƒ…å ±é€šä¿¡å­¦ä¼šç ”ç©¶å ±å‘Š, Vol. 123, No. 403, 2024å¹´3æœˆ. \[éŸ³å£°ãƒ»éŸ³éŸ¿ãƒ»ä¿¡å·å‡¦ç†ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—, ã‚·ãƒ§ãƒ¼ãƒˆã‚ªãƒ¼ãƒ©ãƒ«\]


- **<u>å±±å†… ä¸€è¼</u>**, é½‹è—¤ ä½‘æ¨¹, çŒ¿æ¸¡ æ´‹<br>
**ã‚¢ã‚¯ã‚»ãƒ³ãƒˆæ½œåœ¨å¤‰æ•°ã®äºˆæ¸¬ã¨åˆ¶å¾¡ãŒå¯èƒ½ãªTTSãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ–¹è¨€éŸ³å£°åˆæˆã®æ¤œè¨**<br>
æ—¥æœ¬éŸ³éŸ¿å­¦ä¼š 2023å¹´ç§‹å­£ç ”ç©¶ç™ºè¡¨ä¼š è¬›æ¼”è«–æ–‡é›†, 2-Q-30, pp. 1255--1256, 2023å¹´9æœˆ. \[æ—¥æœ¬éŸ³éŸ¿å­¦ä¼š, ãƒã‚¹ã‚¿ãƒ¼ç™ºè¡¨\]<br>
\[[pdf](/files/yamauchi23asja_paper.pdf)\] \[[poster](/files/yamauchi23asja_poster.pdf)\]



## Preprint

- Wataru Nakata\*, **<u>Kazuki Yamauchi</u>**\*, Dong Yang, Hiroaki Hyodo, and Yuki Saito (\***Equal contribution**)<br>
**UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge**<br>
Technical Report for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge, Mar. 2024.<br>
ğŸ‰ **Ranked 1st in TTS (Acoustic+Vocoder) track** \[[link](https://huggingface.co/spaces/discrete-speech/interspeech2024_discrete_speech_tts_full)\]<br>
\[[arXiv](https://arxiv.org/abs/2403.13720)\] \[[code](https://huggingface.co/sarulab-speech/UTDUSS-Vocoder)\]
