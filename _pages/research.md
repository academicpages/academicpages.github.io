---
layout: archive
title: ""
excerpt: ""
author_profile: true
permalink: /research/
---

## Research

**VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives**  
Zhuofan Ying, Peter Hase, Mohit Bansal  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/2206.11212.pdf) [[code]](https://github.com/zfying/visfis)

**When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data**  
Peter Hase, Mohit Bansal  
*ACL 2022 Workshop on Natural Language Supervision.* [[pdf v2]](https://peterbhase.github.io/files/when-expl-help-LNLS-ACL2022.pdf) [[pdf v1]](https://arxiv.org/pdf/2102.02201.pdf) [[code]](https://github.com/peterbhase/ExplanationRoles)

**GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models**  
Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/2203.07281.pdf) [[code]](https://github.com/archiki/GrIPS)

**Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs**  
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/2111.13654.pdf) [[code]](https://github.com/peterbhase/SLAG-Belief-Updating)

**Low-Cost Algorithmic Recourse for Users With Uncertain Cost Functions**  
Prateek Yadav, Peter Hase, Mohit Bansal  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/2111.01235.pdf) [[code]](https://github.com/prateeky2806/EMC-COLS-recourse)  

**The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations**  
Peter Hase, Harry Xie, Mohit Bansal  
*NeurIPS 2021.* [[pdf]](https://arxiv.org/pdf/2106.00786.pdf) [[code]](https://github.com/peterbhase/ExplanationSearch)  

**FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging**  
Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, Caiming Xiong  
*EMNLP 2021.* [[pdf]](https://arxiv.org/pdf/2012.15781.pdf) [[code]](https://github.com/salesforce/fast-influence-functions)  

**Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?**  
Peter Hase, Shiyue Zhang, Harry Xie, Mohit Bansal  
*Findings of EMNLP.* [[pdf]](https://arxiv.org/pdf/2010.04119.pdf) [[code]](https://github.com/peterbhase/LAS-NL-Explanations)  

**Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?**  
Peter Hase, Mohit Bansal  
*ACL 2020.* [[pdf]](https://arxiv.org/pdf/2005.01831.pdf) [[code]](https://github.com/peterbhase/InterpretableNLP-ACL2020)  

**Interpretable Image Recognition with Hierarchical Prototypes**  
Peter Hase, Chaofan Chen, Oscar Li, Cynthia Rudin  
*AAAI-HCOMP 2019.* [[pdf]](https://arxiv.org/pdf/1906.10651.pdf) [[code]](https://github.com/peterbhase/interpretable-image)  

**Shall I Compare Thee to a Machine-Written Sonnet? An Approach to Algorithmic Sonnet Generation**  
John Benhardt, Peter Hase, Liuyi Zhu, Cynthia Rudin  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/1811.05067.pdf) [[code]](https://github.com/peterbhase/poetry-generation)  



