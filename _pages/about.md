---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# About Me
I am a 3rd year PhD student in the [Computer Science department](https://cse.ucsd.edu/) at the [University of California San Diego](https://ucsd.edu/) advised by Prof. [Sanjoy Dasgupta](https://cseweb.ucsd.edu/~dasgupta/). I'm also closely collaborating with Prof. [Misha Belkin](http://misha.belkin-wang.org/). Previously, I was a research fellow in the [Machine Teaching Group](https://machineteaching.mpi-sws.org/adishsingla.html) at [Max Planck Institute for Software Systems](https://www.mpi-sws.org/), where I had the great fortune to be advised by Dr. [Adish Singla](https://machineteaching.mpi-sws.org/adishsingla.html). 

My journey into computer science started at [Chennai Mathematical Institute (CMI, India)](https://www.cmi.ac.in/), where I completed my BSc in Mathematics and Computer Science (2013-2016) and MSc in Computer Science (2016-2018) under the supervision of Prof. [K Venkata Subrahmanyam](https://www.cmi.ac.in/~kv/).<br>

I am broadly interested in the theoretical aspects of machine learning. More specifically, I'm interested in statistical machine learning, algorithms, interactive learning, optimization, and theory of deep learning. I'm enthusiastic to explore and apply ideas from probability theory, analysis, differential geometry, and statistics to understand 
the computational and statistical efficiency of learning methods, and the extent to which machines can learn from data.<br>

**Contact**: (username id) akk002 at ucsd dot edu

<i> I'm looking for a research/engineer position for summer internship 2024. Please request CV via email. </i>

# Recent News
1. [June-Sept, 2023] I was a research scientist intern at Adobe Research (San Jose, CA).
1. [Aug, 2022] Attended the [Deep learning theory workshop](https://simons.berkeley.edu/workshops/deep-learning-theory-workshop) at Simons Institute, UC Berkeley.
1. [July, 2022] Attended a summer school on Discrete Mathematics at Charles University, Prague (CZK).

# Publications and Preprints

1. <b> Mirror Descent on Reproducing Kernel Banach Space (RKBS) </b> <br>
**Akash Kumar**, Parthe Pandit, Misha Belkin <br>
<i>In preparation.</i>

2. <b> Convergence of Nearest Neighbor Selective Classification </b> <br>
**Akash Kumar**, Sanjoy Dasgupta<br>
<i>In preparation.</i>

3. <b> Robust Empirical Risk Minimization with Tolerance </b> <br>
Robi Bhattacharjee, Kamalika Chaudhuri, Max Hopkins, **Akash Kumar**, Hantao Yu (alphabetical order)<br>
<i> Accepted in The 34th International Conference on Algorithmic Learning Theory (ALT'23), 2023
</i> <br>A preliminary version appeared in AdvML Frontiers @ ICML 2022<br>
[[ArXiv 2023](https://arxiv.org/abs/2210.00635)]

4. <b> Teaching via Best-Case Counterexamples
in the Learning-with-Equivalence-Queries Paradigm </b> <br>
**Akash Kumar**, [Yuxin Chen](https://yuxinchen.org/), [Adish Singla](https://machineteaching.mpi-sws.org/adishsingla.html).<br>
<i> Accepted in The 35th Conference on Neural Information Processing Systems (NeurIPS'21), 2021
</i> <br> [[Proc 2021](https://papers.nips.cc/paper/2021/file/e22dd5dabde45eda5a1a67772c8e25dd-Paper.pdf)], [[Openreview](https://openreview.net/forum?id=Ee7IOrpLwT)]<br>

5. <b> The Teaching Dimension of Kernel Perceptrons </b> <br>
**Akash Kumar**, Hanqi Zhang, [Adish Singla](https://machineteaching.mpi-sws.org/adishsingla.html), [Yuxin Chen](https://yuxinchen.org/).<br>
<i> Accepted in The 24th International Conference on Artificial Intelligence and Statistics (AISTATS'21), 2021 
</i> <br> [[ArXiv 2021](https://arxiv.org/pdf/2010.14043.pdf)], [[Proc 2021](http://proceedings.mlr.press/v130/kumar21a.html)]<br>

6. <b> Average-case Complexity of Teaching Convex Polytopes via Halfspace Queries </b> <br>
**Akash Kumar**, [Adish Singla](https://machineteaching.mpi-sws.org/adishsingla.html), [Yisong Yue](http://www.yisongyue.com/), [Yuxin Chen](https://yuxinchen.org/).<br> 
[[ArXiv 2020](https://arxiv.org/abs/2006.14677)]
<br><i>Rejected from ICML 2021 with 6 [reviews](https://akashkumar-d.github.io/files/ICML'21.pdf)</i><br>
<i>Rejected from NeurlPS 2020 with 4 [reviews](https://akashkumar-d.github.io/files/NeurIPS'20.pdf)<br></i>


7. <b> Deletion to Induced Matching </b> <br>
**Akash Kumar**, Mithilesh Kumar.<br> 
[[ArXiv 2020](https://arxiv.org/abs/2008.09660)]

# Talks
Feature Learning in Large Language Models (Adobe Research, San Jose)<br>
Teaching via Best-case Counterexamples (UCSD AI Seminar)

  
# Some notes
<b> [Improved Certified Adversarial Lower Bound Using Adaptive Relaxations](https://drive.google.com/file/d/1lZmiU3NnEhWHOtVuGhURxeFS4DWaYP_n/view?usp=sharing) </b> <br>
<i>Ongoing project on adversarial deep learning.</i>

<b> [Escaping Saddle Points and Tensor Decomposition](https://drive.google.com/file/d/1MAcwvvqGJCmr4VCnvE0kCFSTUB8w4mSA/view?usp=sharing) </b> <br>
<i>Master's Thesis under the guidance of Dr. K V Subrahmanyam. [[Slides](https://drive.google.com/file/d/1X4wGdlJvXqvzu-4C4qRFSEkSxy3ZF4Bg/view?usp=sharing)]</i>

<b> [Natural Proofs Vs Derandomization](https://drive.google.com/file/d/1TeHXyLIIUfp0p4iPqRqgNKwUx92ZO0Qn/view?usp=sharing) </b> <br>
<i>Project report completed as part of the Advanced Complexity course at Chennai Mathematical Institute.</i>
 
