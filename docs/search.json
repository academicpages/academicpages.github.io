[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\n\nWelcome to my website!\nHi, thanks for stopping by.\nI am a machine learning research at Arena AI where we are working on generative models of customer behavior to help build autonomous, self-learning systems at large enterprise companies. Previously, I have worked on economic models of competition and network growth as a research scientist at Facebook, on advertising, pricing, and marketplace growth as a senior machine learning engineer at Instacart (L6), and on causal inference and advertiser behavior at Microsoft Research.\nI earned a PhD in Economics at Stanford Graduate School of Business, advised by Susan Athey, Guido Imbens, and Wesley Hartmann. My research focused on developing new methods for causal inference and machine learning, with applications to marketing and economics. In my spare time, I teach an online class on Applied Causal Inference, focused on applications of ML and causal inference in the tech industry.\nI live in Brooklyn, where I enjoy cooking, swing dancing, and playing with my dog, Peanut.\nYou can contact me at rndonnelly@gmail.com."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Welfare Effects of Personalized Rankings\n\n\n\n\n\nDeveloped a novel model to measure the impact of personalized search results on consumer welfare\n\n\n\n\n\n\nMay 15, 2023\n\n\nRobert Donnelly, Ayush Kanodia, and Ilya Morozov\n\n\n\n\n\n\n  \n\n\n\n\nCounterfactual inference for consumer choice across many product categories\n\n\n\n\n\nDeveloped a variational Bayesian model of customer behavior in grocery stores to understand how they substitute between products when prices and product availability changes. Validated the ability of the model to make counterfactual predictions using held out data from weeks that experienced changes in pricing or out-of-stock products.\n\n\n\n\n\n\nNov 17, 2021\n\n\nRob Donnelly, Fran Ruiz, David Blei, and Susan Athey\n\n\n\n\n\n\n  \n\n\n\n\nEstimating heterogeneous consumer preferences for restaurants and travel time using mobile location data\n\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2018\n\n\nAthey, Susan, David Blei, Robert Donnelly, Francisco Ruiz, and Tobias Schmidt\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/18-restaurant_choice.html",
    "href": "research/18-restaurant_choice.html",
    "title": "Estimating heterogeneous consumer preferences for restaurants and travel time using mobile location data",
    "section": "",
    "text": "We estimate a model of consumer choices over restaurants using data from thousands of anonymous mobile phone users. We analyze how consumers reallocate their demand after a restaurant closes and make predictions about what types of restaurants would attract the most customers in a given location.\nDownload paper here"
  },
  {
    "objectID": "research/23-personalized_rankings.html",
    "href": "research/23-personalized_rankings.html",
    "title": "Welfare Effects of Personalized Rankings",
    "section": "",
    "text": "Developed a novel latent-factorization model in PyTorch for customer search and used it to measure the impact of personalized search research on customer welfare based on data from an experiment run at Wayfair.\nDownload paper here"
  },
  {
    "objectID": "research/21-counterfactual_predictions.html",
    "href": "research/21-counterfactual_predictions.html",
    "title": "Counterfactual inference for consumer choice across many product categories",
    "section": "",
    "text": "Developed a variational Bayesian model of customer behavior in grocery stores to understand how they substitute between products when prices and product availability changes. Validated the ability of the model to make counterfactual predictions using held out data from weeks that experienced changes in pricing or out-of-stock products.\nDownload paper here"
  },
  {
    "objectID": "posts/2023-01-15-simple-x-learner/2023-01-15-simple-x-learner.html",
    "href": "posts/2023-01-15-simple-x-learner/2023-01-15-simple-x-learner.html",
    "title": "A Simpler Alternative to X-Learner for Uplift Modeling",
    "section": "",
    "text": "Meta-learners like S-Learner, T-Learner, and X-Learner are some of the most widely used approaches for Uplift modeling. When teaching about these approaches, I find that students often find the X-learner model somewhat confusing to understand. In this post, I describe a modified approach I call simplified X-learner (Xs-learner) that is easier to understand, faster to implement, and in my experience often works as well or better in practice."
  },
  {
    "objectID": "posts/2023-01-15-simple-x-learner/2023-01-15-simple-x-learner.html#uplift-modeling",
    "href": "posts/2023-01-15-simple-x-learner/2023-01-15-simple-x-learner.html#uplift-modeling",
    "title": "A Simpler Alternative to X-Learner for Uplift Modeling",
    "section": "Uplift Modeling",
    "text": "Uplift Modeling\nA/B testing is a common method used at tech companies to make informed decisions. For example, imagine you want to send out a coupon to users and you want to know how much it will increase the chances of them completing their first order with your service. By running an A/B test, you can determine on average how effective the coupon is. However, you may also want to know which users the coupon will help you generate higher profits and which users the coupon will cause you to lose money.\nUplift modeling is a technique that lets us go beyond learning the average effect of a treatment and instead helps us understand how the effect of the treatment varies across your users. This allows us to more efficiently decide which treatment to send to each user."
  },
  {
    "objectID": "posts/2023-01-15-simple-x-learner/2023-01-15-simple-x-learner.html#meta-learners",
    "href": "posts/2023-01-15-simple-x-learner/2023-01-15-simple-x-learner.html#meta-learners",
    "title": "A Simpler Alternative to X-Learner for Uplift Modeling",
    "section": "Meta-learners",
    "text": "Meta-learners\nSome of the most common approaches for solving uplift problems are known as meta-learners, because they are ways to take existing supervised learning algorithms and using their predictions in order to make estimates of the treatment effect for each user.\nI’ll be demonstrating each of these approaches using a dataset from Lenta, a large Russian grocery store that sent out text messages to their users and saw whether it would increase their probability of making a purchase. In each of the examples I will be using the following notation:\n\nY: Did the user make a purchase (the outcome variable)\nT: Did the user receive a text message(the treatment variable)\nX: All the other information we know about the user, e.g. age, gender, purchase history. (The Lenta dataset has almost 200 features describing each user)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklift.datasets import fetch_lenta\nfrom sklift.viz import plot_qini_curve\n\nfrom numpy.random import default_rng\nrng = default_rng()\nWe’ll use the sklift package, which has a useful function that helps download the data for the Lenta uplift experiment and do some basic processing of the data.\ndata = fetch_lenta()\nY = data['target_name']\nX = data['feature_names']\n\ndf = pd.concat([data['target'], data['treatment'], data['data']], axis=1)\ngender_map = {'Ж': 0, 'М': 1}\ngroup_map = {'test': 1, 'control': 0}\ndf['gender'] = df['gender'].map(gender_map)\ndf['treatment'] = df['group'].map(group_map)\nT = 'treatment'\n\n# Split our data into a training and an evaluation sample\n\ndf_train, df_test = train_test_split(df, test_size=0.3, random_state=42)\n\nS-Learner\nS-learner is the simplest and easiest to understand of these approaches. With S-learner you fit a single machine learning model using all of your data, with the treatment variable (did you get a text message) as one of the features. You can then use this model to predict “what would happen if the user got the text” and “what would happen if the user did not get the text”. The difference between these two predictions is your estimate of the treatment effect of the text message on the user.\nIn all my examples, I use XGBoost as a simple and effective baseline ML model that is fast to train and generally works well on many problems. In any real world problem you should be testing more than one type of model and should be doing cross validation to find hyperparameters that work well for your particular problem.\nslearner = XGBClassifier()\nslearner.fit(df_train[X+[T]], df_train[Y])\n\n# Calculate the difference in predictions when T=1 vs T=0\n# This is our estimate of the effect of the coupon for each user in our data\nslearner_te = slearner.predict_proba(df_test[X].assign(**{T: 1}))[:, 1] \\\n            - slearner.predict_proba(df_test[X].assign(**{T: 0}))[:, 1]\nOne downside of the S-learner model is that there is nothing that tells the model to give special attention to the treatment variable. This means that often your machine learning model will focus on other variables that are stronger predictors of the outcome and end up ignoring the effect of the treatment. This means that on average your estimates of the treatment will be biased towards 0.\n\n\n\nS-learner treatment effect distribution\n\n\n\n\nT-learner\nT-learner uses two separate models. The first model looks only at the users who did not receive the coupon. The second model looks only at the users who did receive the coupon. To predict the treatment effect, we take the difference between the predictions of these two models. T-learner essentially forces your models to pay attention to the treatment variable since you make sure that each of the models only focuses on either the treated or untreated observations in your data.\ntlearner_0 = XGBClassifier()\ntlearner_1 = XGBClassifier()\n\n\n# Split data into treated and untreated\ndf_train_0 = df_train[df_train[T] == 0]\ndf_train_1 = df_train[df_train[T] == 1]\n\n# Fit the models on each sample\ntlearner_0.fit(df_train_0[X], df_train_0[Y])\ntlearner_1.fit(df_train_1[X], df_train_1[Y])\n\n# Calculate the difference in predictions\ntlearner_te = tlearner_1.predict_proba[df_test[X]](:, 1) \\\n            - tlearner_0.predict_proba[df_test[X]](:, 1)\n\n\n\nT-learner treatment effect distribution\n\n\n\n\nSimplified X-learner (Xs-learner)\nThe simplified X-learner use 3 models to form its predictions. The first two are exactly the same models we used for T-learner: one model trained only using the treated observations, and the other model trained using only the untreated observations.\nWith T-learner we formed our treatment effect estimates by taking the difference between the predictions of these two models (predicted outcome when treated minus predicted outcome when untreated). The Xs-learner takes the actual outcome of the user under the treatment their received and compares that to the predicted outcome if they received the other treatment (actual outcome minus predicted outcome).\n# We could also just reuse the models we made for the T-learner\nxlearner_0 = XGBClassifier()\nxlearner_1 = XGBClassifier()\n\n# Split data into treated and untreated\ndf_train_0 = df_train[df_train[T] == 0]\ndf_train_1 = df_train[df_train[T] == 1]\n\n# Fit the models on each sample\nxlearner_0.fit(df_train_0[X], df_train_0[Y])\nxlearner_1.fit(df_train_1[X], df_train_1[Y])\n\n# Calculate the difference between actual outcomes and predictions\nxlearner_te_0 = xlearner_1.predict_proba[df_train_0[X]](:, 1) - df_train_0[Y]\nxlearner_te_1 = df_train_1[Y] - xlearner_0.predict_proba[df_train_1[X]](:, 1)\nWe can’t use these differences directly, because we would not be able to make predictions for any new users since we wouldn’t know the actual outcomes for these new users. So we need to train one more model. This model predicts the treatment effect as a function of the X variables.\n# Even though the outcome is binary, the treatment effects are continuous\nxlearner_combined = XGBRegressor()\n\n# Fit the combined model\nxlearner_combined.fit(\n  # Stack the X variables for the treated and untreated users\n  pd.concat[[df_train_0, df_train_1]](X),\n  # Stack the X-learner treatment effects for treated and untreated users\n  pd.concat([xlearner_te_0, xlearner_te_1])\n)\n\n# Predict treatment effects for each user\nxlearner_simple_te = xlearner_combined.predict(df_test[X])\n\n\n\nXs-learner treatment effect distribution\n\n\n\n\nFull X-learner\nThe simplified X-Learner required 3 ML models. The full X-learner as originally proposed by Künzel et al. requires 5 ML models.\nInstead of fitting one combined model that predicts the treatment effects for everyone, the full X-learner uses two separate models, one for the treated users and one for the untreated users. This gives us two difference models that can predict treatment effects for new users. Künzel et al. recommend taking a weighted average of the two models, with the weights determined by a final propensity score model that predicts the probability of receiving the treatment.\n# Define the new models that are not used in the simple version\nxlearner_te_model_0 = XGBRegressor()\nxlearner_te_model_1 = XGBRegressor()\nxlearner_propensity = XGBClassifier()\n\nxlearner_te_model_0.fit(df_train_0[X], xlearner_te_0)\nxlearner_te_model_1.fit(df_train_1[X], xlearner_te_1)\n\n# Calculate predictions from both models\nxlearner_te_model_0_te = xlearner_te_model_0.predict(df_test[X])\nxlearner_te_model_1_te = xlearner_te_model_1.predict(df_test[X])\n\n# Calculate the propensity scores\nxlearner_propensity.fit(df_train[X], df_train[T])\nxlearner_propensities = xlearner_propensity.predict_proba[df_test[X]](:, 1)\n\n# Calculate the treatment effects as propensity weighted average\nxlearner_te = xlearner_propensities *xlearner_te_model_0_te + (1 - xlearner_propensities)* xlearner_te_model_1_te\n\n\n\nX-learner treatment effect distribution"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "A Simpler Alternative to X-Learner for Uplift Modeling\n\n\n\n\n\nA guide to the simplified X-learner approach for Uplift modeling\n\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\nNo matching items"
  }
]