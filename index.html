<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title>Prof. Junwei Liang / 梁俊卫</title>
  <!-- 搜索引擎优化stuff -->
	<meta name="description"
    content="Academic website for Junwei Liang. Dr. Junwei Liang is currently a tenure-track Assistant Professor at The Hong Kong University of Science and Technology (Guangzhou). He is also an affiliate assistant professor at HKUST computer science & engineering department. He was a senior researcher at Tencent Youtu Lab working on cutting-edge computer vision research and applications. Prior to that, he received his Ph.D. degree from Carnegie Mellon University, working with Prof. Alexander Hauptmann. He is the recipient of Baidu Scholarship and Yahoo Fellowship, and awarded Rising Star Award at the World AI Conference in 2020. He is the winner of several public safety video analysis competitions, including ASAPS and TRECVID ActEV. His work has helped and been reported by major news agencies like the Washington Post and New York Times. His research interests include human trajectory forecasting, action recognition, and large-scale computer vision and video analytics in general. His mission: develop AI technologies for social good.">
	<meta name="keywords" content="Junwei Liang,CMU,HKUST,HKUST-GZ,Professor,computer vision,PhD,梁俊卫,Carnegie Mellon University,The Hong Kong University of Science and Technology">

	<!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156016426-1"></script>
  <script>
  	// for Google Analytics, for free!
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-156016426-1');

  </script>

</head>
<body>
<div id="sidebar">
  <img class='me' src="resources/me.jpeg"></img>
  <br/>
  <div class="info">
    <h2 class="name">Prof. Junwei Liang</h2>
    <h2 class="name_chinese">梁俊卫</h2>
    <h2 class="email">junweiliang1114@gmail.com</h2>
    <h2 class="email">junweiliang@hkust-gz.edu.cn</h2>
    <h2 class="email">HKUST (Guangzhou) / HKUST</h2>
    <h2 class="email">Office: E4-304</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Google Scholar</a>
    </h2>
    <h2 class="link">
      <a href="https://www.semanticscholar.org/author/Junwei-Liang/1915796">[Semantic Scholar]</a>
      <a href="https://www.researchgate.net/profile/Junwei_Liang3">[Research Gate]</a>
    </h2>
    <h2 class="link">
      <a href="https://github.com/JunweiLiang">[Github]</a>
      <a href="https://www.linkedin.com/in/junweiliang/">[LinkedIn]</a>
      <a href="https://www.youtube.com/channel/UC-z7ZWp8Rbu2xhxnbAL_bRQ">[Youtube]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.zhihu.com/people/junwei-liang-50">[知乎]</a>
      <a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">[小红书]</a>
      <a href="https://twitter.com/JunweilLiang">[Twitter]</a>
    </h2>

    <!--
    <a class="quickLink" href="https://medium.com/@junweil">
      <img class='medium' style="" src="resources/medium.png"></img>
    </a>
    <a class="quickLink" href="https://dblp.org/pers/hd/l/Liang_0001:Junwei">
      <img class='dblp' style="height:20px" src="resources/dblp.png"></img>
    </a>
    <a class="quickLink" href="http://aminer.cn/profile/junwei-liang/562cb48c45cedb3398c9e13b">
      <img class='aminer' style="height:20px;width: 50px;margin-top:4px" src="resources/aminer.png"></img>
    </a>
    <a class="quickLink" href="https://g.co/kgs/gTWf5W">
      <img class='aminer' name="Google knowledge graph" style="height:30px;width: 30px;margin-top:0px" src="resources/gkg.png"></img>
    </a>-->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./projects.html#projects">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Projects
    </a>
    <a class="nav_item" href="./projects.html#publications">
      <i class="icon icon-file icon-white"></i> &nbsp; Publications
    </a>
    <a class="nav_item" href="./teaching.html#teaching">
      <i class="icon icon-user icon-white"></i> &nbsp; Teaching / Talks
    </a>
    <a class="nav_item" href="./index.html#awards">
      <i class="icon icon-bookmark icon-white"></i> &nbsp; Honors / Awards
    </a>
    <a class="nav_item" href="./index.html#media">
      <i class="icon icon-volume-up icon-white"></i> &nbsp; Selected Media
    </a>
    <a class="nav_item" href="./awesome.html">
      <i class="icon icon-list icon-white"></i> &nbsp; Awesome Lists
    </a>
    <a class="nav_item" href="./letter.html">
      <i class="icon icon-pencil icon-white"></i> &nbsp; Letter
    </a>

  </div>
</div>

<div id="main">
  <div class="title">
    <a class="title_link" id="bio" href="#bio">Bio</a>
    <!--<img src="https://visitor-badge.glitch.me/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>-->
    <img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>

    <img id="logo" src="resources/hkustgz-logo.jpg"></img>
  </div>
  <div class="content">
    I am an assistant professor at <a href="https://hkust-gz.edu.cn/academics/four-hubs/information-hub/artificial-intelligence">The Hong Kong University of Science and Technology (Guangzhou campus)</a> in the AI Thrust. I am also an <a href="https://cse.hkust.edu.hk/admin/people/faculty/?c=affiliate_gz">affiliate assistant professor</a> at HKUST computer science & engineering department. Please see <a href="./projects.html">these projects</a> for an overview of my research. <span style="font-weight: bold">My mission: develop AI technologies for social good.</span>

    <div class="linebreak"></div>

    Prior to joining HKUST-GZ, I was a senior researcher at Tencent Youtu Lab working with <a href="https://scholar.google.com/citations?user=Ljk2BvIAAAAJ&hl=en">Chunhua Shen</a>, where we landed commercial AI products in tens-of-millions-RMB-scale projects. I previously graduated from Carnegie Mellon University, where I was advised by <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a>.
    My Ph.D. and research work were mostly funded by <a href="https://www.iarpa.gov/research-programs/diva">IARPA</a>, <a href="https://www.nist.gov/ctl/pscr/real-time-video-analytics-situation-awareness">NIST</a> and <a href="https://nsf.gov/awardsearch/showAward?AWD_ID=1650994&HistoricalAwards=false">NSF</a> grants.
    I was a research intern at Google AI multiple times and collaborated with <a href="https://scholar.google.com/citations?user=jIKjjSYAAAAJ&hl=en">Lu Jiang</a>, <a href="https://scholar.google.com/citations?user=S-hBSfIAAAAJ&hl=en">Liangliang Cao</a>, <a href="https://scholar.google.com/citations?user=feX1fWAAAAAJ">Jia Li</a> and <a href="https://scholar.google.com/citations?user=MxxZkEcAAAAJ&hl=en">Kevin Murphy</a>.
    I obtained my undergraduate degree from <a href="https://www.ruc.edu.cn/en">RUC</a>, advised by <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=zh-CN">Qin Jin</a>.

    <div class="linebreak"></div>

    <!--
    <span style="color: red">I have multiple <strike>fully-funded Ph.D.</strike> and <strike>research assistant/intern positions available.</strike> (Our lab currently has around 10 people, and I want to ensure that everyone is producing meaningful research before I consider expanding the team.) </span> Please checkout potential <a href="./projects.html">projects</a> and <a href="./letter.html">this letter</a> if you are interested in joining our lab. 招生文: [<a href="https://zhuanlan.zhihu.com/p/562523740">知乎</a>] [<a href="https://www.xiaohongshu.com/discovery/item/6323ed9e00000000110142ad">小红书</a>]
    -->
     <span style="color: red">I am looking for one PhD. student to work on Embodied AI. Send me an email if you are interested and have two or more first-authored published papers at top conferences. </span>
     Our lab currently has around 9 students. Please checkout our <a href="https://precognition.team">lab resources</a> and <a href="./letter.html">this letter</a>. 招生文: [<a href="https://zhuanlan.zhihu.com/p/562523740">知乎</a>] [<a href="https://www.xiaohongshu.com/discovery/item/6323ed9e00000000110142ad">小红书</a>]

    <div class="linebreak"></div>

    For fellow assistant professors and Ph.D. students, I have gathered some <span style="font-weight: bold">awesome lists</span> of resources that may be useful for your success. See <a href="./awesome.html">here</a> and feel free to contribute on Github. [<a href="https://wx.zsxq.com/mweb/views/topicdetail/topicdetail.html?topic_id=411558545282218&group_id=142181451122&inviter_id=582155511885254">被CVer盗了hhh</a>]

    <div class="linebreak"></div>

    If you want to meet, check out <a href="https://calendar.google.com/calendar/embed?src=junweiliang1114%40gmail.com&ctz=Asia%2FShanghai" target="_blank">my public calendar</a> first and propose a meeting via email.

  </div>

  <div class="subtitle">
    <a class="title_link" id="group" href="#group">Precognition Lab</a>
    (Website: <a href="https://precognition.team">precognition.team</a>)
    (Demos: <a href="https://precognition.team/projects.html#demos">Link</a>)
  </div>

  <div class="content">
    Our research lab, the Precognition Lab (智能感知与预测实验室), is interested in building human-level <span style="font-weight: bold;">Embodied AI</span> systems that can effectively perceive, reason, and interact with the real-world for the good of humans.
    Here is an up-to-date <a href="https://precognition.team/projects.html#roadmap">research roadmap</a>.
    Here are our on-going or finished <a href="./projects.html">research grants</a>.



    <div class="linebreak"></div>

    Our lab's computing resources include 32 RTX 3090/4090 GPUs and a cluster of 24 A6000 GPUs with a 100TB NAS. See <a href="https://www.linkedin.com/feed/update/urn:li:activity:7099940517966200832/">this post</a>.
    And we have three mobile platforms with a robot arm:

    <div class="linebreak"></div>

    <img src="resources/robot1.png" style="height:300px;margin:20px 30px 0 0"></img>
    <img src="resources/robot2.png" style="height:300px;margin:20px 30px 0 0"></img>
    <img src="resources/robot3.png" style="height:300px;margin:20px 30px 0 0"></img>

  </div>

  <div class="title">
    <a class="title_link" id="news" href="#news">News</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-info">02/2024</span>
        Serve as a panelist at the VALSE Embodied AI webinar.
        [<a href="https://mp.weixin.qq.com/s/788DpNUbjklH-unhznkewg?poc_token=HJlaAmaj-hqsfo_KezM6IrqyY7MsZZ5wYfVpxEyz">VALSE</a>]
        [<a href="https://live.bilibili.com/22300737">bilibili</a>]
      </li>
      <li>
        <span class="label label-info">02/2024</span> Co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2024-precognition">The 6th workshop on Precognition: Seeing through the Future</a> @CVPR 2024.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2024-computervision-visionforecasting-activity-7163724230201733120-ERWa/">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/682320005">知乎</a>] [<a href="https://www.xiaohongshu.com/explore/65cd76e400000000070055a8">小红书</a>]
      </li>
      <li>
        <span class="label label-info">12/2023</span> Keynote speech at the CEII2023 Workshop
        [<a href="https://mp.weixin.qq.com/s/FRZ_r2BrvCz2Y7RreJdLgA">Schedule</a>]
      </li>
      <li>
        <span class="label label-info">10/2023</span> Co-organizing the Open-world Visual Perception Workshop (“开放世界下的视觉感知和增强”主题论坛) @PRCV 2023
        [<a href="https://mp.weixin.qq.com/s/ib9aKBhQhoaAFqZB93F3wQ">Schedule</a>]
      </li>
      <li>
        <span class="label label-info">09/2023</span> Hosting HKUST AI Seminar series. Many thanks to the incoming speakers from around the world!
        [<a href="https://hkust-seminar.github.io/">Course Website</a>]
      </li>
      <li>
        <span class="label label-info">08/2023</span> HKUST-GZ PhD Summer Camp has started! Welcome!
        [<a href="https://www.kaggle.com/competitions/hkustgz-ai-summer-camp-junwei-p1">Project 1</a>]
        [<a href="https://www.kaggle.com/competitions/hkustgz-ai-summer-camp-junwei-p2">Project 2</a>]
      </li>
      <li>
        <span class="label label-info">07/2023</span> Attended a series of talks and events. No more traveling till fall!
        [<a href="https://www.xiaohongshu.com/explore/64a809f5000000001a011856">WAIC @Shanghai</a>]
        [<a href="https://www.xiaohongshu.com/explore/6483cb490000000013034466">RUC Seminar @Beijing</a>]
      </li>
      <li>
        <span class="label label-info">06/2023</span> The Precognition Workshop was successfully held at CVPR! Thanks to all the co-organizers and program committee members!
        [<a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">Workshop Site</a>]
        [<a href="https://www.youtube.com/watch?v=Z3JhfOp0eGM">CVPR Workshop Recording</a>]
      </li>
      <li>
        <span class="label label-info">02/2023</span> One paper accepted by <span style="font-weight:bold">CVPR 2023</span>. Congrats to Xiaoyu!
      </li>
      <li>
        <span class="label label-info">02/2023</span> I'm teaching <a href="https://hkust-aiaa5032.github.io/">AIAA 5032 Foundations of Artificial Intelligence</a> and <a href="https://hkust-aiaa5036.github.io/">AIAA 5036 Autonomous AI</a> this semester at HKUST (Guangzhou).
      </li>
      <li>
        <!--<a href="https://www.linkedin.com/posts/khoa-luu-90900215_cvpr2023-activity-7009293998955655168-M4lu?utm_source=share&utm_medium=member_desktop">Precognition workshop</a>-->
        <span class="label label-info">01/2023</span> I am co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">The 5th workshop on Precognition: Seeing through the Future</a> @CVPR 2023. [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2023-workshop-computervision-activity-7030466054787121152-NacF?utm_source=share&utm_medium=member_desktop">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/603134088">知乎</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> Presented first-ever lecture at HKUST (Guangzhou).
        [<a href="https://www.youtube.com/watch?v=i2M9codDGes">AI Seminar</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> <span style="font-weight: bold">Two</span> papers accepted at <span style="font-weight: bold">NeurIPS 2022</span>.
        [<a href="https://arxiv.org/abs/2209.12362">Multi-Action</a> (<a href="https://nips.cc/virtual/2022/spotlight/65262" style="color:red">Spotlight paper</a>, 3.7% acceptance rate, 384/10411)]
        [<a href="https://arxiv.org/abs/2209.13307">Video Retrieval</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> Joined HKUST-GZ as a Tenure-Track Assistant Professor. Started an <a href="https://github.com/JunweiLiang/awesome_lists">awesome list</a> collection for TTAPs and PhD students.
      </li>
      <li>
        <span class="label label-info">09/2022</span> Invited to present at a young researcher forum by <a href="https://scholar.google.com/citations?user=qpBtpGsAAAAJ&hl=en">Prof. Xiaoou Tang</a> and <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a>.
      </li>
      <li>
        <span class="label label-info">06/2022</span> Achieved <span style="font-weight:bold;">second-place</span> out of 150 teams on the <a href="https://arxiv.org/pdf/2204.10380.pdf">public leaderboard</a> of the Naturalist Driver Action Recognition Task - AI City Challenge @ CVPR 2022.
        [<a href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.pdf">CVPRW Paper</a>]
        [<a href="https://www.youtube.com/watch?v=u4CrNKt4P54">Presentation</a>] [<a href="https://github.com/JunweiLiang/aicity_action">Code and Model</a>]
      </li>
      <li>
        <span class="label label-info">10/2021</span> Published a <a href="https://www.techbeat.net/talk-info?id=588">research talk</a> at TechBeat.net on Pedestrian Trajectory Prediction. [<a href="https://www.techbeat.net/talk-info?id=588">将门TechBeat</a>] [<a href="https://www.bilibili.com/video/BV1Y44y1x7nv/">B站</a>]
      </li>
      <!--
      <li>
        <span class="label label-info">09/2021</span> Joined <span style="font-weight:bold">Tencent Youtu Lab</span> as a researcher.
      </li>
      -->
      <li>
        <span class="label label-info">08/2021</span> Received Doctoral Consortium Award at ICCV 2021, mentored by <a href="https://people.epfl.ch/alexandre.alahi?lang=en">Prof. Alexandre Alahi</a>.
      </li>
      <li>
        <span class="label label-info">08/2021</span> 1 paper accepted by <span style="font-weight:bold">ICCV 2021</span>.
      </li>
      <li>
        <span class="label label-info">08/2021</span> Our <a href="https://vera.cs.cmu.edu">VERA</a> system helps another major <span style="font-weight:bold">Washington Post</span> news report. [<a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup/">link</a>]
          <a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup/">
            <img class="press" src="resources/wapo.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">07/2021</span> Successfully defended my Ph.D. thesis: From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. [<a href="thesis/">link</a>]
      </li>
      <li>
        <span class="label label-info">04/2021</span> Featured in a <a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">front-page news report</a> (04/15) by Washington Post using crowding counting technologies. [<a href="https://www.youtube.com/watch?v=rsQTY9083r8?t=1086">video</a>] [<a href="https://www.zhihu.com/zvideo/1366151651770834944">知乎</a>]
        <a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">
            <img class="press" src="resources/wapo.png"></img>
        </a>
      </li>
      <li>
        <span class="label label-info">01/2021</span> <span style="font-weight: bold;">Invited presentation</span> at ICPR'20 pattern forecasting workshop. [<a href="https://sites.google.com/di.uniroma1.it/patcast/program?authuser=0">link</a>]
      </li>
      <!--<li>
        [10/2020] Successfully proposed my PhD. thesis.
      </li>-->
      <li>
        <span class="label label-info">09/2020</span> We won the <a href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2020-automated-stream-analysis">Automated Streams Analysis for Public Safety Challenge</a> with a <a href="https://www.herox.com/ASAPS1/update/3483">$30k prize</a>.
      </li>
      <li>
        <span class="label label-info">08/2020</span> Our <a href="https://arxiv.org/abs/2006.16479">paper</a> has been accepted by WACV 2021 (one strong-accept) and <span style="font-weight:bold">reported by CMU news</span>:
        <a href="https://www.cmu.edu/news/stories/archives/2020/august/drones-hurricane-damage.html">
            <img class="press" src="resources/cmu.png"></img>
        </a>
      </li>
      <li>
        <span class="label label-info">08/2020</span> Analyzed videos for journalist from <span style="font-weight:bold">the Washington Post</span> on a <a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">major news</a>.
          <a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">
            <img class="press" src="resources/wapo.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">07/2020</span> Awarded <a href="https://baijiahao.baidu.com/s?id=1671984902144018200&wfr=spider&for=pc"><span style="font-style: italic;">"AI Rising Star"</span></a> at the <a href="https://worldaic.com.cn/portal/en/index.html">World AI Conference</a>.
      </li>
      <li>
        <span class="label label-info">07/2020</span> <a href="https://precognition.team/next/simaug/"><span style="font-style: italic;">SimAug</span></a> paper accepted by <span style="font-weight:bold">ECCV 2020</span>.
      </li>
      <li>
        <span class="label label-info">06/2020</span> <a href="https://precognition.team/next/multiverse/index.html"><span style="font-style: italic;">Multiverse</span></a> (<span style="font-weight:bold">CVPR 2020</span>) code and dataset are released! [<a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">blog</a>] [<a href="https://zhuanlan.zhihu.com/p/148343447">知乎</a>] [<a href="https://github.com/JunweiLiang/Multiverse">code</a>]
      </li>
      <!--
      <li>
        <span class="label label-info">04/2020</span> A vision-based <a href="https://github.com/JunweiLiang/social-distancing-prediction"><span style="font-style: italic;">Social Distancing Early Forecasting</span></a> system is open-sourced</span>. Project received $6200 <a href="https://edu.google.com/programs/credits/research/">Google Cloud Research Grant</a>.
      </li>
      <li>
        <span class="label label-info">03/2020</span> <span style="font-weight:bold">Guest lecture</span> at CMU 11-775 class for grad students. [<a href="https://youtu.be/nbT7IIU8Sdc">Video</a>]
      </li>
      <li>
        [12/2019] <a href="https://precognition.team/next/multiverse/index.html"><span style="font-style: italic;">Multiverse</span></a> paper is out! Accepted by <span style="font-weight:bold">CVPR 2020</span>.
      </li>
      -->
      <li>
        <span class="label label-info">12/2019</span> Received <a href="http://scholarship.baidu.com/">Baidu Scholarship</a> (10 recipients globally).
        Press Coverage:
          <a href="http://news.ruc.edu.cn/archives/267603">
            <img class="press" src="resources/ruc.png"></img>
          </a>,
          <a href="http://m.china.com.cn/appshare/doc_1_20_1489589.html?from=groupmessage&isappinstalled=0">
            <img class="press" src="resources/china.png"></img>
          </a>,
          <a href="https://baijiahao.baidu.com/s?id=1654884571460145099&wfr=spider&for=pc">
            <img class="press" src="resources/baidu.png"></img>
          </a>,
          <a href="https://www.yanxishe.com/blogDetail/17504">
            <img class="press" src="resources/yanxishe.png"></img>
          </a>,
          <a href="http://app.bjheadline.com/8816/newshow.php?newsid=5514954&src=stream&typeid=20&uid=335186&did=16835adeb9fa457b8ec1f9b570dcc4b1&show=0&fSize=M&ver=2.6.3&ff=fz&mood=wx&from=groupmessage&isappinstalled=0">
            <img class="press" src="resources/bjheadline.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">09/2019</span> Our <a href="https://vera.cs.cmu.edu/">Shooter Localization System</a> won <span style="font-weight:bold">Best Demo</span> award at <a href="https://cbmi2019.org/">CBMI2019</a>. [<a href="https://vera.cs.cmu.edu/" target="_blank">Project Site</a>]
        <br/>Press Coverage:
          <a href="https://www.cmu.edu/news/stories/archives/2019/november/system-locates-shooters-using-smartphone-video.html">
            <img class="press" src="resources/cmu.png"></img>
          </a>,
          <a href="https://pittsburgh.cbslocal.com/2019/11/20/cmu-develops-video-system-locate-mass-shooters/">
            <img class="press" src="resources/cbs.png"></img>
          </a>,
          <a href="https://www.post-gazette.com/business/tech-news/2019/11/20/Carnegie-Mellon-CMU-develops-cellphone-smartphone-video-system-location-shooter-triangulate/stories/201911200101">
            <img class="press" src="resources/post.png"></img>
          </a>,
          <a href="https://gizmodo.com/smartphone-videos-can-now-be-analyzed-and-used-to-pinpo-1839979803">
            <img class="press" src="resources/gizmodo.png"></img>
          </a>,
          <a href="https://www.dailymail.co.uk/sciencetech/article-7707501/Carnegie-Mellon-aims-end-pro-longed-massacres-locates-active-shooters.html">
            <img class="press" src="resources/dailymail.png"></img>
          </a>,
          <a href="https://www.techspot.com/news/82881-researchers-develop-system-can-pinpoint-shooter-location-using.html">
            <img class="press" src="resources/techspot.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">06/2019</span> Presented Future Prediction paper at <span style="font-weight:bold">CVPR 2019</span>. It was reported by the media and it received <span style="font-weight:bold">30k+ views</span> in a week. <a href="https://precognition.team/next" target="_blank"><i title="Go to project page" class="icon-zoom-in"></i></a> [<a href="https://twitter.com/jcniebles/status/1141366303921303552" target="_blank">Tweets</a>]
      </li>
      <li><span class="label label-info">04/2019</span> Our CMU team's (INF & MUDSML) system achieved the <span style="font-weight:bold">best performance</span> on the <a href="https://actev.nist.gov/prizechallenge#tab_leaderboard" target="_blank">activity detection challenge</a> (<a href="resources/actev-prizechallenge-06-2019.png" target="_blank">Cached</a>) in surveillance videos hosted by NIST & IARPA. <!--The competitors include all other DIVA-funded teams from universities and companies as well as other strong participants from all over the world.--> We have released our code and model for Object Detection & Tracking <a href="https://github.com/JunweiLiang/Object_Detection_Tracking">here</a>. </li>
      <li><span class="label label-info">12/2018</span> <span style="font-weight:bold">MemexQA</span> paper accepted by <span style="font-weight:bold">TPAMI 2019</span>. <a href="https://precognition.team/memexqa" target="_blank"><i title="Go to project page" class="icon-zoom-in"></i></a></li>

      <li><span class="label label-info">06/2018</span> Presented MemexQA paper at <span style="font-weight:bold">CVPR 2018</span>. [<a href="https://youtu.be/TBOnKekODCI?t=1h11m29s" target="_blank">Spotlight Talk</a>]</li>
      <!--<li>[03/2017] Two papers accepted by ICASSP 2017.</li>
      <li>[02/2017] Two demo papers accepted by <span style="font-weight:bold">AAAI 2017</span>.</li>-->
      <li><span class="label label-info">11/2016</span> <span style="font-weight:bold">Best performer</span> in the NIST TRECVID 2016 Ad-hoc Video Search Challenge (no annotation track).</li>
      <!--<li>[02/2016] One oral paper accepted by <span style="font-weight:bold">IJCAI 2016</span>.</li>-->
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="awards" href="#awards">Awards</a>
  </div>


  <div class="content">
    <ul>
      <li>ICCV Doctoral Consortium Award <div class="float-right">2021</div></li>
      <li>
        <a href="https://baijiahao.baidu.com/s?id=1671984902144018200&wfr=spider&for=pc"><span style="font-style: italic;">Rising Star</span></a> (云帆奖-明日之星), World AI Conference <div class="float-right">2020</div>
      </li>
      <li>Baidu Scholarship (10 Ph.D. students worldwide) <div class="float-right">2019</div></li>
      <li>Winner, <a href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2020-automated-stream-analysis">Automated Streams Analysis for Public Safety Challenge</a> - $30k prize <div class="float-right">2020</div></li>
      <li>Best Demo Award at CBMI2019 <div class="float-right">2019</div></li>
      <li>Yahoo! Fellowship <div class="float-right">2016 - 2018</div></li>
      <li>Winner, TRECVID ActEV Challenge <div class="float-right">2019</div></li>
      <li>Winner, TRECVID Ad-hoc Video Search Challenge, no annotation track  <div class="float-right">2016</div></li>
      <li>CMU LTI Student Research Symposium Best Paper Honorable Mentions <div class="float-right">2018</div></li>
      <li>Google Cloud COVID-19 Research Grant - $6200 <div class="float-right">2020</div></li>
      <!--
      <li>CVPR, CES, IJCAI, ICASSP, NIST PSCR, NIST TRECVID student travel grants <div class="float-right">2016-2020</div></li>
      <li>Best Undergraduate Thesis (Top 5%) <div class="float-right">2015</div></li>
      <li>Second Prize, the National Undergraduates Computer Design Competition of China <div class="float-right">2014</div></li>
      <li>National Prize (Top 10%), National Undergraduates Innovation Project <div class="float-right">2013</div></li>
      -->
    </ul>
  </div>


  <div class="title">
    <a class="title_link" id="media" href="#media">Selected Media</a>
  </div>

  <div class="content">
    <ul>
      For more up-to-date media coverage, please visit my <a href="https://precognition.team/index.html#media">lab website.</a>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">How Shireen Abu Akleh was killed</span> (provided gunshot and shooter analysis), June 2022.
        [<a href="https://www.washingtonpost.com/investigations/interactive/2022/shireen-abu-akleh-death/?itid=lk_inline_manual_4/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">Anatomy of a crackdown</span> (provided gunshot and shooter analysis), August 25, 2021.
        [<a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup//">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">17 requests for backup in 78 minutes</span> (provided crowd counting analysis), April 15, 2021.
        [<a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Carnegie Mellon University News.</span> <span style="font-style: italic;">Amateur Drone Videos Could Aid in Natural Disaster Damage Assessment</span>, August 28, 2020.
      </li>
      <li>
        <span style="font-weight: bold">AZO Robotics.</span> <span style="font-style: italic;">New AI System Helps Detect Damage Caused to Buildings by Hurricanes</span>, August 31, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">Lewd cheerleader videos, sexist rules: Ex-employees decry Washington’s NFL team workplace</span> (featured in the video analytics), August 26, 2020.
        [<a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">CBS.</span> <span style="font-style: italic;">Researchers At Carnegie Mellon University Develop Video System To Locate Mass Shooters Using Smartphones</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">post-gazette.</span> <span style="font-style: italic;">CMU develops video system that can locate mass shooter</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">GIZMODO.</span> <span style="font-style: italic;">Smartphone Videos Can Now Be Analyzed and Used to Pinpoint the Location of a Shooter</span>, November 21, 2019.
      </li>
      <li>
        <span style="font-weight: bold">DailyMail.</span> <span style="font-style: italic;">Active shooters can be located within minutes by new software that analyzes smartphone video from the scene and can even identify the type of gun</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">Techspot.</span> <span style="font-style: italic;">Researchers develop system that can pinpoint a shooter's location using smartphone videos</span>, November 21, 2019.
      </li>
      <li>
        <span style="font-weight: bold">New York Times.</span> <span style="font-style: italic;">Who Killed the Kiev Protesters? A 3-D Model Holds the Clues</span> (featured in the video analytics), May 30, 2018.
      </li>
      <li>
        <span style="font-weight: bold">读芯术.</span> <span style="font-style: italic;">卡内基梅隆大学梁俊卫：视频中行人的多种未来轨迹预测</span>, August, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Baidu.</span> <span style="font-style: italic;">乘风破浪的AI技术青年——首届WAIC云帆奖名单公布</span>, July 11, 2020.
      </li>
      <li>
        <span style="font-weight: bold">China.com.cn.</span> <span style="font-style: italic;">人大高瓴人工智能学院“高屋建瓴-青年说”首期开讲</span>, Jan 6, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Baidu.</span> <span style="font-style: italic;">AI界的中国力量！百度奖学金助力中国AI人才绽放光芒！</span>, Jan 5, 2020.
      </li>
      <li>
        <span style="font-weight: bold">量子位.</span> <span style="font-style: italic;">李飞飞团队造出”窥视未来”新AI:去哪干啥一起猜, 准确率压倒老前辈</span>, received 30k+ views in a week, Feb 13, 2019.
      </li>
      <li>
        <span style="font-weight: bold">机器之心.</span> <span style="font-style: italic;">遇见未来！李飞飞等提出端到端系统Next预测未来路径与活动</span>, Feb 14, 2019.
      </li>
      <li>
        Aminer.cn, AI 2000 ranking (2019 - 2022).
        <br/>
        <img style="height: 400px" src="resources/ai_2000_2019_2022_rank.jpg"></img>
      </li>
    </ul>
  </div>


<!--

  <div class="title">
    <a class="title_link" id="exp" href="#exp">Research Experience</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="title">Researcher at Tencent Youtu Lab</span> <div class="float-right time">2021 - present</div>
        <div class="info">
          Work on large-scale video and language models and efficient long-term action detection applications.
        </div>
      </li>
      <li>
        <span class="title">Research Assistant at Carnegie Mellon University</span> <div class="float-right time">2015 - 2021</div>
        <div class="info">
          Worked on Large-scale Video Analysis and Retrieval. Studied unsupervised learning of video concept detectors from the Internet. Also participated in the development of event reconstruction tool.
          The project is for Synchronization and localization of noisy user-generated videos to reconstruct the event scene and timeline from unorganized social media videos, affiliates with CMU <a href="http://www.cmu.edu/chrs/" target="_blank">Center for Human Rights Science</a>.
          I'm also the major contributor to the government-funded projects: <a href="https://www.nist.gov/ctl/pscr/real-time-video-analytics-situation-awareness" target="_blank">PSCR by NIST</a> (2017-2020), <a href="https://www.iarpa.gov/index.php/research-programs/diva" target="_blank">DIVA by IARPA</a> (2017-2021) and <a href="https://www.iarpa.gov/index.php/research-programs/aladdin-video" target="_blank">ALADDIN by IARPA</a> (2017).
          Advised by <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ" target="_blank">Prof. Alexander Hauptmann</a>.
          [<a href="https://www.nist.gov/video/real-time-video-analytics-situation-awareness" target="_blank">PSCR 2018 presentation</a>, <a href="https://www.nist.gov/ctl/pscr/2019-stakeholder-meeting-analytics-sessions" target="_blank">2019</a>]
        </div>
      </li>
      <li>
        <span class="title">Research Intern at Google Cloud AI</span> <div class="float-right time">May 2020 - Aug 2020</div>
        <div class="info">
          Worked on viewpoint equivariant representation learning for activity recognition.
          Advised by <a href="https://scholar.google.com/citations?user=_lswGcYAAAAJ&hl=en" target="_blank">Dr. Ting Yu</a>, <a href="https://scholar.google.com/citations?user=vM1SktEAAAAJ&hl=en" target="_blank">Dr. Xuehan Xiong</a> and <a href="http://llcao.net/" target="_blank">Prof. Liangliang Cao</a>.
        </div>
      </li>
      <li>
        <span class="title">Research Intern at Google AI</span> <div class="float-right time">May 2019 - Aug 2019</div>
        <div class="info">
          Worked on future person activity and trajectory prediction in videos. Integrated research models to a Google Cloud product. Used 3D simulator (carla.org) to collect multi-modal future behavioral data.
          Advised by <a href="http://www.cs.cmu.edu/~lujiang/" target="_blank">Dr. Lu Jiang</a> and <a href="https://www.cs.ubc.ca/~murphyk/" target="_blank">Prof. Kevin Murphy</a>.
        </div>
      </li>
      <li>
        <span class="title">Student Researcher at Google Cloud AI</span> <div class="float-right time">May 2018 - Dec 2018</div>
        <div class="info">
          Worked on activity recognition and prediction in multi-perspective streaming videos. Studied principal computer vision and high-level semantic reasoning models for interperson and person-object interaction to help AI better understand human activities. Advised by <a href="http://www.cs.cmu.edu/~lujiang/" target="_blank">Dr. Lu Jiang</a> and <a href="http://www.niebles.net/" target="_blank">Prof. Juan Carlos Niebles</a>.
        </div>
      </li>
      <li>
        <span class="title">Research Assistant at Renmin University of China</span> <div class="float-right time">2013 - 2015</div>
        <div class="info">
          Studied semantic concept annotation on user-generated videos using audio. Participated HUAWEI semantic concept annotation of UGC videos grand challenge 2014 and ranked 3rd in the evaluation. Also worked on natural language description generation for images and videos with deep models. Ranked 1st in ImageCLEF 2015 “image to sentence” subtask in the evaluation. Advised by <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=zh-CN" target="_blank">Prof. Qin Jin</a>.
        </div>
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="education" href="#education">Education</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="title">Ph.D. in Artificial Intelligence</span> <div class="float-right time">2017 - 2021</div>
        <div class="info">School of Computer Science, Carnegie Mellon University</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a></div>
        <div class="info">Thesis: From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video [<a href="thesis/">Link</a>]</div>
      </li>
      <li>
        <span class="title">M.S. in Artificial Intelligence</span> <div class="float-right time">2015 - 2017</div>
        <div class="info">School of Computer Science, Carnegie Mellon University</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a></div>
      </li>
      <li>
        <span class="title">B.S. in Computer Science</span> <div class="float-right time">2011 - 2015</div>
        <div class="info">School of Information, Renmin University of China</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=en">Qin Jin</a></div>
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="web" href="#web">Web App Experience</a>
  </div>
  <div class="content">
    <ul>
      <li>
        <span class="title"><a href="https://vera.cs.cmu.edu/">Shooter Localization from Social Media Videos</a></span>
        <div class="info">Widely reported by news media. Presented at CES 2020.</div>
      </li>
      <li>
        <span class="title"><a href="https://github.com/JunweiLiang/Lecture_Attendance_Management">Attendance Management System</a></span>
        <div class="info">
          Used within <a href="https://www.lti.cs.cmu.edu/">CMU LTI</a> for a course with over a hundred students every semester since 2017.
        </div>
      </li>
      <li>
        <span class="title">Major CMS websites for organizations</span>
        <div class="info"><a href="http://www.hillhouseacademy.com/">Hillhouse Academy</a>,  <a href="https://dasai.ruc.edu.cn/index.php/site/designer">Annual Computer Design Competition in China (>2000 users annually)</a> </div>
      </li>
    </ul>
  </div>
-->


</div>


<!--
	a Junwei Liang's production
	contact: junweiliang1114@gmail.com
-->
</body>
</html>
