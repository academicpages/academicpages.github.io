---
title: 'Generative Model--Flow模型'
date: 2025-08-01
permalink: /posts/2025/08/blog-post-9/
tags:
  - 教程
---

这个generative model系列，主要是根据我平时看论文的时候，遇到的generative model，结合之前看到过的内容，和网上看的教程，做一个记录，之后如果这个系列更新的多，可以进行重新的梳理。主体顺序按照苏剑林的生成扩散模型漫谈这个系列进行介绍。

第一篇博客，由于看到了有关空间转录组，H&E image和Flow Matching相关的一篇icml论文，所以先学习一下Flow模型的基础知识，仔细想想确实generative model中，VAE，GAN和Diffusion都算是比较熟悉原理，只有Flow不太了解，正好趁此机会进行学习。

## 分布的变换--数学基础

### 变量变换定理（Change of Variables Theorem）

假设我们有一个连续随机变量 z，其概率密度函数为 $p_Z(z)$。现在，我们通过一个可逆的、可微的函数 $\mathbf{x} = g(z)$ 将 z 变换为另一个随机变量 $\mathbf{x}$。那么，$\mathbf{x}$ 的概率密度函数 $p_X(\mathbf{x})$ 可以通过以下公式计算：

$$p_X(\mathbf{x}) = p_Z(g^{-1}(\mathbf{x})) \left|\text{det}\left(\frac{\partial g^{-1}(\mathbf{x})}{\partial \mathbf{x}}\right)\right|$$

其中：

- $g^{-1}(\mathbf{x})$ 是函数 $g$ 的反函数，即将 $\mathbf{x}$ 变换回 $z$ 的过程，即 $z = g^{-1}(\mathbf{x})$。

- $\frac{\partial g^{-1}(\mathbf{x})}{\partial \mathbf{x}}$ 是反函数 $g^{-1}$ 对 $\mathbf{x}$ 的雅可比矩阵 (Jacobian Matrix)。

- $\text{det}(\cdot)$ 表示矩阵的行列式。

- $\lvert \cdot \rvert$ 表示取绝对值。

在流模型中，我们通常将上述公式写成对数形式，这也是机器学习中一个常用的方法，在实际计算中更为稳定和方便：

$$\log p_X(\mathbf{x}) = \log p_Z(g^{-1}(\mathbf{x})) + \log \left|\text{det}\left(\frac{\partial g^{-1}(\mathbf{x})}{\partial \mathbf{x}}\right)\right|$$

这个公式是流模型进行最大似然估计的理论基础。模型的目标就是通过学习函数 $g$ 的参数，使得在给定训练数据 $\mathbf{x}$ 的情况下，其对数似然 $\log p_X(\mathbf{x})$ 最大化。

### 构建Flow模型：一系列可逆变换的串联

由于使用一个单一函数进行复杂分布之间的变换比较困难，FLow模型和Diffusion一样采用相同的策略，构建一个由一系列可逆变换函数 $f_1, f_2, \ldots, f_K$ 组成的流水线。

假设我们从一个简单分布（如标准正态分布）中采样得到 $z_0 \sim p_Z(z_0)$，然后依次通过一系列变换：

$$z_1 = f_1(z_0)$$
$$z_2 = f_2(z_1)$$
$$\vdots$$
$$x = z_K = f_K(z_{K-1})$$

由于每个变换 $f_i$ 都是可逆的，整个变换过程 $g = f_K \circ \cdots \circ f_1$ 也是可逆的。根据链式法则，整个变换的雅可比行列式的对数可以方便地计算为各个变换雅可比行列式对数之和：

$$\log \left| \det\left(\frac{\partial x}{\partial z_0}\right) \right| = \sum_{i=1}^{K} \log \left| \det\left(\frac{\partial z_i}{\partial z_{i-1}}\right) \right|$$

因此，最终的对数似然可以表示为：

$$\log p_X(x) = \log p_Z(z_0) + \sum_{i=1}^{K} \log \left| \det\left(\frac{\partial f_i(z_{i-1})}{\partial z_{i-1}}\right) \right|$$

这个公式清晰地展示了流模型是如何通过一系列简单的变换累积地构建出复杂的概率分布的。


## 连续正则化流（Continuous Normalizing Flows, CNF）

### 正则化流（Normalizing Flows， NF）

NF其实就是变量变换定理在机器学习领域的直接应用，通过公式

$$p_X(\mathbf{x}) = p_Z(g^{-1}(\mathbf{x})) \left|\text{det}\left(\frac{\partial g^{-1}(\mathbf{x})}{\partial \mathbf{x}}\right)\right|$$

进行分布的变换。在实际操作中，需要从先验的简单分布中采样得到 $z_0 \sim p_Z(z_0)$，然后通过一个或者多个可逆变换最后得到给定的观测 $\mathbf{x}$的概率分布 $p_X(\mathbf{x})$。

### 连续正则化流（Continuous Normalizing Flows， CNF）

我们可以发现，NF是一种离散概率分布到另一类离散概率分布的变换。但是，如果我们想得到的一个连续的概率分布，比如高斯分布，那么这种离散到离散的变换就不太适用了。而连续正则化流 CNF 可以将任何连续分布 $p_z(z)$ 转变为 $p_X(x)$，从而实现从连续到连续的概率分布的转化。

CNF将变换的过程建模为一个连续的轨迹或者“流”，这个流通常用一个常微分方程（ODE）来描述。



