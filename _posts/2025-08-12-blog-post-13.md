---
title: 'MIT 6.S184 生成模型课程深度解析：从噪声到数据的数学之旅'
date: 2025-08-12
permalink: /posts/2025/08/blog-post-13/
tags:
  - 教程
  - 机器学习
  - 生成模型
  - 数学
---

# MIT 6.S184 - Generative AI With Stochastic Differential Equations 课程笔记

> "Creating noise from data is easy; creating data from noise is generative modeling."
> 
> ——Song et al.

## 引言

在探索流模型（Flow Models）和扩散模型（Diffusion Models）的学习过程中，我发现许多研究者都会提及MIT的经典课程——**Generative AI With Stochastic Differential Equations**。这门课程以其严谨的数学理论和深入的技术洞察而著称。

本文是基于课程笔记《[An Introduction to Flow Matching and Diffusion Models](https://arxiv.org/abs/2506.02070)》的深度学习总结，旨在系统地梳理现代生成模型的数学基础和核心思想。我们将从最基本的概念出发，逐步构建起完整的理论框架，最终理解如何在实践中构建高质量的生成模型。

---

## 第一章：生成建模的本质——从采样的角度理解生成

在深入技术细节之前，我们需要回答一个根本问题：当我们说"生成"时，我们究竟在谈论什么？

### 1.1 万物皆向量：数字化表示的统一语言

现代计算机无法直接理解图片、视频或声音这些复杂的现实世界对象。为了让机器能够处理这些信息，我们需要将它们转换为数学上可以操作的形式——**向量（Vector）**。

**具体来说：**

- **图像数据**：一张尺寸为 $H \times W$ 的RGB图片可以表示为一个三维数组，展平后成为属于 $\mathbb{R}^{H \times W \times 3}$ 空间的向量
- **视频数据**：包含 $T$ 帧的视频序列对应向量空间 $\mathbb{R}^{T \times H \times W \times 3}$
- **音频数据**：同样可以通过采样转换为高维向量表示

**核心洞察**：我们想要生成的所有复杂对象，在数学上都可以统一表示为高维向量 $z \in \mathbb{R}^d$。因此，"生成一个对象"这个抽象任务可以精确地转化为"生成一个高维向量"这个数学问题。

### 1.2 重新定义"生成"：从模糊概念到精确的采样过程

让我们以"生成一张猫的图片"为例来理解这个转化过程。显然，符合要求的"猫的图片"有无数种可能，没有唯一的"最佳答案"。这意味着我们需要从统计学的角度来思考这个问题。

我们可以假设存在一个包含所有可能真实猫咪图片的"概念空间"，这个空间用数学语言描述就是一个**概率分布（Probability Distribution）**，我们称之为**数据分布** $p_{\text{data}}$。在这个分布中，越接近真实猫咪外观的图片，其出现的概率密度就越高。

**关键转换**："生成一个物体"这个任务被精确地重新定义为：从数据分布 $p_{\text{data}}$ 中进行**采样（Sampling）**

$$
z \sim p_{\text{data}}
$$

这个看似简单的等式实际上包含了生成建模的核心思想：我们不是要找到一个"完美"的答案，而是要学会从正确的概率分布中随机抽取样本。

我们在训练阶段使用的**数据集（Dataset）** $\{z_1, z_2, \ldots, z_N\}$ 正是从 $p_{\text{data}}$ 中采样得到的有限样本集合，它们为我们提供了理解和逼近真实数据分布的窗口。

### 1.3 条件生成：在约束下的创造

在实际应用中，我们往往希望对生成过程施加某些控制。例如，我们可能想生成"一只在雪山背景下奔跑的金毛犬"这样具有特定属性的图像。

这种需求引出了**条件生成（Conditional Generation）**的概念。数学上，这意味着我们要从**条件概率分布** $p_{\text{data}}(\cdot \mid y)$ 中采样，其中 $y$ 是我们施加的条件（如文本描述、类别标签等）：

$$
z \sim p_{\text{data}}(\cdot|y)
$$

现代生成模型的一个重要目标是训练出能够处理多种不同条件的**统一模型**，使其能够根据用户提供的任意条件 $y$ 进行相应的生成。

### 1.4 从噪声到数据：生成模型的宏大蓝图

现在我们面临一个根本性的挑战：我们并不知道 $p_{\text{data}}$ 的具体数学形式，更无法直接从中采样。那么，如何实现我们的生成目标呢？

现代生成模型采用了一个巧妙的迂回策略，其核心思想可以概括为以下三步：

1. **选择简单起点**：从一个我们完全了解且易于采样的**初始分布** $p_{\text{init}}$ 开始，通常选择**标准高斯分布** $\mathcal{N}(0, I)$，即纯粹的随机噪声
2. **学习变换过程**：训练一个模型来学习一种特殊的**变换（Transformation）**
3. **实现神奇转化**：这个变换能够将来自 $p_{\text{init}}$ 的简单噪声样本，逐步转换成看起来像是来自 $p_{\text{data}}$ 的真实数据样本

**核心策略**：学习一个从噪声到数据的映射关系，这正是所有现代生成模型的共同哲学。

---

## 第二章：两大数学引擎——常微分方程与随机微分方程

实现"从噪声到数据"这一宏大目标需要强大的数学工具。现代生成模型主要依赖两种不同但相关的数学框架：**常微分方程（ODE）**和**随机微分方程（SDE）**。

### 2.1 流模型：确定性的优雅之路

流模型基于**常微分方程（Ordinary Differential Equation, ODE）**构建，描述的是一个完全**确定性（Deterministic）**的演化过程。

#### 2.1.1 核心概念

**向量场（Vector Field）** $u_t(x)$ 是流模型的灵魂，它可以被想象为一个智能导航系统：

- 在任意时刻 $t$ 和任意位置 $x$，向量场都能提供一个精确的速度向量
- 这个向量指明了在该时刻、该位置的粒子应该"朝哪个方向、以多快的速度"移动

**微分方程约束**：粒子的运动严格遵循向量场的指引：

$$
\frac{d}{dt}X_t = u_t(X_t)
$$

#### 2.1.2 生成过程的数学实现

1. **网络参数化**：使用神经网络 $u_t^\theta(x)$ 来学习和表示理想的向量场
2. **初始化**：从噪声分布 $p_{\text{init}}$ 中采样初始点 $X_0$
3. **数值求解**：通过数值方法（如**欧拉法** $X_{t+h} = X_t + h \cdot u_t^\theta(X_t)$）求解ODE
4. **轨迹追踪**：从 $t=0$（纯噪声）到 $t=1$（目标数据）的完整演化轨迹
5. **样本生成**：最终得到的 $X_1$ 即为我们生成的样本

**确定性特征**：在流模型中，一旦初始噪声 $X_0$ 确定，整个生成过程和最终结果都是完全确定的，不存在随机性。

### 2.2 扩散模型：拥抱随机性的力量

扩散模型在流模型的确定性基础上引入了**随机性（Stochasticity）**，通过**随机微分方程（Stochastic Differential Equation, SDE）**来描述更复杂的演化过程。

#### 2.2.1 SDE的数学结构

$$
dX_t = u_t(X_t)dt + \sigma_t dW_t
$$

这个方程包含两个关键组成部分：

**漂移项（Drift Term）** $u_t(X_t)dt$：

- 描述系统演化的确定性趋势
- 由神经网络 $u_t^\theta$ 学习和控制
- 提供了系统演化的"主要方向"

**扩散项（Diffusion Term）** $\sigma_t dW_t$：

- 引入随机扰动和不确定性
- $\sigma_t$ 控制随机性的强度
- $W_t$ 是**布朗运动（Brownian Motion）**，提供随机性源泉

#### 2.2.2 随机性带来的优势

扩散模型的生成过程具有以下特点：

- **多样性增强**：即使从相同的初始点 $X_0$ 开始，每次生成的结果都不完全相同
- **探索能力**：随机性帮助模型探索更广泛的解空间
- **鲁棒性提升**：随机扰动可以帮助模型逃离局部最优解

#### 2.2.3 两种模型的关系

**重要洞察**：流模型实际上是扩散模型的特殊情况。当噪声强度 $\sigma_t = 0$ 时，SDE退化为ODE，扩散模型就变成了流模型。这种统一的视角帮助我们更好地理解两种方法的本质联系。

---

## 第三章：寻找完美导航——训练目标的数学推导

现在我们面临生成模型训练的核心问题：如何找到那个能够将噪声完美转换为数据的理想向量场？这一章将通过严谨的数学推导，从第一性原理出发，构建我们的训练目标。

### 3.1 逆向工程：从"溶解"到"重建"

#### 3.1.1 概率路径的概念

与其直接考虑如何"创造"数据，我们采用逆向思维：一个清晰的数据样本是如何逐步"溶解"成纯粹噪声的？

这个思考引出了**概率路径（Probability Path）**的核心概念。概率路径不是空间中的几何路径，而是**概率分布在时间轴上的演化轨迹**。

#### 3.1.2 条件概率路径

**条件概率路径** $p_t(x \mid z)$ 描述了给定特定数据点 $z$ 的条件下，系统在时间 $t$ 的概率分布。这个路径必须满足严格的边界条件：

- **起始条件**：$p_0(x \mid z) = p_{\text{init}}$（通常为标准高斯分布）
- **终止条件**：$p_1(x \mid z) = \delta_z$（狄拉克δ函数，表示确定性地位于点 $z$）

这样的路径设计确保了我们从已知的简单分布（噪声）出发，最终精确到达目标数据点。

#### 3.1.3 边际概率路径

在实际应用中，我们关心的不是单个数据点，而是整个数据集的分布。**边际概率路径** $p_t(x)$ 通过对所有可能的真实数据进行"平均"来获得：

$$
p_t(x) = \int p_t(x|z) p_{\text{data}}(z) dz
$$

这个积分的物理意义是：

1. 从真实数据集 $p_{\text{data}}$ 中随机选择一个样本 $z$
2. 根据条件概率路径 $p_t(x\mid z)$ 采样得到 $x$
3. 大量重复这个过程，得到的 $x$ 的分布就是 $p_t(x)$

#### 3.1.4 高斯条件概率路径

为了具体化这些抽象概念，我们引入最基本且重要的**高斯条件概率路径**：

$$
p_t(\cdot|z) = \mathcal{N}(\alpha_t z, \beta_t^2 I_d)
$$

其中参数 $\alpha_t$ 和 $\beta_t$ 满足边界条件：

- **$t=0$ 时**：$\alpha_0 = 0, \beta_0 = 1$ → $p_0(\cdot \mid z) = \mathcal{N}(0, I_d)$（标准高斯噪声）
- **$t=1$ 时**：$\alpha_1 = 1, \beta_1 = 0$ → $p_1(\cdot \mid z) = \delta_z$（原始数据）

**实用表示**：高斯路径上任意时刻的点可以通过简单的线性插值获得：

$$
x_t = \alpha_t z + \beta_t \epsilon
$$

其中 $\epsilon \sim \mathcal{N}(0, I_d)$ 是标准高斯噪声。

### 3.2 从概率路径到向量场：连续性方程的威力

#### 3.2.1 条件向量场的推导

对于给定数据点 $z$，存在一个**条件向量场** $u_t^{\text{target}}(x \mid z)$ 驱动概率分布沿着预定路径演化。

**核心推导思想**：寻找变换 $\psi_t^{\text{target}}(x_0\mid z)$，使得：

$$
X_0 \sim \mathcal{N}(0, I_d), \quad X_t = \psi_t^{\text{target}}(X_0 \mid z) \Rightarrow X_t \sim \mathcal{N}(\alpha_t z, \beta_t^2 I_d)
$$

构造变换：$\psi_t^{\text{target}}(x_0 \mid z) = \alpha_t z + \beta_t x_0$

根据链式法则：

$$
\frac{d}{dt}\psi_t^{\text{target}}(x_0 \mid z) = u_t^{\text{target}}(\psi_t^{\text{target}}(x_0 \mid z)|z)
$$

代入得到：

$$
\dot{\alpha}_t z + \dot{\beta}_t x_0 = u_t^{\text{target}}(\alpha_t z + \beta_t x_0  \mid z)
$$

通过变量替换和整理，最终得到**条件向量场的解析表达式**：

$$
u_t^{\text{target}}(x|z) = \left(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t}\alpha_t\right)z + \frac{\dot{\beta}_t}{\beta_t}x
$$

#### 3.2.2 边际向量场的边际化技巧

**关键定理**：边际向量场可以通过条件向量场的加权平均获得：

$$
u_t^{\text{target}}(x) = \int u_t^{\text{target}}(x|z) p_t(z \mid x) dz
$$

这里权重 $p_t(z \mid x)$ 是后验概率，表示给定观测 $x$ 时，它来源于数据点 $z$ 的概率。

**数学证明**：

基于连续性方程 $\frac{\partial p}{\partial t} = -\nabla \cdot (p \mathbf{u})$：

**宏观层面**：

$$
\frac{\partial p_t(x)}{\partial t} = -\nabla \cdot (p_t(x) u_t^{\text{target}}(x))
$$

**微观层面**：

$$
\frac{\partial p_t(x|z)}{\partial t} = -\nabla \cdot (p_t(x|z) u_t^{\text{target}}(x|z))
$$

结合全概率公式 $p_t(x) = \int p_t(x\mid z) p_{\text{data}}(z) dz$，通过求导交换和散度运算的线性性，可以严格证明上述边际化关系。

**物理直观**：边际向量场是所有可能条件向量场的"民主投票"结果，每个条件向量场的影响力由其对应的后验概率决定。

### 3.3 随机微分方程的修正项

#### 3.3.1 分数函数的引入

当我们使用随机微分方程时，随机扰动会使系统偏离理想路径。为了保持概率路径不变，我们需要引入**修正项**。

**分数函数（Score Function）**定义为：

$$
\nabla \log p_t(x)
$$

它指向概率密度增长最快的方向，在物理上可以理解为一种"恢复力"。

#### 3.3.2 SDE扩展技巧

**核心定理**：要在引入随机项 $\sigma_t dW_t$ 的同时保持概率路径 $p_t$ 不变，新的漂移项必须为：

$$
u_t = u_t^{\text{target}} + \frac{\sigma_t^2}{2}\nabla \log p_t
$$

**证明思路**：使用Fokker-Planck方程

SDE $dX_t = u_t(X_t)dt + \sigma_t dW_t$ 对应的概率演化方程为：

$$
\frac{\partial p_t}{\partial t} = -\nabla \cdot (p_t u_t) + \frac{\sigma_t^2}{2}\Delta p_t
$$

要保持原有的概率路径，我们需要：

$$
\frac{\partial p_t}{\partial t} = -\nabla \cdot (p_t u_t^{\text{target}})
$$

通过代数运算可以验证，选择修正后的漂移项确实能满足这个要求。

#### 3.3.3 高斯路径的分数函数

对于高斯条件概率路径：

$$
p_t(x|z) = C \exp\left(-\frac{\|x - \alpha_t z\|^2}{2\beta_t^2}\right)
$$

其分数函数为：

$$
\nabla \log p_t(x|z) = -\frac{x - \alpha_t z}{\beta_t^2}
$$

注意到 $\epsilon = \frac{x - \alpha_t z}{\beta_t}$，这解释了为什么许多实际模型选择训练神经网络来**预测噪声** $\epsilon$。

#### 3.3.4 朗之万动力学

作为一个有趣的特例，当 $u_t^{\text{target}} = 0$ 时，我们得到：

$$
dX_t = \frac{\sigma_t^2}{2}\nabla \log p_t(X_t) dt + \sigma_t dW_t
$$

这就是著名的**朗之万动力学（Langevin Dynamics）**，它在随机力和恢复力之间达到平衡，使得粒子的统计分布始终保持为 $p_t$。

---

## 第四章：训练算法的设计与实现

有了第三章推导出的理论基础，现在我们需要将这些数学洞察转化为可实际执行的训练算法。本章将详细介绍流匹配和分数匹配两种主要的训练方法。

### 4.1 流匹配：从理论到实践的优雅转换

#### 4.1.1 边际流匹配的理想与现实

最直观的想法是直接让神经网络 $u_\theta$ 逼近理想的边际向量场：

$$
L_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \text{Uniform}, x \sim p_t}\left[\|u_\theta(x) - u_t^{\text{target}}(x)\|^2\right]
$$

这个损失函数在理论上完美无缺，但在实践中面临严重挑战：计算边际向量场需要求解复杂的积分

$$
u_t^{\text{target}}(x) = \int u_t^{\text{target}}(x|z) \cdot \frac{p_t(x|z)p_{\text{data}}(z)}{p_t(x)} dz
$$

这个积分在高维空间中几乎无法精确计算。

#### 4.1.2 条件流匹配：巧妙的等价转换

为了绕过这个计算难题，研究者们提出了**条件流匹配**方法：

$$
L_{\text{CFM}}(\theta) = \mathbb{E}_{t \sim \text{Uniform}, z \sim p_{\text{data}}, x \sim p_t(\cdot|z)}\left[\|u_\theta(x) - u_t^{\text{target}}(x|z)\|^2\right]
$$

**关键定理**：$L_{\text{FM}}(\theta) = L_{\text{CFM}}(\theta) + C$（其中 $C$ 是与 $\theta$ 无关的常数）

**严格证明**：

展开边际流匹配损失：

$$
L_{\text{FM}}(\theta) = \mathbb{E}_{t,x}\left[\|u_t^\theta(x)\|^2 - 2u_t^\theta(x)^T u_t^{\text{target}}(x) + \|u_t^{\text{target}}(x)\|^2\right]
$$

关键在于证明第二项的等价性：

$$
\mathbb{E}_{t,x}[u_t^\theta(x)^T u_t^{\text{target}}(x)] = \mathbb{E}_{t,z,x \sim p_t(\cdot|z)}[u_t^\theta(x)^T u_t^{\text{target}}(x|z)]
$$

**推导过程**：
$$
\begin{aligned}
&\mathbb{E}_{t,x}[u_t^\theta(x)^T u_t^{\text{target}}(x)]\\
&= \int p_t(x) u_t^\theta(x)^T u_t^{\text{target}}(x) dx dt\\
&= \int p_t(x) u_t^\theta(x)^T \left[\int u_t^{\text{target}}(x|z) \frac{p_t(x|z)p_{\text{data}}(z)}{p_t(x)} dz\right] dx dt\\
&= \int u_t^\theta(x)^T u_t^{\text{target}}(x|z) p_t(x|z) p_{\text{data}}(z) dz dx dt\\
&= \mathbb{E}_{t,z \sim p_{\text{data}}, x \sim p_t(\cdot|z)}[u_t^\theta(x)^T u_t^{\text{target}}(x|z)]
\end{aligned}
$$


这个证明的精妙之处在于利用了边际化的数学性质，将复杂的边际期望转换为易于计算的条件期望。

#### 4.1.3 高斯路径的具体实现

将条件流匹配应用到高斯概率路径，结合第三章的推导结果：

$$
u_t^{\text{target}}(x|z) = \left(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t}\alpha_t\right)z + \frac{\dot{\beta}_t}{\beta_t}x
$$

损失函数变为：

$$
L_{\text{CFM}}(\theta) = \mathbb{E}_{t,z,\epsilon}\left[\left\|u_t^\theta(\alpha_t z + \beta_t \epsilon) - \left(\dot{\alpha}_t z + \dot{\beta}_t \epsilon\right)\right\|^2\right]
$$

**CondOT路径的简化**：选择 $\alpha_t = t$，$\beta_t = 1-t$，得到：

$$
\dot{\alpha}_t = 1, \quad \dot{\beta}_t = -1
$$

最终的优化目标简化为：

$$
L(\theta) = \mathbb{E}_{t,z,\epsilon}\left[\|u_t^\theta(tz + (1-t)\epsilon) - (z - \epsilon)\|^2\right]
$$

**训练算法的程序化描述**：

```
算法1: 条件流匹配训练
输入: 数据集 {z_i}, 神经网络 u_θ
for each training step:
    1. z ~ p_data           # 采样真实数据
    2. t ~ Uniform(0,1)     # 采样随机时间
    3. ε ~ N(0,I)          # 采样随机噪声
    4. x = t*z + (1-t)*ε   # 构造中间状态
    5. loss = ||u_θ(x,t) - (z-ε)||²
    6. 反向传播更新 θ
```

### 4.2 分数匹配：拥抱随机性的训练范式

#### 4.2.1 从SDE到分数匹配

当我们采用随机微分方程时，系统的演化方程变为：

$$
dX_t = \left[u_t^{\text{target}}(X_t) + \frac{\sigma_t^2}{2}\nabla \log p_t(X_t)\right] dt + \sigma_t dW_t
$$

这意味着我们需要同时学习向量场 $u_t^{\text{target}}$ 和分数函数 $\nabla \log p_t$。

#### 4.2.2 去噪分数匹配的核心思想

类似于条件流匹配的思路，我们可以用条件分数匹配来替代边际分数匹配：

$$
L_{\text{SM}}(\theta) = L_{\text{CSM}}(\theta) + C
$$

对于高斯概率路径，条件分数函数为：

$$
\nabla \log p_t(x|z) = -\frac{x - \alpha_t z}{\beta_t^2}
$$

注意到 $\epsilon = \frac{x - \alpha_t z}{\beta_t}$，我们可以训练网络直接预测噪声：

$$
s_\theta(x) = -\frac{\epsilon_\theta(x)}{\beta_t}
$$

**DDPM损失函数**：

$$
L_{\text{DDPM}}(\theta) = \mathbb{E}_{t,z,\epsilon}\left[\|\epsilon_\theta(\alpha_t z + \beta_t \epsilon) - \epsilon\|^2\right]
$$

**训练过程的物理图像**：

1. 取一张干净图片 $z$
2. 添加已知噪声 $\epsilon$ 得到加噪图片 $x$
3. 训练网络从加噪图片中"猜出"当初添加的噪声

#### 4.2.3 流匹配与分数匹配的统一

令人惊喜的是，在高斯概率路径下，这两种看似不同的方法实际上是**等价的**。

**转换公式**：

$$
u_t^{\text{target}}(x|z) = \left(\beta_t^2 \frac{\dot{\alpha}_t}{\alpha_t} - \dot{\beta}_t \beta_t\right) \nabla \log p_t(x|z) + \frac{\dot{\alpha}_t}{\alpha_t} x
$$

$$
u_t^{\text{target}}(x) = \left(\beta_t^2 \frac{\dot{\alpha}_t}{\alpha_t} - \dot{\beta}_t \beta_t\right) \nabla \log p_t(x) + \frac{\dot{\alpha}_t}{\alpha_t} x
$$

**证明思路**：

**Step 1**: 对于条件情况，从

$$
u_t^{\text{target}}(x|z) = \left(\dot{\alpha}_t - \frac{\dot{\beta}_t}{\beta_t}\alpha_t\right)z + \frac{\dot{\beta}_t}{\beta_t}x
$$

结合 $\nabla \log p_t(x \mid z) = \frac{\alpha_t z - x}{\beta_t^2}$，通过代数运算可以推导出转换公式。

**Step 2**: 对于边际情况，利用

$$
u_t^{\text{target}}(x) = \int u_t^{\text{target}}(x|z) p_t(z|x) dz
$$

和分数函数的边际化性质：

$$
\nabla \log p_t(x) = \int \nabla \log p_t(x|z) p_t(z|x) dz
$$

通过积分的线性性质，可以得到边际向量场的转换公式。

**重要结论**：在高斯概率路径的框架下，流匹配和分数匹配本质上等价，但在非高斯情况下，流匹配具有更好的普适性，因为向量场的定义更加直接，而分数函数与向量场的转换关系在复杂分布下可能不易建立。

---

## 第五章：构建实用的图像生成系统

经过前四章的理论铺垫，我们现在已经掌握了生成模型的核心数学原理。本章将这些理论知识转化为实际可用的图像生成系统，重点讨论条件生成和引导技术。

### 5.1 从技术概念到应用概念的转换

在开始具体实现之前，我们需要明确区分两个重要但不同的概念：

- **条件向量场** $u_t^{\text{target}}(x\mid z)$：这是第三章推导出的技术概念，描述给定数据点 $z$ 时的演化规律，主要用于模型内部的训练目标构建
- **引导向量场** $u_t^{\text{target}}(x\mid y)$：这是面向应用的概念，描述如何根据用户指令 $y$（如文本描述、类别标签等）来生成相应的内容

我们的目标是训练一个**引导生成模型**：

- **输入**：当前状态 $x$、时间 $t$、用户条件 $y$
- **输出**：引导向量场 $u_t^\theta(x\mid y)$
- **目标**：生成符合条件分布 $p_{\text{data}}(x\mid y)$ 的样本

### 5.2 流模型的条件引导

#### 5.2.1 基础引导训练

最直接的方法是将第四章的条件流匹配扩展到引导场景：

$$
L_{\text{CFM-guided}}(\theta) = \mathbb{E}_{(z,y) \sim p_{\text{data}}, t \sim \text{Uniform}, x \sim p_t(\cdot|z)}\left[\|u_\theta(x|y) - u_t^{\text{target}}(x|z)\|^2\right]
$$

这个损失函数的物理含义是：训练网络根据给定的指令 $y$，正确地预测如何将噪声状态 $x$ 向对应的真实数据 $z$ 演化。

**训练数据的构造**：

- 数据集中的每个样本 $z$ 都配有相应的标签或描述 $y$
- 形成配对数据 $(z, y)$ 用于训练

#### 5.2.2 无分类器引导：现代生成模型的核心

实践中发现，基础引导方法生成的样本虽然与条件相关，但**条件一致性**往往不够强。为了解决这个问题，研究者开发了**无分类器引导（Classifier-Free Guidance, CFG）**技术。

**数学理论基础**：

从条件概率的贝叶斯公式出发：

$$
\nabla \log p_t(x|y) = \nabla \log p_t(x) + \nabla \log p_t(y|x)
$$

这个等式告诉我们，条件分数函数可以分解为：

- **无条件分数** $\nabla \log p_t(x)$：描述数据的一般特征
- **分类器分数** $\nabla \log p_t(y\mid x)$：描述当前状态与目标条件的匹配程度

**引导增强策略**：

为了增强条件的影响力，我们可以人为地放大分类器项：

$$
\tilde{\nabla} \log p_t(x|y) = \nabla \log p_t(x) + w \cdot \nabla \log p_t(y|x)
$$

其中 $w > 1$ 是引导强度参数。

结合第三章的转换公式：

$$
u_t^{\text{target}}(x|y) = a_t x + b_t \nabla \log p_t(x|y)
$$

我们得到增强后的向量场：

$$
\tilde{u}_t(x|y) = u_t^{\text{target}}(x) + w \cdot b_t \nabla \log p_t(y|x)
$$

**CFG的关键洞察**：

利用 $\nabla \log p_t(y\mid x) = \nabla \log p_t(x\mid y) - \nabla \log p_t(x)$，最终的CFG公式为：

$$
\tilde{u}_t(x|y) = (1-w) u_t^{\text{target}}(x) + w u_t^{\text{target}}(x|y)
$$

这个公式的美妙之处在于：

1. **两次预测**：分别在有条件和无条件情况下预测向量场
2. **线性外插**：通过线性组合增强条件信息的影响
3. **单一模型**：只需要训练一个模型即可实现两种预测

**训练策略**：

为了让同一个模型同时具备条件和无条件生成能力：

- 训练时以一定概率（如15%）**随机丢弃条件** $y$
- 用特殊的**空标签** $\varnothing$ 替代被丢弃的条件
- 模型学会区分真实条件和空标签，自动获得两种能力

**生成时的CFG流程**：

```
算法2: CFG引导生成
输入: 噪声 x₀, 条件 y, 引导强度 w
for t in [0, 1]:
    # 无条件预测
    u_uncond = u_θ(x_t, t, ∅)
  
    # 有条件预测  
    u_cond = u_θ(x_t, t, y)
  
    # CFG组合
    u_guided = (1-w) * u_uncond + w * u_cond
  
    # 更新状态
    x_{t+dt} = x_t + dt * u_guided
```

### 5.3 扩散模型的条件引导

扩散模型的引导技术与流模型平行发展，核心思想相同但实现细节略有不同。

#### 5.3.1 分数函数的CFG

对于扩散模型，CFG公式变为：

$$
\tilde{s}_t(x|y) = (1-w) \nabla \log p_t(x|\varnothing) + w \nabla \log p_t(x|y)
$$

#### 5.3.2 噪声预测的CFG

在DDPM框架下，CFG可以直接应用于噪声预测：

$$
\tilde{\epsilon}(x|y) = (1-w) \epsilon_\theta(x|\varnothing) + w \epsilon_\theta(x|y)
$$

这种形式更加直观，也是目前主流扩散模型（如Stable Diffusion、DALL-E等）采用的标准做法。

### 5.4 引导强度的选择与效果

**引导强度 $w$ 的影响**：

- $w = 1$：标准的条件生成，平衡多样性和条件一致性
- $w > 1$：增强条件一致性，但可能降低多样性
- $w < 1$：增加随机性，条件约束较弱
- $w = 0$：完全无条件生成

**实践建议**：

- 文本到图像任务：通常选择 $w \in [7, 15]$
- 类别条件生成：通常选择 $w \in [1, 5]$
- 需要根据具体应用场景调优

### 5.5 现代生成系统的架构总结

**完整的条件生成系统**包含：

1. **数据预处理**：构建 $(z, y)$ 配对数据集
2. **条件编码**：将文本/标签转换为向量表示
3. **网络架构**：设计能处理条件输入的神经网络
4. **训练策略**：实施条件dropout训练
5. **推理引导**：应用CFG技术增强生成质量

这个完整的流程将第一章的概念框架、第二章的数学工具、第三章的理论推导、第四章的训练算法，最终整合为可以实际部署的生成系统。

---

## 结语：从数学理论到实践应用的完整之旅

通过这五章的深入探索，我们完成了从生成模型最基础的概念到实际应用系统的完整理论之旅。

### 核心思想的回顾

**第一章**建立了生成建模的概念基础：万物皆向量，生成即采样，目标是学习从噪声到数据的映射。

**第二章**介绍了两大数学引擎：确定性的常微分方程（ODE）和随机性的随机微分方程（SDE），为后续的理论推导奠定了工具基础。

**第三章**通过严谨的数学推导，从第一性原理出发构建了训练目标，解决了"学什么"的问题。

**第四章**将理论转化为实际可执行的训练算法，解决了"怎么学"的问题。

**第五章**展示了如何将训练好的模型转化为实用的生成系统，解决了"怎么用"的问题。

### 致谢

感谢MIT 6.S184课程的授课团队为我们提供了如此高质量的学习资源，也感谢所有为生成模型理论发展做出贡献的研究者们。希望这份笔记能够帮助更多的学习者理解和掌握这些美妙的数学理论。

**愿我们都能在从噪声到数据的神奇变换中，感受到数学与艺术交融的无穷魅力！**
