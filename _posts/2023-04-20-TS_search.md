---
title: 'Transition State Searches on Neural Network Potential-Energy Surfaces'
date: 2023-04-17
permalink: /posts/2020/04/ts_nnp/
tags:
  - research
  - TS
  - GNNP
---

Background
======
Predicting reactive behavior is essential to catalysis, drug design, and the development of new materials. Reactive behavior of a system can be described through thermodynamic and kinetic rate parameters, however, theoretical calculation of these parameters requires knowledge of the transition state.
Locating transition states is a computationally expensive task that frequently requires significant human intervention. Transition states are generally found through path finding algorithms that explore the potential energy surface between minima. Significant efforts have been made to develop more efficient path finding algorithms as evaluation of ab initio potentials is computationally expensive.
In this work we train a graph neural network on quantum chemical calculations of organic molecules and use the resulting model with a nonlocal path-finding algorithm to find the transition state of organic reactions.

Transition State Search Algorithms
======
Modern transition state search algorithms can be split into two steps, first, a nonlocal path-finding algorithm gives a guess of the transition state location on the potential energy surface. Second, a local surface walking algorithm refines the transition state guess to the exact transition state. The energy and gradient of the potential are evaluated at every location passed through by either algorithm, therefor in the number of gradient calculations is frequently uses as a metric for the efficiency of an algorithm.

In this work we use an improved implementation of the freezing string method (FSM) as our nonlocal path-finding method and use QChems local surface-walking algorithm to refine all guesses. The FSM takes a chain of states approach to locating transition states which exist as first order saddle points connecting minima on the potential energy surface. The chain of states refers to a series of interpolated intermediate structures connecting the product and reactant, referred to as a string. In the FSM, the string is constructed by interpolating a fixed distance in the direction of the opposing minima and creating a new structure. This structure is then optimized down the perpendicular component of the gradient n times, such that it lies approximately along the minimum energy path, and the process is repeated until the string is fully formed. The highest energy image in the string is taken as the transition state guess. The progression of the freezing string method is shown below in figure 2. In this version of the FSM, we use linear synchronous transit (LST) interpolation to generate realistic initial structures. These structures are optimized using the conjugate gradient method.
<figure>
    <center><img src='/images/fsm_gif_hyper-2.gif' width='800' height='700'></center>
    <figcaption>Figure 1: The freezing string method on the Müller-Brown potential</figcaption>
</figure>

Graph Neural Networks in Chemistry
======
Quantum chemical calculations predict the energies and forces of moderately sized chemical systems with high levels of accuracy, which has enabled detailed understanding of chemical processes at the atomic level. However, the high computational cost of these methods prohibits their use in simulations where many calculations are required. 
    Neural network potentials (NNPs) have emerged as an attractive alternative. Neural network potential energy surfaces are constructed by training neural networks on structure-energy data resulting from quantum chemical methods. Modern NNPs achieve chemical accuracy (error<2kcal/mol) while predicting the potential-energy of a system as a function of the atomic positions at a fraction of the computational cost of quantum chemistry methods.
Graphs are mathematical structures consisting of nodes connected by edges and are frequently used to model pairwise relations between objects. Using graphs to represent molecules is intuitive, atoms are represented by nodes and bonds are represented by edges. Graph neural networks (GNNs) take graph structured data as the input and make a prediction about the entire graph, or individual nodes or edges. In this work we create molecular graphs, where each node contains the chemical identity and X,Y,Z coordinates of an atom, bonds are inferred from interatomic distances. Graph neural networks use a message passing scheme to construct a feature vector for each node that is representative of the electronic environment and predict the potential-energy of the molecule, shown in figure 3.
<figure>
    <center><img src='/images/Formaldehyde.jpg' width='400' height='700'></center>
    <figcaption>Figure 2: Time to predict molecular properties via quantum chemistry and graph neural networks</figcaption>
</figure>
In this work we train SchNet, a GNN, on the ANI-1 dataset. The ANI-1 dataset consists of energies computed at the ωB97X/6-31G* level of theory for minimum energy structures of small organic molecules and off-equilibrium conformations obtained by normal mode sampling. We use Bayesian optimization to search for optimal model parameters. When using an 80/20 training/validation split on the 23M conformers in the ANI-1 dataset we achieve a validation MSE of 1.42 with our best model. 

Results
======
We benchmark this methodology with a set of classic organic reactions, and a set of 50 organic elementary reactions. In the case of 50 organic reactions the GNNP based methods find the transition state in 44/50 reactions whereas ab initio based methods are successful in 49/50 methods, shown in figure 2.
<figure>
    <center><img src='/images/MIT50_bayesian.jpeg' width='400' height='700'></center>
    <figcaption>Figure 3: Gradient calls required to find the transition state from reactant and product for reactions drawn from a 2020 dataset of elementary organic reactions</figcaption>
</figure>
The results of this methodology on the set of classic organic reactions are shown below in figure 3. This benchmark set covers a variety of reaction types, including formation/breaking of pi bonds, ring formation, and isomerization. When successful, we find that GNNP driven searches require significantly fewer gradient calculations. 
<figure>
    <center><img src='/images/Shaama_set_bayesian.jpeg' width='400' height='700'></center>
    <figcaption>Figure 4: Gradient calls required to find the transition state of classic organic test reactions using large (11 Nodes) and small (21 Nodes) spaced freezing string method calculations using a GNNP and a quantum chemistry potential</figcaption>
</figure>
In some exceptions, marked by an asterisk, the transition state is not found. It should be noted that these exceptions do not exclusively occur with GNNP based calculations, and are a result of both algorithm hyperparameters and the potential.

There are significant computational cost savings during the path finding optimization by replacing the electronic energy gradient calls with gradients of neural network potential energy function obtained by automatic differentiation. In each reaction across both sets of reactions the calculations using GNNP during the nonlocal path-finding step require significantly fewer gradient calculations, as the GNNP gradient calculations are negligible compared to ab initio gradient calculations. 

