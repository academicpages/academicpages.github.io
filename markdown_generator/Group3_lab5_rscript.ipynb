{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99aa613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from Penn (pos = 3):\n",
      "\n",
      "    abdt, agegt54, agelt35, black, dep, durable, female, hispanic,\n",
      "    husd, inuidur1, inuidur2, lusd, muld, nondurable, othrace, q1, q2,\n",
      "    q3, q4, q5, q6, recall, tg\n",
      "\n",
      "The following objects are masked from Penn (pos = 5):\n",
      "\n",
      "    abdt, agegt54, agelt35, black, dep, durable, female, hispanic,\n",
      "    husd, inuidur1, inuidur2, lusd, muld, nondurable, othrace, q1, q2,\n",
      "    q3, q4, q5, q6, recall, tg\n",
      "\n",
      "The following objects are masked from Penn (pos = 6):\n",
      "\n",
      "    abdt, agegt54, agelt35, black, dep, durable, female, hispanic,\n",
      "    husd, inuidur1, inuidur2, lusd, muld, nondurable, othrace, q1, q2,\n",
      "    q3, q4, q5, q6, recall, tg\n",
      "\n",
      "The following objects are masked from Penn (pos = 7):\n",
      "\n",
      "    abdt, agegt54, agelt35, black, dep, durable, female, hispanic,\n",
      "    husd, inuidur1, inuidur2, lusd, muld, nondurable, othrace, q1, q2,\n",
      "    q3, q4, q5, q6, recall, tg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part of Bootstraping\n",
    "## loading the data\n",
    "library(boot)\n",
    "Penn <- as.data.frame(read.table(\"../data/penn_jae.dat\", header=T ))\n",
    "n <- dim(Penn)[1]\n",
    "p_1 <- dim(Penn)[2]\n",
    "Penn<- subset(Penn, tg== 4| tg==0)\n",
    "attach(Penn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "263fbf8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(ISLR2): there is no package called 'ISLR2'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(ISLR2): there is no package called 'ISLR2'\nTraceback:\n",
      "1. library(ISLR2)"
     ]
    }
   ],
   "source": [
    "library(ISLR2)\n",
    "set.seed(1)\n",
    "library(boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbe84559",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in (tg == 4) | summary(T4): solo son posibles operaciones para variables de tipo numérico, compleja o lógico\n",
     "output_type": "error",
     "traceback": [
      "Error in (tg == 4) | summary(T4): solo son posibles operaciones para variables de tipo numérico, compleja o lógico\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "T4<- (tg==4)|\n",
    "summary(T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ece167ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:5:0: unexpected end of input\n3:                            \n4:                      \n  ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:5:0: unexpected end of input\n3:                            \n4:                      \n  ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Here we define the structure of our regression:\n",
    "regresion <- log(inuidur1)~T4+ (female+black+othrace+factor(dep)+q2+q3+q4+q5+q6+agelt35+agegt54+durable+lusd+husd\n",
    "                           \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dbc443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot.fn <- function(data, index) {\n",
    "  coef(lm(log(inuidur1)~T4+ (female+black+othrace+factor(dep)+q2+q3+q4+q5+q6+agelt35+agegt54+durable+lusd+husd), data = data, subset = index)) } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed5ccc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Penn, statistic = boot.fn, R = 1000)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "         original        bias    std. error\n",
       "t1*   2.178462326  0.0052223099  0.15609830\n",
       "t2*  -0.071692484  0.0002443722  0.03633901\n",
       "t3*   0.126368328 -0.0005970869  0.03548195\n",
       "t4*  -0.293767980 -0.0023950938  0.05902701\n",
       "t5*  -0.472445058 -0.0009669991  0.23662611\n",
       "t6*   0.029866899 -0.0015803274  0.05316184\n",
       "t7*   0.096186517  0.0001454218  0.04751720\n",
       "t8*   0.073678072 -0.0047203080  0.15280807\n",
       "t9*  -0.038506537 -0.0029706659  0.15249683\n",
       "t10* -0.054949195 -0.0046900884  0.15342405\n",
       "t11* -0.144177912 -0.0042572281  0.15104799\n",
       "t12*  0.003361318 -0.0024182498  0.16165529\n",
       "t13* -0.162772168 -0.0000218298  0.03768461\n",
       "t14*  0.229666708 -0.0001082586  0.05800139\n",
       "t15*  0.126557359 -0.0024061924  0.04917096\n",
       "t16* -0.175352572 -0.0007373578  0.04123540\n",
       "t17* -0.105224727 -0.0003037563  0.04271973"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, here we define our bootstapping proccess: \n",
    "example <- boot(Penn, boot.fn , 1000)\n",
    "example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c831e752",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in stats::model.frame(formula = name, data = Penn, drop.unused.levels = TRUE): objeto 'name' no encontrado\n",
     "output_type": "error",
     "traceback": [
      "Error in stats::model.frame(formula = name, data = Penn, drop.unused.levels = TRUE): objeto 'name' no encontrado\nTraceback:\n",
      "1. summary(lm(name, data = Penn))",
      "2. lm(name, data = Penn)",
      "3. eval(mf, parent.frame())",
      "4. eval(mf, parent.frame())",
      "5. stats::model.frame(formula = name, data = Penn, drop.unused.levels = TRUE)"
     ]
    }
   ],
   "source": [
    "# We do this to know which is the orden of our coefficents.\n",
    "summary(lm(name, data = Penn))$coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23e5cbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nSUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of t\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71700dc",
   "metadata": {},
   "source": [
    "# HTE I: Binary treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbbfc3f",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"devtools\")  # if you don't have this installed yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "devtools::install_github('susanathey/causalTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The causalTree package is not in CRAN, the most common R repository.\n",
    "# To install it, uncomment the next lines as appropriate.\n",
    "install.packages(\"devtools\")  # if you don't have this installed yet.\n",
    "devtools::install_github('susanathey/causalTree') \n",
    "library(causalTree)\n",
    "\n",
    "# use e.g., install.packages(\"grf\") to install any of the following packages.\n",
    "library(grf)\n",
    "library(rpart)\n",
    "library(glmnet)\n",
    "library(splines)\n",
    "library(MASS)\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f579e",
   "metadata": {},
   "source": [
    "As with other chapters in this tutorial, the code below should still work by replacing the next snippet of code with a different dataset, provided that you update the key variables `treatment`, `outcome`, and `covariates` below. Also, please make sure to read the comments as they may be subtle differences depending on whether your dataset was created in a randomized or observational setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data <- as.data.frame(read.table(\"../data/penn_jae.dat\", header=T ))\n",
    "n <- nrow(data)\n",
    "data<- subset(data, tg== 4| tg==0)\n",
    "attach(data)\n",
    "T4<- (tg==4)\n",
    "summary(T4)\n",
    "treatment <- \"tg\"\n",
    "\n",
    "outcome <- \"inuidur1\"\n",
    "\n",
    "# Additional covariates\n",
    "covariates <- c(\"female\",\"black\",\"othrace\",\"factor\",\"dep\",\"q2\",\"q3\",\"q4\",\"q5\",\"q6\",\"agelt35\",\"agegt54\",\"durable\",\"lusd\",\"husd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b92a4",
   "metadata": {},
   "source": [
    "## Pre-specified hypotheses\n",
    "\n",
    "We will begin by learning how to test pre-specified null hypotheses of the form\n",
    "\\begin{equation} \n",
    "\\label{eq:1}\n",
    "H_{0}: E[Y(1) - Y(0) | G_i = 1] = E[Y(1) - Y(0) | G_i = 0] \n",
    "\\end{equation}\n",
    "\n",
    "That is, that the treatment effect is the same regardless of membership to some group\n",
    "$G_i$. Importantly, for now we’ll assume that the group $G_i$ was **pre-specified** -- it was decided _before_ looking at the data.\n",
    "\n",
    "In a randomized setting, if the both the treatment  $W_i$ and group membership $G_i$ are binary, we can write\n",
    "\\begin{equation}\n",
    "  E[Y_i(W_i)|G_i] = E[Y_i|W_i, G_i] = \\beta_0 + \\beta_w W_i + \\beta_g G_i + \\beta_{wg} W_i G_i\n",
    "\\end{equation}\n",
    "\n",
    "<font size=1>\n",
    "When $W_i$ and $G_i$ are binary, this decomposition is true without loss of generality. Why?\n",
    "</font>\n",
    "\n",
    "This allows us to write the average effects of $W_i$ and $G_i$ on $Y_i$ as\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    E[Y(1) | G_i=1] &= \\beta_0 + \\beta_w W_i + \\beta_g G_i + \\beta_{wg} W_i G_i, \\\\\n",
    "    E[Y(1) | G_i=0] &= \\beta_0 + \\beta_w W_i,  \\\\\n",
    "    E[Y(0) | G_i=1] &= \\beta_0 + \\beta_g G_i,  \\\\\n",
    "    E[Y(0) | G_i=0] &= \\beta_0.\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Rewriting the null hypothesis \\ref{eq:1} in terms of the decomposition \\@ref(eq:decomp), we see that it boils down to a test about the coefficient in the interaction: $\\beta_{xw} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only valid in randomized settings\n",
    "\n",
    "# Suppose this his group was defined prior to collecting the data\n",
    "# Recall from last chapter -- this is equivalent to running a t-test\n",
    "fmla <- log(inuidur1)~T4+ (female+black+othrace+factor(dep)+q2+q3+q4+q5+q6+agelt35+agegt54+durable+lusd+husd)\n",
    "ols <- lm(fmla, data=data)\n",
    "coeftest(ols, vcov=vcovHC(ols, type='HC2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286bcdc7",
   "metadata": {},
   "source": [
    "## Data-driven hypotheses\n",
    "\n",
    "Pre-specifying hypotheses prior to looking at the data is in general good practice to avoid \"p-hacking\" (e.g., slicing the data into different subgroups until a significant result is found). However, valid tests can also be attained if by **sample splitting**: we can use a subset of the sample to find promising subgroups, then test hypotheses about these subgroups in the remaining sample. This kind of sample splitting for hypothesis testing is called **honesty**.\n",
    "\n",
    "### Via causal trees\n",
    "\n",
    "**Causal trees** [(Athey and Imbens)](PNAS, 2016)](https://www.pnas.org/content/pnas/113/27/7353.full.pdf) are an intuitive algorithm that is available in the randomized setting to discover subgroups with different treatment effects.\n",
    "\n",
    "At a high level, the idea is to divide the sample into three subsets (not necessarily of equal size). The `splitting` subset is used to fit a decision tree whose objective is modified to maximize heterogeneity in treatment effect estimates across leaves. The `estimation` subset is then used to produce a valid estimate of the treatment effect at each leaf of the fitted tree. Finally, a `test` subset can be used to validate the tree estimates.\n",
    "\n",
    "The next snippet uses `honest.causalTree` function from the [`causalTree`](https://github.com/susanathey/causalTree) package. For more details, see the [causalTree documentation](https://github.com/susanathey/causalTree/blob/master/briefintro.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only valid for randomized data!\n",
    "fmla <- paste(outcome, \" ~\", paste(covariates, collapse = \" + \"))\n",
    "fmla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing data into three subsets\n",
    "indices <- split(seq(nrow(data)), sort(seq(nrow(data)) %% 3))\n",
    "names(indices) <- c('split', 'est', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the forest\n",
    "ct.unpruned <- honest.causalTree(\n",
    "  formula=fmla,            # Define the model\n",
    "  data=data[indices$split,],\n",
    "  treatment=data[indices$split, treatment],\n",
    "  est_data=data[indices$est,],\n",
    "  est_treatment=data[indices$est, treatment],\n",
    "  minsize=1,                 # Min. number of treatment and control cases in each leaf\n",
    "  HonestSampleSize=length(indices$est), #  Num obs used in estimation after splitting\n",
    "  \n",
    "  # We recommend not changing the parameters below\n",
    "  split.Rule=\"CT\",            # Define the splitting option\n",
    "  cv.option=\"TOT\",            # Cross validation options\n",
    "  cp=0,                       # Complexity parameter\n",
    "  split.Honest=TRUE,          # Use honesty when splitting\n",
    "  cv.Honest=TRUE              # Use honesty when performing cross-validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of cross-validated values by tuning parameter.\n",
    "ct.cptable <- as.data.frame(ct.unpruned$cptable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain optimal complexity parameter to prune tree.\n",
    "cp.selected <- which.min(ct.cptable$xerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.optimal <- ct.cptable[cp.selected, \"CP\"]\n",
    "\n",
    "# Prune the tree at optimal complexity parameter.\n",
    "ct.pruned <- prune(tree=ct.unpruned, cp=cp.optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict point estimates (on estimation sample)\n",
    "tau.hat.est <- predict(ct.pruned, newdata=data[indices$est,])\n",
    "\n",
    "# Create a factor column 'leaf' indicating leaf assignment in the estimation set\n",
    "num.leaves <- length(unique(tau.hat.est))\n",
    "leaf <- factor(tau.hat.est, levels=sort(unique(tau.hat.est)), labels = seq(num.leaves))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d80b4",
   "metadata": {},
   "source": [
    "Note: if your tree is not splitting at all, try decreasing the parameter `minsize` that controls the minimum size of each leaf. The next snippet plots the learned tree. The values in the cell are the estimated treatment effect and an estimate of the fraction of the population that falls within each leaf. Both are estimated using the `estimation` sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpart.plot(\n",
    "  x=ct.pruned,        # Pruned tree\n",
    "  type=3,             # Draw separate split labels for the left and right directions\n",
    "  fallen=TRUE,        # Position the leaf nodes at the bottom of the graph\n",
    "  leaf.round=1,       # Rounding of the corners of the leaf node boxes\n",
    "  extra=100,          # Display the percentage of observations in the node\n",
    "  branch=.1,          # Shape of the branch lines\n",
    "  box.palette=\"RdBu\") # Palette for coloring the node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266dd53b",
   "metadata": {},
   "source": [
    "<font size=1>\n",
    "Interpret the heatmap above. What describes the subgroups with strongest and weakest estimated treatment effect?\n",
    "</font>\n",
    "\n",
    "\n",
    "### Via grf\n",
    "\n",
    "The function `causal_forest` from the package `grf` allows us to get estimates of the CATE \\@ref(eq:cate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc48615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from forest fitted above.\n",
    "tau.hat <- predict(forest.tau)$predictions  # tau(X) estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbde9a1",
   "metadata": {},
   "source": [
    "Having fit a non-parametric method such as a causal forest, a researcher may (incorrectly) start by looking at the distribution of its predictions of the treatment effect. One might be tempted to think: \"if the histogram is concentrated at a point, then there is no heterogeneity; if the histogram is spread out, then our estimator has found interesting heterogeneity.\" However, this may be false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not use this for assessing heterogeneity. See text above.\n",
    "hist(tau.hat, main=\"CATE estimates\", freq=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e426c8",
   "metadata": {},
   "source": [
    "If the histogram is concentrated at a point, we may simply be underpowered: our method was not able to detect any heterogeneity, but maybe it would detect it if we had more data. If the histogram is spread out, we may be overfitting: our model is producing very noisy estimates $\\widehat{\\tau}(x)$, but in fact the true  $\\tau(x)$ can be much smoother as a function of $x$.\n",
    "\n",
    "The `grf` package also produces a measure of variable importance that indicates how often a variable was used in a tree split. Again, much like the histogram above, this can be a rough diagnostic, but it should not be interpreted as indicating that, for example, variable with low importance is not related to heterogeneity. The reasoning is the same as the one presented in the causal trees section: if two covariates are highly correlated, the trees might split on one covariate but not the other, even though both (or maybe neither) are relevant in the true data-generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp <- c(variable_importance(forest.tau))\n",
    "names(var_imp) <- covariates\n",
    "sorted_var_imp <- sort(var_imp, decreasing = TRUE)\n",
    "sorted_var_imp[1:5]  # showing only first few"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c34e9",
   "metadata": {},
   "source": [
    "#### Data-driven subgroups\n",
    "\n",
    "Just as with causal trees, we can use causal forests to divide our observations into subgroups. In place of leaves, we'll rank observation into (say) quintiles according to their estimated CATE prediction; see, e.g., [Chernozhukov, Demirer, Duflo, Fernández-Val (2020)](https://arxiv.org/abs/1712.04802) for similar ideas.\n",
    "\n",
    "There's a subtle but important point that needs to be addressed here. As we have mentioned before, when predicting the conditional average treatment effect $\\tau(X_i)$ for observation $i$ we should in general avoid using a model that was fitted using observation $i$. This sort of sample splitting (which we called **honesty** above) is one of the required ingredients to get unbiased estimates of the CATE using the methods described here. However, when ranking estimates of two observations $i$ and $j$, we need something a little stronger: we must ensure that the model was not fit using _either_ $i$ _or_ $j$'s data. \n",
    "\n",
    "One way of overcoming this obstacle is simple. First, divide the data into $K$ folds (subsets). Then, cycle through the folds, fitting a CATE model on $K-1$ folds. Next, for each held-out fold, _separately_ rank the unseen observations into $Q$ groups based on their prediction  (i.e., if $Q=5$, then we rank observations by estimated CATE into \"top quintile\", \"second quintile\", and so on). After concatenating the independent rankings together, we can study the differences in observations in each rank-group. \n",
    "\n",
    "[This gist](https://gist.github.com/halflearned/bea4e5137c0c81fd18a75f682da466c8) computes the above for `grf`, and it should not be hard to modify it so as to replace forests by any other non-parametric method. However, for `grf` specifically, there's a small trick that allows us to obtain a valid ranking: we can pass a vector of fold indices to the argument `clusters` and rank observations within each fold. This works because estimates for each fold (\"cluster\")   trees are computed using trees that were not fit using observations from that fold. Here's how to do it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid randomized data and observational data with unconfoundedness+overlap.\n",
    "# Note: read the comments below carefully. \n",
    "# In randomized settings, do not estimate forest.e and e.hat; use known assignment probs.\n",
    "\n",
    "# Prepare dataset\n",
    "fmla <- formula(paste0(\"~ 0 + \", paste0(covariates, collapse=\"+\")))\n",
    "X <- model.matrix(fmla, data)\n",
    "W <- data[,treatment]\n",
    "Y <- data[,outcome]\n",
    "\n",
    "# Number of rankings that the predictions will be ranking on \n",
    "# (e.g., 2 for above/below median estimated CATE, 5 for estimated CATE quintiles, etc.)\n",
    "num.rankings <- 5  \n",
    "\n",
    "# Prepare for data.splitting\n",
    "# Assign a fold number to each observation.\n",
    "# The argument 'clusters' in the next step will mimick K-fold cross-fitting.\n",
    "num.folds <- 10\n",
    "folds <- sort(seq(n) %% num.folds) + 1\n",
    "\n",
    "# Comment or uncomment depending on your setting.\n",
    "# Observational setting with unconfoundedness+overlap (unknown assignment probs):\n",
    "# forest <- causal_forest(X, Y, W, clusters = folds)\n",
    "# Randomized settings with fixed and known probabilities (here: 0.5).\n",
    "forest <- causal_forest(X, Y, W, W.hat=.5, clusters = folds)\n",
    "\n",
    "# Retrieve out-of-bag predictions.\n",
    "# Predictions for observation in fold k will be computed using \n",
    "# trees that were not trained using observations for that fold.\n",
    "tau.hat <- predict(forest)$predictions\n",
    "\n",
    "# Rank observations *within each fold* into quintiles according to their CATE predictions.\n",
    "ranking <- rep(NA, n)\n",
    "for (fold in seq(num.folds)) {\n",
    "  tau.hat.quantiles <- quantile(tau.hat[folds == fold], probs = seq(0, 1, by=1/num.rankings))\n",
    "  ranking[folds == fold] <- cut(tau.hat[folds == fold], tau.hat.quantiles, include.lowest=TRUE,labels=seq(num.rankings))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5fbfb",
   "metadata": {},
   "source": [
    "The next snippet computes the average treatment effect within each group defined above, i.e., $\\E[Y_i(1) - Y_i(0)|G_i = g]$. This can done in two ways. First, by computing a simple difference-in-means estimate of the ATE based on observations within each group. This is valid only in randomized settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f00aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Valid only in randomized settings.\n",
    "# Average difference-in-means within each ranking\n",
    "\n",
    "# Formula y ~ 0 + ranking + ranking:w\n",
    "fmla <- paste0(outcome, \" ~ 0 + ranking + ranking:\", treatment)\n",
    "ols.ate <- lm(fmla, data=transform(data, ranking=factor(ranking)))\n",
    "ols.ate <- coeftest(ols.ate, vcov=vcovHC(ols.ate, type='HC2'))\n",
    "interact <- which(grepl(\":\", rownames(ols.ate)))\n",
    "ols.ate <- data.frame(\"ols\", paste0(\"Q\", seq(num.rankings)), ols.ate[interact, 1:2])\n",
    "rownames(ols.ate) <- NULL # just for display\n",
    "colnames(ols.ate) <- c(\"method\", \"ranking\", \"estimate\", \"std.err\")\n",
    "ols.ate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456eabc",
   "metadata": {},
   "source": [
    "Another option is to average the AIPW scores within each group. This valid for both randomized settings and observational settings with unconfoundedness and overlap. Moreover, AIPW-based estimators should produce estimates with tighter confidence intervals in large samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d34b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f876be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTE I: Binary treatment\n",
    "\n",
    "## Analysis\n",
    "\n",
    "install.packages(\"devtools\")  # if you don't have this installed yet.\n",
    "\n",
    "\n",
    "devtools::install_github('susanathey/causalTree')\n",
    "\n",
    "# The causalTree package is not in CRAN, the most common R repository.\n",
    "# To install it, uncomment the next lines as appropriate.\n",
    "install.packages(\"devtools\")  # if you don't have this installed yet.\n",
    "devtools::install_github('susanathey/causalTree') \n",
    "library(causalTree)\n",
    "\n",
    "# use e.g., install.packages(\"grf\") to install any of the following packages.\n",
    "library(grf)\n",
    "library(rpart)\n",
    "library(glmnet)\n",
    "library(splines)\n",
    "library(MASS)\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "library(ggplot2)\n",
    "\n",
    "As with other chapters in this tutorial, the code below should still work by replacing the next snippet of code with a different dataset, provided that you update the key variables `treatment`, `outcome`, and `covariates` below. Also, please make sure to read the comments as they may be subtle differences depending on whether your dataset was created in a randomized or observational setting.\n",
    "\n",
    "# Read in data\n",
    "data <- as.data.frame(read.table(\"../data/penn_jae.dat\", header=T ))\n",
    "n <- nrow(data)\n",
    "data<- subset(data, tg== 4| tg==0)\n",
    "attach(data)\n",
    "T4<- (tg==4)\n",
    "summary(T4)\n",
    "treatment <- \"tg\"\n",
    "\n",
    "outcome <- \"inuidur1\"\n",
    "\n",
    "# Additional covariates\n",
    "covariates <- c(\"female\",\"black\",\"othrace\",\"factor\",\"dep\",\"q2\",\"q3\",\"q4\",\"q5\",\"q6\",\"agelt35\",\"agegt54\",\"durable\",\"lusd\",\"husd\")\n",
    "\n",
    "## Pre-specified hypotheses\n",
    "\n",
    "We will begin by learning how to test pre-specified null hypotheses of the form\n",
    "\\begin{equation} \n",
    "\\label{eq:1}\n",
    "H_{0}: E[Y(1) - Y(0) | G_i = 1] = E[Y(1) - Y(0) | G_i = 0] \n",
    "\\end{equation}\n",
    "\n",
    "That is, that the treatment effect is the same regardless of membership to some group\n",
    "$G_i$. Importantly, for now we’ll assume that the group $G_i$ was **pre-specified** -- it was decided _before_ looking at the data.\n",
    "\n",
    "In a randomized setting, if the both the treatment  $W_i$ and group membership $G_i$ are binary, we can write\n",
    "\\begin{equation}\n",
    "  E[Y_i(W_i)|G_i] = E[Y_i|W_i, G_i] = \\beta_0 + \\beta_w W_i + \\beta_g G_i + \\beta_{wg} W_i G_i\n",
    "\\end{equation}\n",
    "\n",
    "<font size=1>\n",
    "When $W_i$ and $G_i$ are binary, this decomposition is true without loss of generality. Why?\n",
    "</font>\n",
    "\n",
    "This allows us to write the average effects of $W_i$ and $G_i$ on $Y_i$ as\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    E[Y(1) | G_i=1] &= \\beta_0 + \\beta_w W_i + \\beta_g G_i + \\beta_{wg} W_i G_i, \\\\\n",
    "    E[Y(1) | G_i=0] &= \\beta_0 + \\beta_w W_i,  \\\\\n",
    "    E[Y(0) | G_i=1] &= \\beta_0 + \\beta_g G_i,  \\\\\n",
    "    E[Y(0) | G_i=0] &= \\beta_0.\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Rewriting the null hypothesis \\ref{eq:1} in terms of the decomposition \\@ref(eq:decomp), we see that it boils down to a test about the coefficient in the interaction: $\\beta_{xw} = 0$\n",
    "\n",
    "# Only valid in randomized settings\n",
    "\n",
    "# Suppose this his group was defined prior to collecting the data\n",
    "# Recall from last chapter -- this is equivalent to running a t-test\n",
    "fmla <- log(inuidur1)~T4+ (female+black+othrace+factor(dep)+q2+q3+q4+q5+q6+agelt35+agegt54+durable+lusd+husd)\n",
    "ols <- lm(fmla, data=data)\n",
    "coeftest(ols, vcov=vcovHC(ols, type='HC2'))\n",
    "\n",
    "## Data-driven hypotheses\n",
    "\n",
    "Pre-specifying hypotheses prior to looking at the data is in general good practice to avoid \"p-hacking\" (e.g., slicing the data into different subgroups until a significant result is found). However, valid tests can also be attained if by **sample splitting**: we can use a subset of the sample to find promising subgroups, then test hypotheses about these subgroups in the remaining sample. This kind of sample splitting for hypothesis testing is called **honesty**.\n",
    "\n",
    "### Via causal trees\n",
    "\n",
    "**Causal trees** [(Athey and Imbens)](PNAS, 2016)](https://www.pnas.org/content/pnas/113/27/7353.full.pdf) are an intuitive algorithm that is available in the randomized setting to discover subgroups with different treatment effects.\n",
    "\n",
    "At a high level, the idea is to divide the sample into three subsets (not necessarily of equal size). The `splitting` subset is used to fit a decision tree whose objective is modified to maximize heterogeneity in treatment effect estimates across leaves. The `estimation` subset is then used to produce a valid estimate of the treatment effect at each leaf of the fitted tree. Finally, a `test` subset can be used to validate the tree estimates.\n",
    "\n",
    "The next snippet uses `honest.causalTree` function from the [`causalTree`](https://github.com/susanathey/causalTree) package. For more details, see the [causalTree documentation](https://github.com/susanathey/causalTree/blob/master/briefintro.pdf).\n",
    "\n",
    "# Only valid for randomized data!\n",
    "fmla <- paste(outcome, \" ~\", paste(covariates, collapse = \" + \"))\n",
    "fmla\n",
    "\n",
    "# Dividing data into three subsets\n",
    "indices <- split(seq(nrow(data)), sort(seq(nrow(data)) %% 3))\n",
    "names(indices) <- c('split', 'est', 'test')\n",
    "\n",
    "# Fitting the forest\n",
    "ct.unpruned <- honest.causalTree(\n",
    "  formula=fmla,            # Define the model\n",
    "  data=data[indices$split,],\n",
    "  treatment=data[indices$split, treatment],\n",
    "  est_data=data[indices$est,],\n",
    "  est_treatment=data[indices$est, treatment],\n",
    "  minsize=1,                 # Min. number of treatment and control cases in each leaf\n",
    "  HonestSampleSize=length(indices$est), #  Num obs used in estimation after splitting\n",
    "  \n",
    "  # We recommend not changing the parameters below\n",
    "  split.Rule=\"CT\",            # Define the splitting option\n",
    "  cv.option=\"TOT\",            # Cross validation options\n",
    "  cp=0,                       # Complexity parameter\n",
    "  split.Honest=TRUE,          # Use honesty when splitting\n",
    "  cv.Honest=TRUE              # Use honesty when performing cross-validation\n",
    ")\n",
    "\n",
    "# Table of cross-validated values by tuning parameter.\n",
    "ct.cptable <- as.data.frame(ct.unpruned$cptable)\n",
    "\n",
    "# Obtain optimal complexity parameter to prune tree.\n",
    "cp.selected <- which.min(ct.cptable$xerror)\n",
    "\n",
    "cp.optimal <- ct.cptable[cp.selected, \"CP\"]\n",
    "\n",
    "# Prune the tree at optimal complexity parameter.\n",
    "ct.pruned <- prune(tree=ct.unpruned, cp=cp.optimal)\n",
    "\n",
    "# Predict point estimates (on estimation sample)\n",
    "tau.hat.est <- predict(ct.pruned, newdata=data[indices$est,])\n",
    "\n",
    "# Create a factor column 'leaf' indicating leaf assignment in the estimation set\n",
    "num.leaves <- length(unique(tau.hat.est))\n",
    "leaf <- factor(tau.hat.est, levels=sort(unique(tau.hat.est)), labels = seq(num.leaves))\n",
    "\n",
    "Note: if your tree is not splitting at all, try decreasing the parameter `minsize` that controls the minimum size of each leaf. The next snippet plots the learned tree. The values in the cell are the estimated treatment effect and an estimate of the fraction of the population that falls within each leaf. Both are estimated using the `estimation` sample.\n",
    "\n",
    "rpart.plot(\n",
    "  x=ct.pruned,        # Pruned tree\n",
    "  type=3,             # Draw separate split labels for the left and right directions\n",
    "  fallen=TRUE,        # Position the leaf nodes at the bottom of the graph\n",
    "  leaf.round=1,       # Rounding of the corners of the leaf node boxes\n",
    "  extra=100,          # Display the percentage of observations in the node\n",
    "  branch=.1,          # Shape of the branch lines\n",
    "  box.palette=\"RdBu\") # Palette for coloring the node\n",
    "\n",
    "<font size=1>\n",
    "Interpret the heatmap above. What describes the subgroups with strongest and weakest estimated treatment effect?\n",
    "</font>\n",
    "\n",
    "\n",
    "### Via grf\n",
    "\n",
    "The function `causal_forest` from the package `grf` allows us to get estimates of the CATE \\@ref(eq:cate). \n",
    "\n",
    "# Get predictions from forest fitted above.\n",
    "tau.hat <- predict(forest.tau)$predictions  # tau(X) estimates\n",
    "\n",
    "Having fit a non-parametric method such as a causal forest, a researcher may (incorrectly) start by looking at the distribution of its predictions of the treatment effect. One might be tempted to think: \"if the histogram is concentrated at a point, then there is no heterogeneity; if the histogram is spread out, then our estimator has found interesting heterogeneity.\" However, this may be false.\n",
    "\n",
    "# Do not use this for assessing heterogeneity. See text above.\n",
    "hist(tau.hat, main=\"CATE estimates\", freq=F)\n",
    "\n",
    "If the histogram is concentrated at a point, we may simply be underpowered: our method was not able to detect any heterogeneity, but maybe it would detect it if we had more data. If the histogram is spread out, we may be overfitting: our model is producing very noisy estimates $\\widehat{\\tau}(x)$, but in fact the true  $\\tau(x)$ can be much smoother as a function of $x$.\n",
    "\n",
    "The `grf` package also produces a measure of variable importance that indicates how often a variable was used in a tree split. Again, much like the histogram above, this can be a rough diagnostic, but it should not be interpreted as indicating that, for example, variable with low importance is not related to heterogeneity. The reasoning is the same as the one presented in the causal trees section: if two covariates are highly correlated, the trees might split on one covariate but not the other, even though both (or maybe neither) are relevant in the true data-generating process.\n",
    "\n",
    "var_imp <- c(variable_importance(forest.tau))\n",
    "names(var_imp) <- covariates\n",
    "sorted_var_imp <- sort(var_imp, decreasing = TRUE)\n",
    "sorted_var_imp[1:5]  # showing only first few\n",
    "\n",
    "#### Data-driven subgroups\n",
    "\n",
    "Just as with causal trees, we can use causal forests to divide our observations into subgroups. In place of leaves, we'll rank observation into (say) quintiles according to their estimated CATE prediction; see, e.g., [Chernozhukov, Demirer, Duflo, Fernández-Val (2020)](https://arxiv.org/abs/1712.04802) for similar ideas.\n",
    "\n",
    "There's a subtle but important point that needs to be addressed here. As we have mentioned before, when predicting the conditional average treatment effect $\\tau(X_i)$ for observation $i$ we should in general avoid using a model that was fitted using observation $i$. This sort of sample splitting (which we called **honesty** above) is one of the required ingredients to get unbiased estimates of the CATE using the methods described here. However, when ranking estimates of two observations $i$ and $j$, we need something a little stronger: we must ensure that the model was not fit using _either_ $i$ _or_ $j$'s data. \n",
    "\n",
    "One way of overcoming this obstacle is simple. First, divide the data into $K$ folds (subsets). Then, cycle through the folds, fitting a CATE model on $K-1$ folds. Next, for each held-out fold, _separately_ rank the unseen observations into $Q$ groups based on their prediction  (i.e., if $Q=5$, then we rank observations by estimated CATE into \"top quintile\", \"second quintile\", and so on). After concatenating the independent rankings together, we can study the differences in observations in each rank-group. \n",
    "\n",
    "[This gist](https://gist.github.com/halflearned/bea4e5137c0c81fd18a75f682da466c8) computes the above for `grf`, and it should not be hard to modify it so as to replace forests by any other non-parametric method. However, for `grf` specifically, there's a small trick that allows us to obtain a valid ranking: we can pass a vector of fold indices to the argument `clusters` and rank observations within each fold. This works because estimates for each fold (\"cluster\")   trees are computed using trees that were not fit using observations from that fold. Here's how to do it. \n",
    "\n",
    "\n",
    "# Valid randomized data and observational data with unconfoundedness+overlap.\n",
    "# Note: read the comments below carefully. \n",
    "# In randomized settings, do not estimate forest.e and e.hat; use known assignment probs.\n",
    "\n",
    "# Prepare dataset\n",
    "fmla <- formula(paste0(\"~ 0 + \", paste0(covariates, collapse=\"+\")))\n",
    "X <- model.matrix(fmla, data)\n",
    "W <- data[,treatment]\n",
    "Y <- data[,outcome]\n",
    "\n",
    "# Number of rankings that the predictions will be ranking on \n",
    "# (e.g., 2 for above/below median estimated CATE, 5 for estimated CATE quintiles, etc.)\n",
    "num.rankings <- 5  \n",
    "\n",
    "# Prepare for data.splitting\n",
    "# Assign a fold number to each observation.\n",
    "# The argument 'clusters' in the next step will mimick K-fold cross-fitting.\n",
    "num.folds <- 10\n",
    "folds <- sort(seq(n) %% num.folds) + 1\n",
    "\n",
    "# Comment or uncomment depending on your setting.\n",
    "# Observational setting with unconfoundedness+overlap (unknown assignment probs):\n",
    "# forest <- causal_forest(X, Y, W, clusters = folds)\n",
    "# Randomized settings with fixed and known probabilities (here: 0.5).\n",
    "forest <- causal_forest(X, Y, W, W.hat=.5, clusters = folds)\n",
    "\n",
    "# Retrieve out-of-bag predictions.\n",
    "# Predictions for observation in fold k will be computed using \n",
    "# trees that were not trained using observations for that fold.\n",
    "tau.hat <- predict(forest)$predictions\n",
    "\n",
    "# Rank observations *within each fold* into quintiles according to their CATE predictions.\n",
    "ranking <- rep(NA, n)\n",
    "for (fold in seq(num.folds)) {\n",
    "  tau.hat.quantiles <- quantile(tau.hat[folds == fold], probs = seq(0, 1, by=1/num.rankings))\n",
    "  ranking[folds == fold] <- cut(tau.hat[folds == fold], tau.hat.quantiles, include.lowest=TRUE,labels=seq(num.rankings))\n",
    "}\n",
    "\n",
    "The next snippet computes the average treatment effect within each group defined above, i.e., $\\E[Y_i(1) - Y_i(0)|G_i = g]$. This can done in two ways. First, by computing a simple difference-in-means estimate of the ATE based on observations within each group. This is valid only in randomized settings.\n",
    "\n",
    "# Valid only in randomized settings.\n",
    "# Average difference-in-means within each ranking\n",
    "\n",
    "# Formula y ~ 0 + ranking + ranking:w\n",
    "fmla <- paste0(outcome, \" ~ 0 + ranking + ranking:\", treatment)\n",
    "ols.ate <- lm(fmla, data=transform(data, ranking=factor(ranking)))\n",
    "ols.ate <- coeftest(ols.ate, vcov=vcovHC(ols.ate, type='HC2'))\n",
    "interact <- which(grepl(\":\", rownames(ols.ate)))\n",
    "ols.ate <- data.frame(\"ols\", paste0(\"Q\", seq(num.rankings)), ols.ate[interact, 1:2])\n",
    "rownames(ols.ate) <- NULL # just for display\n",
    "colnames(ols.ate) <- c(\"method\", \"ranking\", \"estimate\", \"std.err\")\n",
    "ols.ate\n",
    "\n",
    "Another option is to average the AIPW scores within each group. This valid for both randomized settings and observational settings with unconfoundedness and overlap. Moreover, AIPW-based estimators should produce estimates with tighter confidence intervals in large samples.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
